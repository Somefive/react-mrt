(this["webpackJsonpreact-mrt"]=this["webpackJsonpreact-mrt"]||[]).push([[0],{147:function(e){e.exports=JSON.parse('{"root":{"paper_id":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","paper_title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","paper_year":2018,"paper_venue":"NAACL-HLT","paper_citations":997,"paper_abstract":"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. \\nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).","citations":[],"references":["a97dc52807d80454e78d255f9fbd7b0fab56bd03","1005645c05585c2042e3410daeed638b55e2474d","843959ffdccf31c6694d135fad07425924f785b1","b9de9599d7241459db9213b5cdd7059696f5ef8d","bc8fa64625d9189f5801837e7b133e7fe3c581f7","1a07186bc10592f0330655519ad91652125cd907","6e795c6e9916174ae12349f5dc3f516570c17ce8","38211dc39e41273c0007889202c69f841e02248a","128cb6b891aee1b5df099acb48e2efecfcff689f","8c1b00128e74f1cd92aede3959690615695d5101","93b8da28d006415866bf48f9a6e06b5242129195","ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c","007ab5528b3bd310a80d553cccad4b78dc496b02","3c78c6df5eb1695b6a399e346dde880af27d1016","4aa9f5150b46320f534de4747a2dd0cd7f3fe292","081651b38ff7533550a3adfc1c00da333a8fe86c","0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38","f37e1b62a767a307c046404ca96bc140b3e68cb5","59761abc736397539bdd01ad7f9d91c8607c0457","bc1d609520290e0460c49b685675eb5a57fa5935","1e077413b25c4d34945cc2707e17e46ed4fe784a","af5c4b80fbf847f69a202ba5a780a3dd18c1a027","a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096","2c5135a0531bc5ad7dd890f018e67a40529f5bcb","783480acff435bfbc15ffcdb4f15eccddaa0c810","e0222a1ae6874f7fff128c3da8769ab95963da04","7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d","204e3073870fae3d05bcbc2f6a8e263d9b72e776","2cd8e8f510c89c7c18268e8ad51c061e459ad321","0c47cad9729c38d9db1f75491b1ee4bd883a5d4e","cb0f3ee1e98faf92429d601cdcd76c69c1e484eb","1510cf4b8abea80b9f352325ca4c132887de21a0","9fa8d73e572c3ca824a04a5f551b602a17831bc5","421fc2556836a6b441de806d7b393a35b6eaea58","0e6824e137847be0599bb0032e37042ed2ef5045","4361e64f2d12d63476fdc88faf72a0f70d9a2ffb","f04df4e20a18358ea2f689b4c129781628ef7fc1","87f40e6f3022adbc1f1905e3e506abad05a9964f","5d833331b0e22ff359db05c62a8bca18c4f04b68","05dd7254b632376973f3a1b4d39485da17814df5","26e743d5bd465f49b9538deaf116c15e61b7951f","27e98e09cf09bc13c913d01676e5f32624011050","3febb2bed8865945e7fddc99efd791887bb7e14f","766ce989b8b8b984f7a4691fd8c9af4bdb2b74cd","dac72f2c509aee67524d3321f77e97e8eff51de6","10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb","ac11062f1f368d97f4c826c317bf50dcc13fdb59","f010affab57b5fcf1cd6be23df79d8ec98c7289c","5ded2b8c64491b4a67f6d39ce473d4b9347a672e","475354f10798f110d34792b6d88f31d6d5cb099e","687bac2d3320083eb4530bf18bb8f8f721477600","26b47e35fe6e4260fdf7b7cc98f279a73c277494"],"score":1.0000000000000002},"branches":[[[{"paper_id":"843959ffdccf31c6694d135fad07425924f785b1","paper_title":"Extracting and composing robust features with denoising autoencoders","paper_year":2008,"paper_venue":"ICML","paper_citations":999,"paper_abstract":"Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.","citations":["df2b0e26d0599ce3e70df8a9da02e51594e0e992"],"references":["0d67362a5630ec3b7562327acc278c1c996454b5"],"score":1.6618924632671561},{"paper_id":"9fa8d73e572c3ca824a04a5f551b602a17831bc5","paper_title":"Domain Adaptation with Structural Correspondence Learning","paper_year":2006,"paper_venue":"EMNLP","paper_citations":925,"paper_abstract":"Discriminative learning methods are widely used in natural language processing. These methods work best when their training and test data are drawn from the same distribution. For many NLP tasks, however, we are confronted with new domains in which labeled data is scarce or non-existent. In such cases, we seek to adapt existing models from a resource-rich source domain to a resource-poor target domain. We introduce structural correspondence learning to automatically induce correspondences among features from different domains. We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger.","citations":["d895647b4a80861703851ef55930a2627fe19492","5ded2b8c64491b4a67f6d39ce473d4b9347a672e","df2b0e26d0599ce3e70df8a9da02e51594e0e992"],"references":["783480acff435bfbc15ffcdb4f15eccddaa0c810","2c5135a0531bc5ad7dd890f018e67a40529f5bcb"],"score":1.6838757299044698},{"paper_id":"6e795c6e9916174ae12349f5dc3f516570c17ce8","paper_title":"Skip-Thought Vectors","paper_year":2015,"paper_venue":"NIPS","paper_citations":846,"paper_abstract":"We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.","citations":["d82b55c35c8673774a708353838918346f6c006f","a97dc52807d80454e78d255f9fbd7b0fab56bd03","0e6824e137847be0599bb0032e37042ed2ef5045","204a4a70428f3938d2c538a4d74c7ae0416306d8","af5c4b80fbf847f69a202ba5a780a3dd18c1a027","ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c","a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096","df2b0e26d0599ce3e70df8a9da02e51594e0e992","bc8fa64625d9189f5801837e7b133e7fe3c581f7","0c47cad9729c38d9db1f75491b1ee4bd883a5d4e","cb0f3ee1e98faf92429d601cdcd76c69c1e484eb","97fb4e3d45bb098e27e0071448b6152217bd35a5","bc1d609520290e0460c49b685675eb5a57fa5935","cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38","59761abc736397539bdd01ad7f9d91c8607c0457","93b8da28d006415866bf48f9a6e06b5242129195"],"references":["0e6824e137847be0599bb0032e37042ed2ef5045","1eb09fecd75eb27825dce4f964b97f4f5cc399d7","0b544dfe355a5070b60986319a3f51fb45d1348e","4e88de2930a4435f737c3996287a90ff87b95c59","0a10d64beb0931efdc24a28edaa91d539194b2e2","0157dcd6122c20b5afc359a799b2043453471f7f","944a1cfd79dbfb6fef460360a0765ba790f4027a","39dba6f22d72853561a4ed684be265e179a39e4f","1510cf4b8abea80b9f352325ca4c132887de21a0","03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f","687bac2d3320083eb4530bf18bb8f8f721477600"],"score":1.4890555681582187},{"paper_id":"2c5135a0531bc5ad7dd890f018e67a40529f5bcb","paper_title":"A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data","paper_year":2005,"paper_venue":"J. Mach. Learn. Res.","paper_citations":822,"paper_abstract":"One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don\'t have a complete understanding of their effectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Specifically we consider learning predictive structures on hypothesis spaces (that is, what kind of classifiers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi-supervised learning setting.","citations":["9fa8d73e572c3ca824a04a5f551b602a17831bc5","3a0e788268fafb23ab20da0e98bb578b06830f7d","4aa9f5150b46320f534de4747a2dd0cd7f3fe292","df2b0e26d0599ce3e70df8a9da02e51594e0e992","2538e3eb24d26f31482c479d95d2e26c0e79b990","1a07186bc10592f0330655519ad91652125cd907","d895647b4a80861703851ef55930a2627fe19492"],"references":[],"score":1.5442702763359477},{"paper_id":"f04df4e20a18358ea2f689b4c129781628ef7fc1","paper_title":"A large annotated corpus for learning natural language inference","paper_year":2015,"paper_venue":"EMNLP","paper_citations":808,"paper_abstract":"Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.","citations":["204a4a70428f3938d2c538a4d74c7ae0416306d8","af5c4b80fbf847f69a202ba5a780a3dd18c1a027","ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c","a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096","3febb2bed8865945e7fddc99efd791887bb7e14f","df2b0e26d0599ce3e70df8a9da02e51594e0e992","bc8fa64625d9189f5801837e7b133e7fe3c581f7","2cd8e8f510c89c7c18268e8ad51c061e459ad321","cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","5ded2b8c64491b4a67f6d39ce473d4b9347a672e","93b8da28d006415866bf48f9a6e06b5242129195"],"references":["f37e1b62a767a307c046404ca96bc140b3e68cb5","687bac2d3320083eb4530bf18bb8f8f721477600","27e5bd13d581ef682b96038dce4c18f260122352"],"score":1.5685211387422942},{"paper_id":"766ce989b8b8b984f7a4691fd8c9af4bdb2b74cd","paper_title":"\\"Cloze procedure\\": a new tool for measuring readability.","paper_year":1953,"paper_venue":"","paper_citations":698,"paper_abstract":"Here is the first comprehensive statement of a research method and its theory which were introduced briefly during a workshop at the 1953 AEJ convention. Included are findings from three pilot studies and two experiments in which \u201ccloze procedure\u201d results are compared with those of two readability formulas.","citations":["d1505c6123c102e53eb19dff312cb25cea840b72","df2b0e26d0599ce3e70df8a9da02e51594e0e992"],"references":[],"score":1.4182965188056895},{"paper_id":"ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c","paper_title":"Supervised Learning of Universal Sentence Representations from Natural Language Inference Data","paper_year":2017,"paper_venue":"EMNLP","paper_citations":474,"paper_abstract":"Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.","citations":["af5c4b80fbf847f69a202ba5a780a3dd18c1a027","a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096","df2b0e26d0599ce3e70df8a9da02e51594e0e992","bc8fa64625d9189f5801837e7b133e7fe3c581f7","0c47cad9729c38d9db1f75491b1ee4bd883a5d4e","cb0f3ee1e98faf92429d601cdcd76c69c1e484eb","bc1d609520290e0460c49b685675eb5a57fa5935","cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","5ded2b8c64491b4a67f6d39ce473d4b9347a672e","1e077413b25c4d34945cc2707e17e46ed4fe784a","93b8da28d006415866bf48f9a6e06b5242129195"],"references":["0e6824e137847be0599bb0032e37042ed2ef5045","f04df4e20a18358ea2f689b4c129781628ef7fc1","1eb09fecd75eb27825dce4f964b97f4f5cc399d7","1510cf4b8abea80b9f352325ca4c132887de21a0","87f40e6f3022adbc1f1905e3e506abad05a9964f","4e88de2930a4435f737c3996287a90ff87b95c59","204a4a70428f3938d2c538a4d74c7ae0416306d8","26e743d5bd465f49b9538deaf116c15e61b7951f","39dba6f22d72853561a4ed684be265e179a39e4f","2538e3eb24d26f31482c479d95d2e26c0e79b990","1a07186bc10592f0330655519ad91652125cd907","6e795c6e9916174ae12349f5dc3f516570c17ce8","97fb4e3d45bb098e27e0071448b6152217bd35a5","23694a80bf1b9b38215be3e23068dd75296bc90f","38211dc39e41273c0007889202c69f841e02248a","f37e1b62a767a307c046404ca96bc140b3e68cb5","5ded2b8c64491b4a67f6d39ce473d4b9347a672e"],"score":1.6047653743310075},{"paper_id":"2cd8e8f510c89c7c18268e8ad51c061e459ad321","paper_title":"A Decomposable Attention Model for Natural Language Inference","paper_year":2016,"paper_venue":"EMNLP","paper_citations":410,"paper_abstract":"We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.","citations":["204a4a70428f3938d2c538a4d74c7ae0416306d8","af5c4b80fbf847f69a202ba5a780a3dd18c1a027","5ded2b8c64491b4a67f6d39ce473d4b9347a672e","b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f","df2b0e26d0599ce3e70df8a9da02e51594e0e992","bc8fa64625d9189f5801837e7b133e7fe3c581f7","204e3073870fae3d05bcbc2f6a8e263d9b72e776","26b47e35fe6e4260fdf7b7cc98f279a73c277494"],"references":["f37e1b62a767a307c046404ca96bc140b3e68cb5","05aba481e8a221df5d8775a3bb749001e7f2525e","f04df4e20a18358ea2f689b4c129781628ef7fc1"],"score":1.5425204168887139},{"paper_id":"4aa9f5150b46320f534de4747a2dd0cd7f3fe292","paper_title":"Semi-supervised Sequence Learning","paper_year":2015,"paper_venue":"NIPS","paper_citations":364,"paper_abstract":"We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a \\"pretraining\\" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.","citations":["d82b55c35c8673774a708353838918346f6c006f","a97dc52807d80454e78d255f9fbd7b0fab56bd03","3febb2bed8865945e7fddc99efd791887bb7e14f","df2b0e26d0599ce3e70df8a9da02e51594e0e992","bc8fa64625d9189f5801837e7b133e7fe3c581f7","0c47cad9729c38d9db1f75491b1ee4bd883a5d4e","0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38","cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","1e077413b25c4d34945cc2707e17e46ed4fe784a"],"references":["2c5135a0531bc5ad7dd890f018e67a40529f5bcb","51a55df1f023571a7e07e338ee45a3e3d66ef73e","39dba6f22d72853561a4ed684be265e179a39e4f","649d03490ef72c5274e3bccd03d7a299d2f8da91","f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97","1510cf4b8abea80b9f352325ca4c132887de21a0","687bac2d3320083eb4530bf18bb8f8f721477600"],"score":1.6060025345829172},{"paper_id":"5ded2b8c64491b4a67f6d39ce473d4b9347a672e","paper_title":"A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference","paper_year":2017,"paper_venue":"NAACL-HLT","paper_citations":314,"paper_abstract":"This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. In addition to being one of the largest corpora available for the task of NLI, at 433k examples, this corpus improves upon available resources in its coverage: it offers data from ten distinct genres of written and spoken English--making it possible to evaluate systems on nearly the full complexity of the language--and it offers an explicit setting for the evaluation of cross-genre domain adaptation.","citations":["af5c4b80fbf847f69a202ba5a780a3dd18c1a027","ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c","df2b0e26d0599ce3e70df8a9da02e51594e0e992","ac11062f1f368d97f4c826c317bf50dcc13fdb59","cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","93b8da28d006415866bf48f9a6e06b5242129195"],"references":["9fa8d73e572c3ca824a04a5f551b602a17831bc5","f04df4e20a18358ea2f689b4c129781628ef7fc1","ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c","2cd8e8f510c89c7c18268e8ad51c061e459ad321","27e5bd13d581ef682b96038dce4c18f260122352","f37e1b62a767a307c046404ca96bc140b3e68cb5"],"score":1.5462048376997686},{"paper_id":"26e743d5bd465f49b9538deaf116c15e61b7951f","paper_title":"Learning Distributed Representations of Sentences from Unlabelled Data","paper_year":2016,"paper_venue":"HLT-NAACL","paper_citations":238,"paper_abstract":"Unsupervised methods for learning distributed representations of words are ubiquitous in today\'s NLP research, but far less is known about the best ways to learn distributed phrase or sentence representations from unlabelled data. This paper is a systematic comparison of models that learn such representations. We find that the optimal approach depends critically on the intended application. Deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-linear models work best for building representation spaces that can be decoded with simple spatial distance metrics. We also propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance.","citations":["a97dc52807d80454e78d255f9fbd7b0fab56bd03","204a4a70428f3938d2c538a4d74c7ae0416306d8","ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c","a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096","bc8fa64625d9189f5801837e7b133e7fe3c581f7","df2b0e26d0599ce3e70df8a9da02e51594e0e992","0c47cad9729c38d9db1f75491b1ee4bd883a5d4e","0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38","bc1d609520290e0460c49b685675eb5a57fa5935","93b8da28d006415866bf48f9a6e06b5242129195"],"references":["843959ffdccf31c6694d135fad07425924f785b1","3cfbb77e5a0e24772cfdb2eb3d4f35dead54b118","cfa2646776405d50533055ceb1b7f050e9014dcb","0b544dfe355a5070b60986319a3f51fb45d1348e","87f40e6f3022adbc1f1905e3e506abad05a9964f","3ed9b9e2972e429cbba3d9f8a5edff2f4f8619fe","39dba6f22d72853561a4ed684be265e179a39e4f","6aa3d8bcca2ebdc52ef7cd786204c338f9d609f2","1510cf4b8abea80b9f352325ca4c132887de21a0","03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f"],"score":1.540018010171464},{"paper_id":"128cb6b891aee1b5df099acb48e2efecfcff689f","paper_title":"The Winograd Schema Challenge","paper_year":2011,"paper_venue":"AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning","paper_citations":198,"paper_abstract":"In this paper, we present an alternative to the Turing Test that has some conceptual and practical advantages. Like the original, it involves responding to typed English sentences, and English-speaking adults will have no difficulty with it. Unlike the original, the subject is not required to engage in a conversation and fool an interrogator into believing she is dealing with a person. Moreover, the test is arranged in such a way that having full access to a large corpus of English text might not help much. Finally, the interrogator or a third party will be able to decide unambiguously after a few minutes whether or not a subject has passed the test.","citations":["df2b0e26d0599ce3e70df8a9da02e51594e0e992","93b8da28d006415866bf48f9a6e06b5242129195"],"references":["27e5bd13d581ef682b96038dce4c18f260122352"],"score":1.588341974693995},{"paper_id":"475354f10798f110d34792b6d88f31d6d5cb099e","paper_title":"Automatically Constructing a Corpus of Sentential Paraphrases","paper_year":2005,"paper_venue":"IWP@IJCNLP","paper_citations":192,"paper_abstract":"An obstacle to research in automatic paraphrase identification and generation is the lack of large-scale, publiclyavailable labeled corpora of sentential paraphrases. This paper describes the creation of the recently-released Microsoft Research Paraphrase Corpus, which contains 5801 sentence pairs, each hand-labeled with a binary judgment as to whether the pair constitutes a paraphrase. The corpus was created using heuristic extraction techniques in conjunction with an SVM-based classifier to select likely sentence-level paraphrases from a large corpus of topicclustered news data. These pairs were then submitted to human judges, who confirmed that 67% were in fact semantically equivalent. In addition to describing the corpus itself, we explore a number of issues that arose in defining guidelines for the human raters.","citations":["93b8da28d006415866bf48f9a6e06b5242129195","df2b0e26d0599ce3e70df8a9da02e51594e0e992","cd18800a0fe0b668a1cc19f2ec95b5003d0a5035"],"references":[],"score":1.568037942140309},{"paper_id":"93b8da28d006415866bf48f9a6e06b5242129195","paper_title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding","paper_year":2018,"paper_venue":"ICLR","paper_citations":159,"paper_abstract":"For natural language understanding (NLU) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark (GLUE), a tool for evaluating and analyzing the performance of models across a diverse range of existing NLU tasks. GLUE is model-agnostic, but it incentivizes sharing knowledge across tasks because certain tasks have very limited training data. We further provide a hand-crafted diagnostic test suite that enables detailed linguistic analysis of NLU models. We evaluate baselines based on current methods for multi-task and transfer learning and find that they do not immediately give substantial improvements over the aggregate performance of training a separate model per task, indicating room for improvement in developing general and robust NLU systems.","citations":["af5c4b80fbf847f69a202ba5a780a3dd18c1a027","cb0f3ee1e98faf92429d601cdcd76c69c1e484eb","df2b0e26d0599ce3e70df8a9da02e51594e0e992","cd18800a0fe0b668a1cc19f2ec95b5003d0a5035"],"references":["bc8fa64625d9189f5801837e7b133e7fe3c581f7","6e795c6e9916174ae12349f5dc3f516570c17ce8","128cb6b891aee1b5df099acb48e2efecfcff689f","ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c","007ab5528b3bd310a80d553cccad4b78dc496b02","f37e1b62a767a307c046404ca96bc140b3e68cb5","a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096","204e3073870fae3d05bcbc2f6a8e263d9b72e776","cb0f3ee1e98faf92429d601cdcd76c69c1e484eb","1510cf4b8abea80b9f352325ca4c132887de21a0","f04df4e20a18358ea2f689b4c129781628ef7fc1","5d833331b0e22ff359db05c62a8bca18c4f04b68","05dd7254b632376973f3a1b4d39485da17814df5","3febb2bed8865945e7fddc99efd791887bb7e14f","26e743d5bd465f49b9538deaf116c15e61b7951f","892e53fe5cd39f037cb2a961499f42f3002595dd","2538e3eb24d26f31482c479d95d2e26c0e79b990","27e5bd13d581ef682b96038dce4c18f260122352","5ded2b8c64491b4a67f6d39ce473d4b9347a672e","475354f10798f110d34792b6d88f31d6d5cb099e","687bac2d3320083eb4530bf18bb8f8f721477600"],"score":1.616909970317315},{"paper_id":"a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096","paper_title":"SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation","paper_year":2017,"paper_venue":"SemEval@ACL","paper_citations":156,"paper_abstract":"Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in all language tracks. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the STS Benchmark is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017).","citations":["93b8da28d006415866bf48f9a6e06b5242129195","df2b0e26d0599ce3e70df8a9da02e51594e0e992","cd18800a0fe0b668a1cc19f2ec95b5003d0a5035"],"references":["f04df4e20a18358ea2f689b4c129781628ef7fc1","87f40e6f3022adbc1f1905e3e506abad05a9964f","ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c","26e743d5bd465f49b9538deaf116c15e61b7951f","892e53fe5cd39f037cb2a961499f42f3002595dd","6e795c6e9916174ae12349f5dc3f516570c17ce8","1510cf4b8abea80b9f352325ca4c132887de21a0","f37e1b62a767a307c046404ca96bc140b3e68cb5","0a10d64beb0931efdc24a28edaa91d539194b2e2"],"score":1.4142485140829688},{"paper_id":"af5c4b80fbf847f69a202ba5a780a3dd18c1a027","paper_title":"SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference","paper_year":2018,"paper_venue":"EMNLP","paper_citations":64,"paper_abstract":"Given a partial description like \\"she opened the hood of the car,\\" humans can reason about the situation and anticipate what might come next (\\"then, she examined the engine\\"). In this paper, we introduce the task of grounded commonsense inference, unifying natural language inference and commonsense reasoning. \\nWe present SWAG, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations. To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data. To account for the aggressive adversarial filtering, we use state-of-the-art language models to massively oversample a diverse set of potential counterfactuals. Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88%), various competitive models struggle on our task. We provide comprehensive analysis that indicates significant opportunities for future research.","citations":["df2b0e26d0599ce3e70df8a9da02e51594e0e992"],"references":["0e6824e137847be0599bb0032e37042ed2ef5045","f04df4e20a18358ea2f689b4c129781628ef7fc1","ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c","50d53cc562225549457cbc782546bfbe1ac6f0cf","3febb2bed8865945e7fddc99efd791887bb7e14f","892e53fe5cd39f037cb2a961499f42f3002595dd","2cd8e8f510c89c7c18268e8ad51c061e459ad321","27e5bd13d581ef682b96038dce4c18f260122352","6e795c6e9916174ae12349f5dc3f516570c17ce8","f37e1b62a767a307c046404ca96bc140b3e68cb5","5ded2b8c64491b4a67f6d39ce473d4b9347a672e","93b8da28d006415866bf48f9a6e06b5242129195"],"score":1.5022070130411387},{"paper_id":"bc1d609520290e0460c49b685675eb5a57fa5935","paper_title":"An efficient framework for learning sentence representations","paper_year":2018,"paper_venue":"ICLR","paper_citations":57,"paper_abstract":"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.","citations":["df2b0e26d0599ce3e70df8a9da02e51594e0e992","cd18800a0fe0b668a1cc19f2ec95b5003d0a5035"],"references":["d82b55c35c8673774a708353838918346f6c006f","a97dc52807d80454e78d255f9fbd7b0fab56bd03","1005645c05585c2042e3410daeed638b55e2474d","446fbff6a2a7c9989b0a0465f960e236d9a5e886","ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c","87f40e6f3022adbc1f1905e3e506abad05a9964f","4e88de2930a4435f737c3996287a90ff87b95c59","26e743d5bd465f49b9538deaf116c15e61b7951f","6e795c6e9916174ae12349f5dc3f516570c17ce8","1510cf4b8abea80b9f352325ca4c132887de21a0","f37e1b62a767a307c046404ca96bc140b3e68cb5","c4fd9c86b2b41df51a6fe212406dda81b1997fd4","0a10d64beb0931efdc24a28edaa91d539194b2e2","687bac2d3320083eb4530bf18bb8f8f721477600"],"score":1.580773099612117},{"paper_id":"4361e64f2d12d63476fdc88faf72a0f70d9a2ffb","paper_title":"Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units","paper_year":2017,"paper_venue":"ArXiv","paper_citations":45,"paper_abstract":"We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic regularizer which randomly applies the identity or zero map to a neuron\'s input. This stochastic regularizer is comparable to nonlinearities aided by dropout, but it removes the need for a traditional nonlinearity. The connection between the GELU and the stochastic regularizer suggests a new probabilistic understanding of nonlinearities. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all tasks.","citations":["df2b0e26d0599ce3e70df8a9da02e51594e0e992","cd18800a0fe0b668a1cc19f2ec95b5003d0a5035"],"references":[],"score":1.399347526863517},{"paper_id":"a97dc52807d80454e78d255f9fbd7b0fab56bd03","paper_title":"Discourse-Based Objectives for Fast Unsupervised Sentence Representation Learning","paper_year":2017,"paper_venue":"ArXiv","paper_citations":35,"paper_abstract":"This work presents a novel objective function for the unsupervised training of neural network sentence encoders. It exploits signals from paragraph-level discourse coherence to train these models to understand text. Our objective is purely discriminative, allowing us to train models many times faster than was possible under prior methods, and it yields models which perform well in extrinsic evaluations.","citations":["bc1d609520290e0460c49b685675eb5a57fa5935","df2b0e26d0599ce3e70df8a9da02e51594e0e992","cd18800a0fe0b668a1cc19f2ec95b5003d0a5035"],"references":["0e6824e137847be0599bb0032e37042ed2ef5045","1eb09fecd75eb27825dce4f964b97f4f5cc399d7","26e743d5bd465f49b9538deaf116c15e61b7951f","51a55df1f023571a7e07e338ee45a3e3d66ef73e","892e53fe5cd39f037cb2a961499f42f3002595dd","4aa9f5150b46320f534de4747a2dd0cd7f3fe292","39dba6f22d72853561a4ed684be265e179a39e4f","687bac2d3320083eb4530bf18bb8f8f721477600","6e795c6e9916174ae12349f5dc3f516570c17ce8","05aba481e8a221df5d8775a3bb749001e7f2525e","622b38e59cf9fe035f258964b8d6fd3238463e5e","033eb044ef6a865a53878397633876827b7a8f20"],"score":1.608910334528798},{"paper_id":"cb0f3ee1e98faf92429d601cdcd76c69c1e484eb","paper_title":"Neural Network Acceptability Judgments","paper_year":2018,"paper_venue":"ArXiv","paper_citations":34,"paper_abstract":"In this work, we explore the ability of artificial neural networks to judge the grammatical acceptability of a sentence. Machine learning research of this kind is well placed to answer important open questions about the role of prior linguistic bias in language acquisition by providing a test for the Poverty of the Stimulus Argument. In service of this goal, we introduce the Corpus of Linguistic Acceptability (CoLA), a set of 10,657 English sentences labeled as grammatical or ungrammatical by expert linguists. We train several recurrent neural networks to do binary acceptability classification. These models set a baseline for the task. Error-analysis testing the models on specific grammatical phenomena reveals that they learn some systematic grammatical generalizations like subject-verb-object word order without any grammatical supervision. We find that neural sequence models show promise on the acceptability classification task. However, human-like performance across a wide range of grammatical constructions remains far off.","citations":["df2b0e26d0599ce3e70df8a9da02e51594e0e992","93b8da28d006415866bf48f9a6e06b5242129195"],"references":["ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c","0b544dfe355a5070b60986319a3f51fb45d1348e","3febb2bed8865945e7fddc99efd791887bb7e14f","39dba6f22d72853561a4ed684be265e179a39e4f","6e795c6e9916174ae12349f5dc3f516570c17ce8","a4cec122a08216fe8a3bc19b22e78fbaea096256","f37e1b62a767a307c046404ca96bc140b3e68cb5","93b8da28d006415866bf48f9a6e06b5242129195"],"score":1.5977741146935942},{"paper_id":"1510cf4b8abea80b9f352325ca4c132887de21a0","paper_title":"Distributed Representations of Sentences and Documents","paper_year":2014,"paper_venue":"ICML","paper_citations":999,"paper_abstract":"Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \\"powerful,\\" \\"strong\\" and \\"Paris\\" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.","citations":["d82b55c35c8673774a708353838918346f6c006f","204a4a70428f3938d2c538a4d74c7ae0416306d8","ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c","a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096","26e743d5bd465f49b9538deaf116c15e61b7951f","0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38","cd18800a0fe0b668a1cc19f2ec95b5003d0a5035"],"references":["3a0e788268fafb23ab20da0e98bb578b06830f7d","1005645c05585c2042e3410daeed638b55e2474d","50d53cc562225549457cbc782546bfbe1ac6f0cf","cfa2646776405d50533055ceb1b7f050e9014dcb","0157dcd6122c20b5afc359a799b2043453471f7f","649d03490ef72c5274e3bccd03d7a299d2f8da91","dac72f2c509aee67524d3321f77e97e8eff51de6","2b669398c4cf2ebe04375c8b1beae20f4ac802fa","c4fd9c86b2b41df51a6fe212406dda81b1997fd4"],"score":1.4597103222908405}],[{"paper_id":"649d03490ef72c5274e3bccd03d7a299d2f8da91","paper_title":"Learning Word Vectors for Sentiment Analysis","paper_year":2011,"paper_venue":"ACL","paper_citations":999,"paper_abstract":"Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term--document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.","citations":["7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d","bc8fa64625d9189f5801837e7b133e7fe3c581f7","4aa9f5150b46320f534de4747a2dd0cd7f3fe292","1510cf4b8abea80b9f352325ca4c132887de21a0","1e077413b25c4d34945cc2707e17e46ed4fe784a","622b38e59cf9fe035f258964b8d6fd3238463e5e"],"references":["dac72f2c509aee67524d3321f77e97e8eff51de6","3a0e788268fafb23ab20da0e98bb578b06830f7d","1a07186bc10592f0330655519ad91652125cd907"],"score":0.4519867965574224},{"paper_id":"51a55df1f023571a7e07e338ee45a3e3d66ef73e","paper_title":"Character-level Convolutional Networks for Text Classification","paper_year":2015,"paper_venue":"NIPS","paper_citations":999,"paper_abstract":"This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.","citations":["622b38e59cf9fe035f258964b8d6fd3238463e5e","a97dc52807d80454e78d255f9fbd7b0fab56bd03","b9de9599d7241459db9213b5cdd7059696f5ef8d","4aa9f5150b46320f534de4747a2dd0cd7f3fe292","8c1b00128e74f1cd92aede3959690615695d5101","1e077413b25c4d34945cc2707e17e46ed4fe784a"],"references":["87f40e6f3022adbc1f1905e3e506abad05a9964f","2538e3eb24d26f31482c479d95d2e26c0e79b990"],"score":0.5002565146954011},{"paper_id":"27e5bd13d581ef682b96038dce4c18f260122352","paper_title":"The PASCAL Recognising Textual Entailment Challenge","paper_year":2005,"paper_venue":"MLCW","paper_citations":855,"paper_abstract":"This paper describes the PASCAL Network of Excellence first Recognising Textual Entailment (RTE-1) Challenge benchmark 1 . The RTE task is defined as recognizing, given two text fragments, whether the meaning of one text can be inferred (entailed) from the other. This application-independent task is suggested as capturing major inferences about the variability of semantic expression which are commonly needed across multiple applications. The Challenge has raised noticeable attention in the research community, attracting 17 submissions from diverse groups, suggesting the generic relevance of the task.","citations":["f04df4e20a18358ea2f689b4c129781628ef7fc1","af5c4b80fbf847f69a202ba5a780a3dd18c1a027","128cb6b891aee1b5df099acb48e2efecfcff689f","5ded2b8c64491b4a67f6d39ce473d4b9347a672e","93b8da28d006415866bf48f9a6e06b5242129195"],"references":[],"score":0.4997981602035703},{"paper_id":"446fbff6a2a7c9989b0a0465f960e236d9a5e886","paper_title":"Context Encoders: Feature Learning by Inpainting","paper_year":2016,"paper_venue":"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","paper_citations":999,"paper_abstract":"We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders - a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.","citations":["bc1d609520290e0460c49b685675eb5a57fa5935"],"references":["843959ffdccf31c6694d135fad07425924f785b1","87f40e6f3022adbc1f1905e3e506abad05a9964f","0d67362a5630ec3b7562327acc278c1c996454b5","1a07186bc10592f0330655519ad91652125cd907"],"score":0.5959421921204725},{"paper_id":"892e53fe5cd39f037cb2a961499f42f3002595dd","paper_title":"Bag of Tricks for Efficient Text Classification","paper_year":2016,"paper_venue":"EACL","paper_citations":773,"paper_abstract":"This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute.","citations":["a97dc52807d80454e78d255f9fbd7b0fab56bd03","af5c4b80fbf847f69a202ba5a780a3dd18c1a027","a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096","93b8da28d006415866bf48f9a6e06b5242129195"],"references":["1a07186bc10592f0330655519ad91652125cd907"],"score":0.4585832728125051},{"paper_id":"204a4a70428f3938d2c538a4d74c7ae0416306d8","paper_title":"A Structured Self-Attentive Sentence Embedding","paper_year":2017,"paper_venue":"ICLR","paper_citations":423,"paper_abstract":"This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.","citations":["ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c","204e3073870fae3d05bcbc2f6a8e263d9b72e776"],"references":["f04df4e20a18358ea2f689b4c129781628ef7fc1","cfa2646776405d50533055ceb1b7f050e9014dcb","0a10d64beb0931efdc24a28edaa91d539194b2e2","4e88de2930a4435f737c3996287a90ff87b95c59","26e743d5bd465f49b9538deaf116c15e61b7951f","2cd8e8f510c89c7c18268e8ad51c061e459ad321","6e795c6e9916174ae12349f5dc3f516570c17ce8","23694a80bf1b9b38215be3e23068dd75296bc90f","1510cf4b8abea80b9f352325ca4c132887de21a0","f37e1b62a767a307c046404ca96bc140b3e68cb5","03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f","687bac2d3320083eb4530bf18bb8f8f721477600"],"score":0.5558320255761235},{"paper_id":"50d53cc562225549457cbc782546bfbe1ac6f0cf","paper_title":"Reasoning With Neural Tensor Networks for Knowledge Base Completion","paper_year":2013,"paper_venue":"NIPS","paper_citations":734,"paper_abstract":"Knowledge bases are an important resource for question answering and other tasks but often suffer from incompleteness and lack of ability to reason over their discrete entities and relationships. In this paper we introduce an expressive neural tensor network suitable for reasoning over relationships between two entities. Previous work represented entities as either discrete atomic units or with a single entity vector representation. We show that performance can be improved when entities are represented as an average of their constituting word vectors. This allows sharing of statistical strength between, for instance, facts involving the \\"Sumatran tiger\\" and \\"Bengal tiger.\\" Lastly, we demonstrate that all models improve when these word vectors are initialized with vectors learned from unsupervised large corpora. We assess the model by considering the problem of predicting additional true relations between entities given a subset of the knowledge base. Our model outperforms previous models and can classify unseen relationships in WordNet and FreeBase with an accuracy of 86.2% and 90.0%, respectively.","citations":["452059171226626718eb677358836328f884298e","af5c4b80fbf847f69a202ba5a780a3dd18c1a027","1510cf4b8abea80b9f352325ca4c132887de21a0"],"references":["2b669398c4cf2ebe04375c8b1beae20f4ac802fa","1a07186bc10592f0330655519ad91652125cd907","687bac2d3320083eb4530bf18bb8f8f721477600","dac72f2c509aee67524d3321f77e97e8eff51de6"],"score":0.4867146357332064},{"paper_id":"0d67362a5630ec3b7562327acc278c1c996454b5","paper_title":"Learning Deep Architectures for AI","paper_year":2007,"paper_venue":"Foundations and Trends in Machine Learning","paper_citations":999,"paper_abstract":"Theoretical results strongly suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one needs deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult optimization task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This paper discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.","citations":["a4cec122a08216fe8a3bc19b22e78fbaea096256"],"references":["23694a80bf1b9b38215be3e23068dd75296bc90f","1005645c05585c2042e3410daeed638b55e2474d","843959ffdccf31c6694d135fad07425924f785b1","1a07186bc10592f0330655519ad91652125cd907"],"score":0.6163402193819392},{"paper_id":"d895647b4a80861703851ef55930a2627fe19492","paper_title":"Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification","paper_year":2007,"paper_venue":"ACL","paper_citations":999,"paper_abstract":"Automatic sentiment classification has been extensively studied and applied in recent years. However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is impractical. We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline. Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains.","citations":["1e077413b25c4d34945cc2707e17e46ed4fe784a"],"references":["9fa8d73e572c3ca824a04a5f551b602a17831bc5","2c5135a0531bc5ad7dd890f018e67a40529f5bcb"],"score":0.43719546630556083},{"paper_id":"d82b55c35c8673774a708353838918346f6c006f","paper_title":"Generating Sentences from a Continuous Space","paper_year":2015,"paper_venue":"CoNLL","paper_citations":564,"paper_abstract":"The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model\'s latent sentence space, and present negative results on the use of the model in language modeling.","citations":["7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d","bc1d609520290e0460c49b685675eb5a57fa5935"],"references":["4aa9f5150b46320f534de4747a2dd0cd7f3fe292","39dba6f22d72853561a4ed684be265e179a39e4f","452059171226626718eb677358836328f884298e","6e795c6e9916174ae12349f5dc3f516570c17ce8","1510cf4b8abea80b9f352325ca4c132887de21a0"],"score":0.48994672486511504}]],[[{"paper_id":"0e6824e137847be0599bb0032e37042ed2ef5045","paper_title":"Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books","paper_year":2015,"paper_venue":"2015 IEEE International Conference on Computer Vision (ICCV)","paper_citations":287,"paper_abstract":"Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in the current datasets. To align movies and books we propose a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.","citations":["a97dc52807d80454e78d255f9fbd7b0fab56bd03","af5c4b80fbf847f69a202ba5a780a3dd18c1a027","ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c","df2b0e26d0599ce3e70df8a9da02e51594e0e992","6e795c6e9916174ae12349f5dc3f516570c17ce8","97fb4e3d45bb098e27e0071448b6152217bd35a5","cd18800a0fe0b668a1cc19f2ec95b5003d0a5035"],"references":["0b544dfe355a5070b60986319a3f51fb45d1348e","944a1cfd79dbfb6fef460360a0765ba790f4027a","39dba6f22d72853561a4ed684be265e179a39e4f","6e795c6e9916174ae12349f5dc3f516570c17ce8","0a10d64beb0931efdc24a28edaa91d539194b2e2"],"score":1.369771639838549},{"paper_id":"7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d","paper_title":"MaskGAN: Better Text Generation via Filling in the _______","paper_year":2018,"paper_venue":"ICLR","paper_citations":95,"paper_abstract":"Neural text generation models are often autoregressive language models or seq2seq models. Neural autoregressive and seq2seq models that generate text by sampling words sequentially, with each word conditioned on the previous model, are state-of-the-art for several machine translation and summarization benchmarks. These benchmarks are often defined by validation perplexity even though this is not a direct measure of sample quality. Language models are typically trained via maximum likelihood and most often with teacher forcing. Teacher forcing is well-suited to optimizing perplexity but can result in poor sample quality because generating text requires conditioning on sequences of words that were never observed at training time. We propose to improve sample quality using Generative Adversarial Network (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation. GANs were originally to designed to output differentiable values, so discrete language generation is challenging for them. We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context. We show qualitatively and quantitatively, evidence that this produces more realistic text samples compared to a maximum likelihood trained model.","citations":["df2b0e26d0599ce3e70df8a9da02e51594e0e992"],"references":["d82b55c35c8673774a708353838918346f6c006f","39dba6f22d72853561a4ed684be265e179a39e4f","649d03490ef72c5274e3bccd03d7a299d2f8da91","93499a7c7f699b6630a86fad964536f9423bb6d0","23694a80bf1b9b38215be3e23068dd75296bc90f","204e3073870fae3d05bcbc2f6a8e263d9b72e776","39f7830cfb2436ff215892fafb6899c7d937042a"],"score":1.6414764745476491},{"paper_id":"b9de9599d7241459db9213b5cdd7059696f5ef8d","paper_title":"Character-Level Language Modeling with Deeper Self-Attention","paper_year":2018,"paper_venue":"AAAI","paper_citations":26,"paper_abstract":"LSTMs and other RNN variants have shown strong performance on character-level language modeling. These models are typically trained using truncated backpropagation through time, and it is common to assume that their success stems from their ability to remember long-term contexts. In this paper, we show that a deep (64-layer) transformer model with fixed context outperforms RNN variants by a large margin, achieving state of the art on two popular benchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. To get good results at this depth, we show that it is important to add auxiliary losses, both at intermediate network layers and intermediate sequence positions.","citations":["df2b0e26d0599ce3e70df8a9da02e51594e0e992"],"references":["5d833331b0e22ff359db05c62a8bca18c4f04b68","0b544dfe355a5070b60986319a3f51fb45d1348e","51a55df1f023571a7e07e338ee45a3e3d66ef73e","f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97","97fb4e3d45bb098e27e0071448b6152217bd35a5","02b3d1d162080d9aefd3fc30a0bcc9a843073b5d","23694a80bf1b9b38215be3e23068dd75296bc90f","204e3073870fae3d05bcbc2f6a8e263d9b72e776","39f7830cfb2436ff215892fafb6899c7d937042a"],"score":1.5537514079441888},{"paper_id":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","paper_title":"Attention Is All You Need","paper_year":2017,"paper_venue":"NIPS","paper_citations":997,"paper_abstract":"The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms. We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.","citations":["7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d","ac11062f1f368d97f4c826c317bf50dcc13fdb59","b9de9599d7241459db9213b5cdd7059696f5ef8d"],"references":["204a4a70428f3938d2c538a4d74c7ae0416306d8","0b544dfe355a5070b60986319a3f51fb45d1348e","39dba6f22d72853561a4ed684be265e179a39e4f","2cd8e8f510c89c7c18268e8ad51c061e459ad321","93499a7c7f699b6630a86fad964536f9423bb6d0","02b3d1d162080d9aefd3fc30a0bcc9a843073b5d"],"score":1.5930817958186907}],[{"paper_id":"02b3d1d162080d9aefd3fc30a0bcc9a843073b5d","paper_title":"Exploring the Limits of Language Modeling","paper_year":2016,"paper_venue":"ArXiv","paper_citations":479,"paper_abstract":"In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.","citations":["421fc2556836a6b441de806d7b393a35b6eaea58","b9de9599d7241459db9213b5cdd7059696f5ef8d","3febb2bed8865945e7fddc99efd791887bb7e14f","204e3073870fae3d05bcbc2f6a8e263d9b72e776","ac11062f1f368d97f4c826c317bf50dcc13fdb59","0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38","59761abc736397539bdd01ad7f9d91c8607c0457"],"references":["1005645c05585c2042e3410daeed638b55e2474d","5d833331b0e22ff359db05c62a8bca18c4f04b68","0b544dfe355a5070b60986319a3f51fb45d1348e","39dba6f22d72853561a4ed684be265e179a39e4f","f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97","38211dc39e41273c0007889202c69f841e02248a","03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f","033eb044ef6a865a53878397633876827b7a8f20"],"score":0.597161573852487},{"paper_id":"93499a7c7f699b6630a86fad964536f9423bb6d0","paper_title":"Effective Approaches to Attention-based Neural Machine Translation","paper_year":2015,"paper_venue":"EMNLP","paper_citations":999,"paper_abstract":"An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT\'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.","citations":["0c47cad9729c38d9db1f75491b1ee4bd883a5d4e","204e3073870fae3d05bcbc2f6a8e263d9b72e776","8c1b00128e74f1cd92aede3959690615695d5101","bc8fa64625d9189f5801837e7b133e7fe3c581f7"],"references":["f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97","944a1cfd79dbfb6fef460360a0765ba790f4027a","0b544dfe355a5070b60986319a3f51fb45d1348e","39dba6f22d72853561a4ed684be265e179a39e4f"],"score":0.6182140590290843},{"paper_id":"1eb09fecd75eb27825dce4f964b97f4f5cc399d7","paper_title":"On the Properties of Neural Machine Translation: Encoder-Decoder Approaches","paper_year":2014,"paper_venue":"SSST@EMNLP","paper_citations":999,"paper_abstract":"Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder\u2010Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.","citations":["ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c","3febb2bed8865945e7fddc99efd791887bb7e14f","10a4db59e81d26b2e0e896d3186ef81b4458b93f","452059171226626718eb677358836328f884298e","6e795c6e9916174ae12349f5dc3f516570c17ce8","a4cec122a08216fe8a3bc19b22e78fbaea096256","0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38"],"references":["944a1cfd79dbfb6fef460360a0765ba790f4027a","0b544dfe355a5070b60986319a3f51fb45d1348e","39dba6f22d72853561a4ed684be265e179a39e4f"],"score":0.5808183853918945},{"paper_id":"0b544dfe355a5070b60986319a3f51fb45d1348e","paper_title":"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation","paper_year":2014,"paper_venue":"EMNLP","paper_citations":999,"paper_abstract":"In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.","citations":["3c78c6df5eb1695b6a399e346dde880af27d1016","6e795c6e9916174ae12349f5dc3f516570c17ce8","a4cec122a08216fe8a3bc19b22e78fbaea096256","cb0f3ee1e98faf92429d601cdcd76c69c1e484eb"],"references":["87f40e6f3022adbc1f1905e3e506abad05a9964f","23694a80bf1b9b38215be3e23068dd75296bc90f"],"score":0.5881718800076983},{"paper_id":"39dba6f22d72853561a4ed684be265e179a39e4f","paper_title":"Sequence to Sequence Learning with Neural Networks","paper_year":2014,"paper_venue":"NIPS","paper_citations":999,"paper_abstract":"Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT\'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM\'s BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM\'s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.","citations":["26e743d5bd465f49b9538deaf116c15e61b7951f","6e795c6e9916174ae12349f5dc3f516570c17ce8","cb0f3ee1e98faf92429d601cdcd76c69c1e484eb"],"references":["944a1cfd79dbfb6fef460360a0765ba790f4027a","0b544dfe355a5070b60986319a3f51fb45d1348e","23694a80bf1b9b38215be3e23068dd75296bc90f"],"score":0.5674286687444487},{"paper_id":"f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97","paper_title":"Recurrent Neural Network Regularization","paper_year":2014,"paper_venue":"ArXiv","paper_citations":999,"paper_abstract":"We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.","citations":["421fc2556836a6b441de806d7b393a35b6eaea58","b9de9599d7241459db9213b5cdd7059696f5ef8d","4aa9f5150b46320f534de4747a2dd0cd7f3fe292","93499a7c7f699b6630a86fad964536f9423bb6d0","39f7830cfb2436ff215892fafb6899c7d937042a","033eb044ef6a865a53878397633876827b7a8f20"],"references":[],"score":0.4001910586351053},{"paper_id":"033eb044ef6a865a53878397633876827b7a8f20","paper_title":"Character-Aware Neural Language Models","paper_year":2015,"paper_venue":"AAAI","paper_citations":739,"paper_abstract":"We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway network over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60% fewer parameters. On languages with rich morphology (Arabic, Czech, French, German, Spanish, Russian), the model outperforms word-level/morpheme-level LSTM baselines, again with fewer parameters. The results suggest that on many languages, character inputs are sufficient for language modeling. Analysis of word representations obtained from the character composition part of the model reveals that the model is able to encode, from characters only, both semantic and orthographic information.","citations":["a97dc52807d80454e78d255f9fbd7b0fab56bd03","421fc2556836a6b441de806d7b393a35b6eaea58","3febb2bed8865945e7fddc99efd791887bb7e14f","ac11062f1f368d97f4c826c317bf50dcc13fdb59","02b3d1d162080d9aefd3fc30a0bcc9a843073b5d"],"references":["0b544dfe355a5070b60986319a3f51fb45d1348e","39dba6f22d72853561a4ed684be265e179a39e4f","2538e3eb24d26f31482c479d95d2e26c0e79b990","f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97","23694a80bf1b9b38215be3e23068dd75296bc90f","c4fd9c86b2b41df51a6fe212406dda81b1997fd4","03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f"],"score":0.5386948870459749},{"paper_id":"944a1cfd79dbfb6fef460360a0765ba790f4027a","paper_title":"Recurrent Continuous Translation Models","paper_year":2013,"paper_venue":"EMNLP","paper_citations":643,"paper_abstract":"We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.","citations":["0e6824e137847be0599bb0032e37042ed2ef5045","1eb09fecd75eb27825dce4f964b97f4f5cc399d7","39f7830cfb2436ff215892fafb6899c7d937042a","39dba6f22d72853561a4ed684be265e179a39e4f","452059171226626718eb677358836328f884298e","93499a7c7f699b6630a86fad964536f9423bb6d0","6e795c6e9916174ae12349f5dc3f516570c17ce8","a4cec122a08216fe8a3bc19b22e78fbaea096256","0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38","03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f"],"references":["1a07186bc10592f0330655519ad91652125cd907","05aba481e8a221df5d8775a3bb749001e7f2525e","23694a80bf1b9b38215be3e23068dd75296bc90f"],"score":0.5771056575822364},{"paper_id":"97fb4e3d45bb098e27e0071448b6152217bd35a5","paper_title":"Layer Normalization","paper_year":2016,"paper_venue":"ArXiv","paper_citations":512,"paper_abstract":"Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feedforward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.","citations":["3febb2bed8865945e7fddc99efd791887bb7e14f","ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c","b9de9599d7241459db9213b5cdd7059696f5ef8d","cd18800a0fe0b668a1cc19f2ec95b5003d0a5035"],"references":["0e6824e137847be0599bb0032e37042ed2ef5045","0b544dfe355a5070b60986319a3f51fb45d1348e","d1505c6123c102e53eb19dff312cb25cea840b72","39dba6f22d72853561a4ed684be265e179a39e4f","6e795c6e9916174ae12349f5dc3f516570c17ce8","0a10d64beb0931efdc24a28edaa91d539194b2e2"],"score":0.4379992343114078},{"paper_id":"39f7830cfb2436ff215892fafb6899c7d937042a","paper_title":"A Theoretically Grounded Application of Dropout in Recurrent Neural Networks","paper_year":2015,"paper_venue":"NIPS","paper_citations":610,"paper_abstract":"Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit, with dropout shown to fail when applied to recurrent layers. Recent results at the intersection of Bayesian modelling and deep learning offer a Bayesian interpretation of common deep learning techniques such as dropout. This grounding of dropout in approximate Bayesian inference suggests an extension of the theoretical results, offering insights into the use of dropout with RNN models. We apply this new variational inference based dropout technique in LSTM and GRU models, assessing it on language modelling and sentiment analysis tasks. The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational tools in deep learning.","citations":["3febb2bed8865945e7fddc99efd791887bb7e14f","3c78c6df5eb1695b6a399e346dde880af27d1016","7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d","b9de9599d7241459db9213b5cdd7059696f5ef8d"],"references":["f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97","944a1cfd79dbfb6fef460360a0765ba790f4027a","0b544dfe355a5070b60986319a3f51fb45d1348e","39dba6f22d72853561a4ed684be265e179a39e4f"],"score":0.39802165655200883}]],[[{"paper_id":"38211dc39e41273c0007889202c69f841e02248a","paper_title":"ImageNet: A large-scale hierarchical image database","paper_year":2009,"paper_venue":"2009 IEEE Conference on Computer Vision and Pattern Recognition","paper_citations":999,"paper_abstract":"The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \u201cImageNet\u201d, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.","citations":["a4cec122a08216fe8a3bc19b22e78fbaea096256"],"references":[],"score":1.2480842958442655},{"paper_id":"687bac2d3320083eb4530bf18bb8f8f721477600","paper_title":"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank","paper_year":2013,"paper_venue":"EMNLP","paper_citations":999,"paper_abstract":"Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.","citations":["f04df4e20a18358ea2f689b4c129781628ef7fc1","4e88de2930a4435f737c3996287a90ff87b95c59","bc8fa64625d9189f5801837e7b133e7fe3c581f7","cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","bc1d609520290e0460c49b685675eb5a57fa5935"],"references":["3a0e788268fafb23ab20da0e98bb578b06830f7d","cfa2646776405d50533055ceb1b7f050e9014dcb","1a07186bc10592f0330655519ad91652125cd907","23694a80bf1b9b38215be3e23068dd75296bc90f","2b669398c4cf2ebe04375c8b1beae20f4ac802fa","05aba481e8a221df5d8775a3bb749001e7f2525e"],"score":1.4955799822489328},{"paper_id":"081651b38ff7533550a3adfc1c00da333a8fe86c","paper_title":"How transferable are features in deep neural networks?","paper_year":2014,"paper_venue":"NIPS","paper_citations":997,"paper_abstract":"Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.","citations":["1e077413b25c4d34945cc2707e17e46ed4fe784a"],"references":["38211dc39e41273c0007889202c69f841e02248a"],"score":1.4259961241849262}],[{"paper_id":"4e88de2930a4435f737c3996287a90ff87b95c59","paper_title":"Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks","paper_year":2015,"paper_venue":"ACL","paper_citations":999,"paper_abstract":"A Long Short-Term Memory (LSTM) network is a type of recurrent neural network architecture which has recently obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. TreeLSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).","citations":["204a4a70428f3938d2c538a4d74c7ae0416306d8","ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c","452059171226626718eb677358836328f884298e","6e795c6e9916174ae12349f5dc3f516570c17ce8","bc1d609520290e0460c49b685675eb5a57fa5935","622b38e59cf9fe035f258964b8d6fd3238463e5e"],"references":["f37e1b62a767a307c046404ca96bc140b3e68cb5","87f40e6f3022adbc1f1905e3e506abad05a9964f","39dba6f22d72853561a4ed684be265e179a39e4f","dac72f2c509aee67524d3321f77e97e8eff51de6","1510cf4b8abea80b9f352325ca4c132887de21a0","2b669398c4cf2ebe04375c8b1beae20f4ac802fa","05aba481e8a221df5d8775a3bb749001e7f2525e","03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f","687bac2d3320083eb4530bf18bb8f8f721477600"],"score":0.49199038777507537},{"paper_id":"a4cec122a08216fe8a3bc19b22e78fbaea096256","paper_title":"Deep Learning","paper_year":2015,"paper_venue":"Nature","paper_citations":999,"paper_abstract":"Machine-learning technology powers many aspects of modern society: from web searches to content filtering on social networks to recommendations on e-commerce websites, and it is increasingly present in consumer products such as cameras and smartphones. Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users\u2019 interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition or machine-learning system required careful engineering and considerable domain expertise to design a feature extractor that transformed the raw data (such as the pixel values of an image) into a suitable internal representation or feature vector from which the learning subsystem, often a classifier, could detect or classify patterns in the input. Representation learning is a set of methods that allows a machine to be fed with raw data and to automatically discover the representations needed for detection or classification. Deep-learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. With the composition of enough such transformations, very complex functions can be learned. For classification tasks, higher layers of representation amplify aspects of the input that are important for discrimination and suppress irrelevant variations. An image, for example, comes in the form of an array of pixel values, and the learned features in the first layer of representation typically represent the presence or absence of edges at particular orientations and locations in the image. The second layer typically detects motifs by spotting particular arrangements of edges, regardless of small variations in the edge positions. The third layer may assemble motifs into larger combinations that correspond to parts of familiar objects, and subsequent layers would detect objects as combinations of these parts. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. In addition to beating records in image recognition and speech recognition, it has beaten other machine-learning techniques at predicting the activity of potential drug molecules, analysing particle accelerator data, reconstructing brain circuits, and predicting the effects of mutations in non-coding DNA on gene expression and disease. Perhaps more surprisingly, deep learning has produced extremely promising results for various tasks in natural language understanding, particularly topic classification, sentiment analysis, question answering and language translation. We think that deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data. New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress.","citations":[],"references":["1005645c05585c2042e3410daeed638b55e2474d","843959ffdccf31c6694d135fad07425924f785b1","cfa2646776405d50533055ceb1b7f050e9014dcb","0b544dfe355a5070b60986319a3f51fb45d1348e","1eb09fecd75eb27825dce4f964b97f4f5cc399d7","0157dcd6122c20b5afc359a799b2043453471f7f","944a1cfd79dbfb6fef460360a0765ba790f4027a","39dba6f22d72853561a4ed684be265e179a39e4f","0d67362a5630ec3b7562327acc278c1c996454b5","2538e3eb24d26f31482c479d95d2e26c0e79b990","452059171226626718eb677358836328f884298e","dac72f2c509aee67524d3321f77e97e8eff51de6","1a07186bc10592f0330655519ad91652125cd907","23694a80bf1b9b38215be3e23068dd75296bc90f","38211dc39e41273c0007889202c69f841e02248a","05aba481e8a221df5d8775a3bb749001e7f2525e","0a10d64beb0931efdc24a28edaa91d539194b2e2","687bac2d3320083eb4530bf18bb8f8f721477600"],"score":0.45873561910986865},{"paper_id":"03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f","paper_title":"A Convolutional Neural Network for Modelling Sentences","paper_year":2014,"paper_venue":"ACL","paper_citations":999,"paper_abstract":"The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline.","citations":["4e88de2930a4435f737c3996287a90ff87b95c59","10a4db59e81d26b2e0e896d3186ef81b4458b93f","26e743d5bd465f49b9538deaf116c15e61b7951f","d1505c6123c102e53eb19dff312cb25cea840b72","6e795c6e9916174ae12349f5dc3f516570c17ce8","02b3d1d162080d9aefd3fc30a0bcc9a843073b5d","622b38e59cf9fe035f258964b8d6fd3238463e5e","033eb044ef6a865a53878397633876827b7a8f20"],"references":["cfa2646776405d50533055ceb1b7f050e9014dcb","944a1cfd79dbfb6fef460360a0765ba790f4027a","dac72f2c509aee67524d3321f77e97e8eff51de6","1a07186bc10592f0330655519ad91652125cd907","05aba481e8a221df5d8775a3bb749001e7f2525e","687bac2d3320083eb4530bf18bb8f8f721477600"],"score":0.585672250871091},{"paper_id":"05aba481e8a221df5d8775a3bb749001e7f2525e","paper_title":"Adaptive Subgradient Methods for Online Learning and Stochastic Optimization","paper_year":2010,"paper_venue":"COLT","paper_citations":999,"paper_abstract":"We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.","citations":["a97dc52807d80454e78d255f9fbd7b0fab56bd03","944a1cfd79dbfb6fef460360a0765ba790f4027a","209ba9f612f34583a23b75c0e8dadc410c400bb2","f37e1b62a767a307c046404ca96bc140b3e68cb5","687bac2d3320083eb4530bf18bb8f8f721477600"],"references":["38211dc39e41273c0007889202c69f841e02248a"],"score":0.5094514422606817},{"paper_id":"622b38e59cf9fe035f258964b8d6fd3238463e5e","paper_title":"Hierarchical Attention Networks for Document Classification","paper_year":2016,"paper_venue":"HLT-NAACL","paper_citations":950,"paper_abstract":"We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.","citations":["a97dc52807d80454e78d255f9fbd7b0fab56bd03","f010affab57b5fcf1cd6be23df79d8ec98c7289c"],"references":["87f40e6f3022adbc1f1905e3e506abad05a9964f","4e88de2930a4435f737c3996287a90ff87b95c59","51a55df1f023571a7e07e338ee45a3e3d66ef73e","d1505c6123c102e53eb19dff312cb25cea840b72","649d03490ef72c5274e3bccd03d7a299d2f8da91","452059171226626718eb677358836328f884298e","03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f","687bac2d3320083eb4530bf18bb8f8f721477600"],"score":0.48124582819186845},{"paper_id":"452059171226626718eb677358836328f884298e","paper_title":"Ask Me Anything: Dynamic Memory Networks for Natural Language Processing","paper_year":2015,"paper_venue":"ICML","paper_citations":504,"paper_abstract":"Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook\'s bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.","citations":["d82b55c35c8673774a708353838918346f6c006f","3febb2bed8865945e7fddc99efd791887bb7e14f","27e98e09cf09bc13c913d01676e5f32624011050","bc8fa64625d9189f5801837e7b133e7fe3c581f7","a4cec122a08216fe8a3bc19b22e78fbaea096256","622b38e59cf9fe035f258964b8d6fd3238463e5e"],"references":["50d53cc562225549457cbc782546bfbe1ac6f0cf","0b544dfe355a5070b60986319a3f51fb45d1348e","1eb09fecd75eb27825dce4f964b97f4f5cc399d7","4e88de2930a4435f737c3996287a90ff87b95c59","944a1cfd79dbfb6fef460360a0765ba790f4027a","39dba6f22d72853561a4ed684be265e179a39e4f","05aba481e8a221df5d8775a3bb749001e7f2525e","f37e1b62a767a307c046404ca96bc140b3e68cb5","0a10d64beb0931efdc24a28edaa91d539194b2e2","687bac2d3320083eb4530bf18bb8f8f721477600"],"score":0.5443015626215484}]],[[{"paper_id":"05dd7254b632376973f3a1b4d39485da17814df5","paper_title":"SQuAD: 100, 000+ Questions for Machine Comprehension of Text","paper_year":2016,"paper_venue":"EMNLP","paper_citations":926,"paper_abstract":"We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. \\nThe dataset is freely available at this https URL","citations":["007ab5528b3bd310a80d553cccad4b78dc496b02","3febb2bed8865945e7fddc99efd791887bb7e14f","27e98e09cf09bc13c913d01676e5f32624011050","3c78c6df5eb1695b6a399e346dde880af27d1016","26b47e35fe6e4260fdf7b7cc98f279a73c277494","b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f","df2b0e26d0599ce3e70df8a9da02e51594e0e992","bc8fa64625d9189f5801837e7b133e7fe3c581f7","104715e1097b7ebee436058bfd9f45540f269845","f010affab57b5fcf1cd6be23df79d8ec98c7289c","8c1b00128e74f1cd92aede3959690615695d5101","93b8da28d006415866bf48f9a6e06b5242129195"],"references":["38211dc39e41273c0007889202c69f841e02248a","d1505c6123c102e53eb19dff312cb25cea840b72"],"score":1.467397642689734},{"paper_id":"007ab5528b3bd310a80d553cccad4b78dc496b02","paper_title":"Bidirectional Attention Flow for Machine Comprehension","paper_year":2016,"paper_venue":"ICLR","paper_citations":611,"paper_abstract":"Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.","citations":["3febb2bed8865945e7fddc99efd791887bb7e14f","27e98e09cf09bc13c913d01676e5f32624011050","3c78c6df5eb1695b6a399e346dde880af27d1016","e0222a1ae6874f7fff128c3da8769ab95963da04","b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f","df2b0e26d0599ce3e70df8a9da02e51594e0e992","bc8fa64625d9189f5801837e7b133e7fe3c581f7","26b47e35fe6e4260fdf7b7cc98f279a73c277494","104715e1097b7ebee436058bfd9f45540f269845","f010affab57b5fcf1cd6be23df79d8ec98c7289c","8c1b00128e74f1cd92aede3959690615695d5101","93b8da28d006415866bf48f9a6e06b5242129195"],"references":["f37e1b62a767a307c046404ca96bc140b3e68cb5","d1505c6123c102e53eb19dff312cb25cea840b72","05dd7254b632376973f3a1b4d39485da17814df5"],"score":1.5305549544806243},{"paper_id":"f010affab57b5fcf1cd6be23df79d8ec98c7289c","paper_title":"TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension","paper_year":2017,"paper_venue":"ACL","paper_citations":214,"paper_abstract":"We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study. Data and code available at -- this http URL","citations":["3c78c6df5eb1695b6a399e346dde880af27d1016","8c1b00128e74f1cd92aede3959690615695d5101","df2b0e26d0599ce3e70df8a9da02e51594e0e992","26b47e35fe6e4260fdf7b7cc98f279a73c277494"],"references":["d1505c6123c102e53eb19dff312cb25cea840b72","05dd7254b632376973f3a1b4d39485da17814df5","622b38e59cf9fe035f258964b8d6fd3238463e5e","007ab5528b3bd310a80d553cccad4b78dc496b02"],"score":1.4657078032201585},{"paper_id":"8c1b00128e74f1cd92aede3959690615695d5101","paper_title":"QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension","paper_year":2018,"paper_venue":"ICLR","paper_citations":164,"paper_abstract":"Current end-to-end machine reading and question answering (Q\\\\&A) models are primarily based on recurrent neural networks (RNNs) with attention. Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs. We propose a new Q\\\\&A model that does not require recurrent networks: It consists exclusively of attention and convolutions, yet achieves equivalent or better performance than existing models. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference. The speed-up gain allows us to train the model with much more data. We hence combine our model with data generated by backtranslation from a neural machine translation model. This data augmentation technique not only enhances the training examples but also diversifies the phrasing of the sentences, which results in immediate accuracy improvements. Our single model achieves 84.6 F1 score on the test set, which is significantly better than the best published F1 score of 81.8.","citations":["df2b0e26d0599ce3e70df8a9da02e51594e0e992","26b47e35fe6e4260fdf7b7cc98f279a73c277494"],"references":["007ab5528b3bd310a80d553cccad4b78dc496b02","05dd7254b632376973f3a1b4d39485da17814df5","3c78c6df5eb1695b6a399e346dde880af27d1016","51a55df1f023571a7e07e338ee45a3e3d66ef73e","d1505c6123c102e53eb19dff312cb25cea840b72","b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f","93499a7c7f699b6630a86fad964536f9423bb6d0","104715e1097b7ebee436058bfd9f45540f269845","f37e1b62a767a307c046404ca96bc140b3e68cb5","f010affab57b5fcf1cd6be23df79d8ec98c7289c","204e3073870fae3d05bcbc2f6a8e263d9b72e776"],"score":1.4809313988673847},{"paper_id":"3c78c6df5eb1695b6a399e346dde880af27d1016","paper_title":"Simple and Effective Multi-Paragraph Reading Comprehension","paper_year":2017,"paper_venue":"ACL","paper_citations":96,"paper_abstract":"We consider the problem of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Our proposed solution trains models to produce well calibrated confidence scores for their results on individual paragraphs. We sample multiple paragraphs from the documents during training, and use a shared-normalization training objective that encourages the model to produce globally correct output. We combine this method with a state-of-the-art pipeline for training models on document QA data. Experiments demonstrate strong performance on several document QA datasets. Overall, we are able to achieve a score of 71.3 F1 on the web portion of TriviaQA, a large improvement from the 56.7 F1 of the previous best system.","citations":["3febb2bed8865945e7fddc99efd791887bb7e14f","27e98e09cf09bc13c913d01676e5f32624011050","df2b0e26d0599ce3e70df8a9da02e51594e0e992","8c1b00128e74f1cd92aede3959690615695d5101","26b47e35fe6e4260fdf7b7cc98f279a73c277494"],"references":["0b544dfe355a5070b60986319a3f51fb45d1348e","007ab5528b3bd310a80d553cccad4b78dc496b02","05dd7254b632376973f3a1b4d39485da17814df5","d1505c6123c102e53eb19dff312cb25cea840b72","b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f","104715e1097b7ebee436058bfd9f45540f269845","f37e1b62a767a307c046404ca96bc140b3e68cb5","f010affab57b5fcf1cd6be23df79d8ec98c7289c","39f7830cfb2436ff215892fafb6899c7d937042a"],"score":1.5653680457029548},{"paper_id":"e0222a1ae6874f7fff128c3da8769ab95963da04","paper_title":"Reinforced Mnemonic Reader for Machine Reading Comprehension","paper_year":2017,"paper_venue":"IJCAI","paper_citations":46,"paper_abstract":"In this paper, we introduce the Reinforced Mnemonic Reader for machine reading comprehension tasks, which enhances previous attentive readers in two aspects. First, a reattention mechanism is proposed to refine current attentions by directly accessing to past attentions that are temporally memorized in a multi-round alignment architecture, so as to avoid the problems of attention redundancy and attention deficiency. Second, a new optimization approach, called dynamic-critical reinforcement learning, is introduced to extend the standard supervised method. It always encourages to predict a more acceptable answer so as to address the convergence suppression problem occurred in traditional reinforcement learning algorithms. Extensive experiments on the Stanford Question Answering Dataset (SQuAD) show that our model achieves state-of-the-art results. Meanwhile, our model outperforms previous systems by over 6% in terms of both Exact Match and F1 metrics on two adversarial SQuAD datasets.","citations":["27e98e09cf09bc13c913d01676e5f32624011050","df2b0e26d0599ce3e70df8a9da02e51594e0e992"],"references":["f37e1b62a767a307c046404ca96bc140b3e68cb5","007ab5528b3bd310a80d553cccad4b78dc496b02","3febb2bed8865945e7fddc99efd791887bb7e14f","b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f","104715e1097b7ebee436058bfd9f45540f269845"],"score":1.579484157408995},{"paper_id":"26b47e35fe6e4260fdf7b7cc98f279a73c277494","paper_title":"Multi-granularity hierarchical attention fusion networks for reading comprehension and question answering","paper_year":2018,"paper_venue":"ACL","paper_citations":35,"paper_abstract":"This paper describes a novel hierarchical attention network for reading comprehension style question answering, which aims to answer questions for a given narrative paragraph. In the proposed method, attention and fusion are conducted horizontally and vertically across layers at different levels of granularity between question and paragraph. Specifically, it first encode the question and paragraph with fine-grained language embeddings, to better capture the respective representations at semantic level. Then it proposes a multi-granularity fusion approach to fully fuse information from both global and attended representations. Finally, it introduces a hierarchical attention network to focuses on the answer span progressively with multi-level softalignment. Extensive experiments on the large-scale SQuAD and TriviaQA datasets validate the effectiveness of the proposed method. At the time of writing the paper (Jan. 12th 2018), our model achieves the first position on the SQuAD leaderboard for both single and ensemble models. We also achieves state-of-the-art results on TriviaQA, AddSent and AddOne-Sent datasets.","citations":["27e98e09cf09bc13c913d01676e5f32624011050","df2b0e26d0599ce3e70df8a9da02e51594e0e992"],"references":["007ab5528b3bd310a80d553cccad4b78dc496b02","05dd7254b632376973f3a1b4d39485da17814df5","3c78c6df5eb1695b6a399e346dde880af27d1016","3febb2bed8865945e7fddc99efd791887bb7e14f","d1505c6123c102e53eb19dff312cb25cea840b72","b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f","2cd8e8f510c89c7c18268e8ad51c061e459ad321","104715e1097b7ebee436058bfd9f45540f269845","f37e1b62a767a307c046404ca96bc140b3e68cb5","f010affab57b5fcf1cd6be23df79d8ec98c7289c","8c1b00128e74f1cd92aede3959690615695d5101"],"score":1.4140483595339357},{"paper_id":"27e98e09cf09bc13c913d01676e5f32624011050","paper_title":"U-Net: Machine Reading Comprehension with Unanswerable Questions","paper_year":2018,"paper_venue":"ArXiv","paper_citations":8,"paper_abstract":"Machine reading comprehension with unanswerable questions is a new challenging task for natural language processing. A key subtask is to reliably predict whether the question is unanswerable. In this paper, we propose a unified model, called U-Net, with three important components: answer pointer, no-answer pointer, and answer verifier. We introduce a universal node and thus process the question and its context passage as a single contiguous sequence of tokens. The universal node encodes the fused information from both the question and passage, and plays an important role to predict whether the question is answerable and also greatly improves the conciseness of the U-Net. Different from the state-of-art pipeline models, U-Net can be learned in an end-to-end fashion. The experimental results on the SQuAD 2.0 dataset show that U-Net can effectively predict the unanswerability of questions and achieves an F1 score of 71.7 on SQuAD 2.0.","citations":["df2b0e26d0599ce3e70df8a9da02e51594e0e992"],"references":["007ab5528b3bd310a80d553cccad4b78dc496b02","3febb2bed8865945e7fddc99efd791887bb7e14f","3c78c6df5eb1695b6a399e346dde880af27d1016","e0222a1ae6874f7fff128c3da8769ab95963da04","d1505c6123c102e53eb19dff312cb25cea840b72","b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f","05dd7254b632376973f3a1b4d39485da17814df5","452059171226626718eb677358836328f884298e","104715e1097b7ebee436058bfd9f45540f269845","f37e1b62a767a307c046404ca96bc140b3e68cb5","26b47e35fe6e4260fdf7b7cc98f279a73c277494"],"score":1.5345412680908912}],[{"paper_id":"d1505c6123c102e53eb19dff312cb25cea840b72","paper_title":"Teaching Machines to Read and Comprehend","paper_year":2015,"paper_venue":"NIPS","paper_citations":974,"paper_abstract":"Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.","citations":["007ab5528b3bd310a80d553cccad4b78dc496b02","05dd7254b632376973f3a1b4d39485da17814df5","27e98e09cf09bc13c913d01676e5f32624011050","3c78c6df5eb1695b6a399e346dde880af27d1016","b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f","97fb4e3d45bb098e27e0071448b6152217bd35a5","cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","104715e1097b7ebee436058bfd9f45540f269845","f010affab57b5fcf1cd6be23df79d8ec98c7289c","8c1b00128e74f1cd92aede3959690615695d5101","622b38e59cf9fe035f258964b8d6fd3238463e5e","26b47e35fe6e4260fdf7b7cc98f279a73c277494"],"references":["03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f","766ce989b8b8b984f7a4691fd8c9af4bdb2b74cd","2538e3eb24d26f31482c479d95d2e26c0e79b990"],"score":0.5599526798250368},{"paper_id":"104715e1097b7ebee436058bfd9f45540f269845","paper_title":"Reading Wikipedia to Answer Open-Domain Questions","paper_year":2017,"paper_venue":"ACL","paper_citations":338,"paper_abstract":"This paper proposes to tackle open- domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.","citations":["27e98e09cf09bc13c913d01676e5f32624011050","3c78c6df5eb1695b6a399e346dde880af27d1016","e0222a1ae6874f7fff128c3da8769ab95963da04","8c1b00128e74f1cd92aede3959690615695d5101","26b47e35fe6e4260fdf7b7cc98f279a73c277494"],"references":["05dd7254b632376973f3a1b4d39485da17814df5","007ab5528b3bd310a80d553cccad4b78dc496b02","d1505c6123c102e53eb19dff312cb25cea840b72","1a07186bc10592f0330655519ad91652125cd907","f37e1b62a767a307c046404ca96bc140b3e68cb5"],"score":0.4653229320161211},{"paper_id":"b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f","paper_title":"Gated Self-Matching Networks for Reading Comprehension and Question Answering","paper_year":2017,"paper_venue":"ACL","paper_citations":245,"paper_abstract":"In this paper, we present the gated selfmatching networks for reading comprehension style question answering, which aims to answer questions from a given passage. We first match the question and passage with gated attention-based recurrent networks to obtain the question-aware passage representation. Then we propose a self-matching attention mechanism to refine the representation by matching the passage against itself, which effectively encodes information from the whole passage. We finally employ the pointer networks to locate the positions of answers from the passages. We conduct extensive experiments on the SQuAD dataset. The single model achieves 71.3% on the evaluation metrics of exact match on the hidden test set, while the ensemble model further boosts the results to 75.9%. At the time of submission of the paper, our model holds the first place on the SQuAD leaderboard for both single and ensemble model.","citations":["3febb2bed8865945e7fddc99efd791887bb7e14f","27e98e09cf09bc13c913d01676e5f32624011050","3c78c6df5eb1695b6a399e346dde880af27d1016","e0222a1ae6874f7fff128c3da8769ab95963da04","bc8fa64625d9189f5801837e7b133e7fe3c581f7","8c1b00128e74f1cd92aede3959690615695d5101","26b47e35fe6e4260fdf7b7cc98f279a73c277494"],"references":["0b544dfe355a5070b60986319a3f51fb45d1348e","007ab5528b3bd310a80d553cccad4b78dc496b02","05dd7254b632376973f3a1b4d39485da17814df5","d1505c6123c102e53eb19dff312cb25cea840b72","2cd8e8f510c89c7c18268e8ad51c061e459ad321","f37e1b62a767a307c046404ca96bc140b3e68cb5"],"score":0.521101160050022}]],[[{"paper_id":"3febb2bed8865945e7fddc99efd791887bb7e14f","paper_title":"Deep contextualized word representations","paper_year":2018,"paper_venue":"NAACL-HLT","paper_citations":999,"paper_abstract":"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pretrained network is crucial, allowing downstream models to mix different types of semi-supervision signals.","citations":["421fc2556836a6b441de806d7b393a35b6eaea58","af5c4b80fbf847f69a202ba5a780a3dd18c1a027","27e98e09cf09bc13c913d01676e5f32624011050","e0222a1ae6874f7fff128c3da8769ab95963da04","26b47e35fe6e4260fdf7b7cc98f279a73c277494","df2b0e26d0599ce3e70df8a9da02e51594e0e992","0c47cad9729c38d9db1f75491b1ee4bd883a5d4e","1e077413b25c4d34945cc2707e17e46ed4fe784a","93b8da28d006415866bf48f9a6e06b5242129195"],"references":["b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f","bc8fa64625d9189f5801837e7b133e7fe3c581f7","24158c9fc293c8a998ac552b1188404a877da292","97fb4e3d45bb098e27e0071448b6152217bd35a5","007ab5528b3bd310a80d553cccad4b78dc496b02","3c78c6df5eb1695b6a399e346dde880af27d1016","4aa9f5150b46320f534de4747a2dd0cd7f3fe292","452059171226626718eb677358836328f884298e","0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38","f37e1b62a767a307c046404ca96bc140b3e68cb5","59761abc736397539bdd01ad7f9d91c8607c0457","39f7830cfb2436ff215892fafb6899c7d937042a","033eb044ef6a865a53878397633876827b7a8f20","1eb09fecd75eb27825dce4f964b97f4f5cc399d7","10a4db59e81d26b2e0e896d3186ef81b4458b93f","02b3d1d162080d9aefd3fc30a0bcc9a843073b5d","5d833331b0e22ff359db05c62a8bca18c4f04b68","87f40e6f3022adbc1f1905e3e506abad05a9964f","f04df4e20a18358ea2f689b4c129781628ef7fc1","05dd7254b632376973f3a1b4d39485da17814df5","2538e3eb24d26f31482c479d95d2e26c0e79b990","dac72f2c509aee67524d3321f77e97e8eff51de6","10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb","8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4","687bac2d3320083eb4530bf18bb8f8f721477600"],"score":1.6462347421212167},{"paper_id":"10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb","paper_title":"Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition","paper_year":2003,"paper_venue":"CoNLL","paper_citations":789,"paper_abstract":"We describe the CoNLL-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance.","citations":["3febb2bed8865945e7fddc99efd791887bb7e14f","df2b0e26d0599ce3e70df8a9da02e51594e0e992","0c47cad9729c38d9db1f75491b1ee4bd883a5d4e","ac11062f1f368d97f4c826c317bf50dcc13fdb59","0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38","f37e1b62a767a307c046404ca96bc140b3e68cb5"],"references":[],"score":1.541773614709047},{"paper_id":"5d833331b0e22ff359db05c62a8bca18c4f04b68","paper_title":"One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling","paper_year":2013,"paper_venue":"INTERSPEECH","paper_citations":423,"paper_abstract":"We propose a new benchmark corpus to be used for measuring progress in statistical language modeling. With almost one billion words of training data, we hope this benchmark will be useful to quickly evaluate novel language modeling techniques, and to compare their contribution when combined with other advanced techniques. We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned Kneser-Ney 5-gram model achieves perplexity 67.6; a combination of techniques leads to 35% reduction in perplexity, or 10% reduction in cross-entropy (bits), over that baseline. \\nThe benchmark is available as a code.google.com project; besides the scripts needed to rebuild the training/held-out data, it also makes available log-probability values for each word in each of ten held-out data sets, for each of the baseline n-gram models.","citations":["421fc2556836a6b441de806d7b393a35b6eaea58","b9de9599d7241459db9213b5cdd7059696f5ef8d","3febb2bed8865945e7fddc99efd791887bb7e14f","df2b0e26d0599ce3e70df8a9da02e51594e0e992","0c47cad9729c38d9db1f75491b1ee4bd883a5d4e","ac11062f1f368d97f4c826c317bf50dcc13fdb59","02b3d1d162080d9aefd3fc30a0bcc9a843073b5d","0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38","93b8da28d006415866bf48f9a6e06b5242129195"],"references":[],"score":1.5954399876162502},{"paper_id":"1e077413b25c4d34945cc2707e17e46ed4fe784a","paper_title":"Universal Language Model Fine-tuning for Text Classification","paper_year":2018,"paper_venue":"ACL","paper_citations":332,"paper_abstract":"Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.","citations":["0c47cad9729c38d9db1f75491b1ee4bd883a5d4e","ac11062f1f368d97f4c826c317bf50dcc13fdb59","df2b0e26d0599ce3e70df8a9da02e51594e0e992","cd18800a0fe0b668a1cc19f2ec95b5003d0a5035"],"references":["ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c","87f40e6f3022adbc1f1905e3e506abad05a9964f","3febb2bed8865945e7fddc99efd791887bb7e14f","51a55df1f023571a7e07e338ee45a3e3d66ef73e","4aa9f5150b46320f534de4747a2dd0cd7f3fe292","bc8fa64625d9189f5801837e7b133e7fe3c581f7","649d03490ef72c5274e3bccd03d7a299d2f8da91","081651b38ff7533550a3adfc1c00da333a8fe86c","d895647b4a80861703851ef55930a2627fe19492","0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38"],"score":1.6682674994820872},{"paper_id":"bc8fa64625d9189f5801837e7b133e7fe3c581f7","paper_title":"Learned in Translation: Contextualized Word Vectors","paper_year":2017,"paper_venue":"NIPS","paper_citations":244,"paper_abstract":"Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.","citations":["3febb2bed8865945e7fddc99efd791887bb7e14f","df2b0e26d0599ce3e70df8a9da02e51594e0e992","0c47cad9729c38d9db1f75491b1ee4bd883a5d4e","ac11062f1f368d97f4c826c317bf50dcc13fdb59","cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","1e077413b25c4d34945cc2707e17e46ed4fe784a","93b8da28d006415866bf48f9a6e06b5242129195"],"references":["f04df4e20a18358ea2f689b4c129781628ef7fc1","ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c","007ab5528b3bd310a80d553cccad4b78dc496b02","05dd7254b632376973f3a1b4d39485da17814df5","26e743d5bd465f49b9538deaf116c15e61b7951f","b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f","4aa9f5150b46320f534de4747a2dd0cd7f3fe292","2cd8e8f510c89c7c18268e8ad51c061e459ad321","649d03490ef72c5274e3bccd03d7a299d2f8da91","452059171226626718eb677358836328f884298e","2538e3eb24d26f31482c479d95d2e26c0e79b990","93499a7c7f699b6630a86fad964536f9423bb6d0","6e795c6e9916174ae12349f5dc3f516570c17ce8","39dba6f22d72853561a4ed684be265e179a39e4f","38211dc39e41273c0007889202c69f841e02248a","f37e1b62a767a307c046404ca96bc140b3e68cb5","0a10d64beb0931efdc24a28edaa91d539194b2e2","687bac2d3320083eb4530bf18bb8f8f721477600"],"score":1.6340068922415818},{"paper_id":"0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38","paper_title":"Semi-supervised sequence tagging with bidirectional language models","paper_year":2017,"paper_venue":"ACL","paper_citations":145,"paper_abstract":"Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks. However, in most cases, the recurrent network that operates on word-level representations to produce context sensitive representations is trained on relatively little labeled data. In this paper, we demonstrate a general semi-supervised approach for adding pre- trained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks. We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers.","citations":["421fc2556836a6b441de806d7b393a35b6eaea58","3febb2bed8865945e7fddc99efd791887bb7e14f","df2b0e26d0599ce3e70df8a9da02e51594e0e992","0c47cad9729c38d9db1f75491b1ee4bd883a5d4e","ac11062f1f368d97f4c826c317bf50dcc13fdb59","cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","1e077413b25c4d34945cc2707e17e46ed4fe784a"],"references":["5d833331b0e22ff359db05c62a8bca18c4f04b68","1eb09fecd75eb27825dce4f964b97f4f5cc399d7","1510cf4b8abea80b9f352325ca4c132887de21a0","87f40e6f3022adbc1f1905e3e506abad05a9964f","10a4db59e81d26b2e0e896d3186ef81b4458b93f","26e743d5bd465f49b9538deaf116c15e61b7951f","944a1cfd79dbfb6fef460360a0765ba790f4027a","4aa9f5150b46320f534de4747a2dd0cd7f3fe292","2538e3eb24d26f31482c479d95d2e26c0e79b990","10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb","6e795c6e9916174ae12349f5dc3f516570c17ce8","24158c9fc293c8a998ac552b1188404a877da292","02b3d1d162080d9aefd3fc30a0bcc9a843073b5d","23694a80bf1b9b38215be3e23068dd75296bc90f","aa9efc8b2737eac0675ba5abb5feab8305482c12","8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4","f37e1b62a767a307c046404ca96bc140b3e68cb5","59761abc736397539bdd01ad7f9d91c8607c0457"],"score":1.6496022468240756},{"paper_id":"421fc2556836a6b441de806d7b393a35b6eaea58","paper_title":"Contextual String Embeddings for Sequence Labeling","paper_year":2018,"paper_venue":"COLING","paper_citations":85,"paper_abstract":"Recent advances in language modeling using recurrent neural networks have made it viable to model language as distributions over characters. By learning to predict the next character on the basis of previous characters, such models have been shown to automatically internalize linguistic concepts such as words, sentences, subclauses and even sentiment. In this paper, we propose to leverage the internal states of a trained character language model to produce a novel type of word embedding which we refer to as contextual string embeddings. Our proposed embeddings have the distinct properties that they (a) are trained without any explicit notion of words and thus fundamentally model words as sequences of characters, and (b) are contextualized by their surrounding text, meaning that the same word will have different embeddings depending on its contextual use. We conduct a comparative evaluation against previous embeddings and find that our embeddings are highly useful for downstream tasks: across four classic sequence labeling tasks we consistently outperform the previous state-of-the-art. In particular, we significantly outperform previous work on English and German named entity recognition (NER), allowing us to report new state-of-the-art F1-scores on the CONLL03 shared task. We release all code and pre-trained language models in a simple-to-use framework to the research community, to enable reproduction of these experiments and application of our proposed embeddings to other tasks: https://github.com/zalandoresearch/flair","citations":["df2b0e26d0599ce3e70df8a9da02e51594e0e992"],"references":["5d833331b0e22ff359db05c62a8bca18c4f04b68","87f40e6f3022adbc1f1905e3e506abad05a9964f","3febb2bed8865945e7fddc99efd791887bb7e14f","39dba6f22d72853561a4ed684be265e179a39e4f","f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97","24158c9fc293c8a998ac552b1188404a877da292","0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38","02b3d1d162080d9aefd3fc30a0bcc9a843073b5d","8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4","f37e1b62a767a307c046404ca96bc140b3e68cb5","033eb044ef6a865a53878397633876827b7a8f20"],"score":1.628812333589885},{"paper_id":"ac11062f1f368d97f4c826c317bf50dcc13fdb59","paper_title":"Dissecting Contextual Word Embeddings: Architecture and Representation","paper_year":2018,"paper_venue":"EMNLP","paper_citations":43,"paper_abstract":"Contextual word representations derived from pre-trained bidirectional language models (biLMs) have recently been shown to provide significant improvements to the state of the art for a wide range of NLP tasks. However, many questions remain as to how and why these models are so effective. In this paper, we present a detailed empirical study of how the choice of neural architecture (e.g. LSTM, CNN, or self attention) influences both end task accuracy and qualitative properties of the representations that are learned. We show there is a tradeoff between speed and accuracy, but all architectures learn high quality contextual representations that outperform word embeddings for four challenging NLP tasks. Additionally, all architectures learn representations that vary with network depth, from exclusively morphological based at the word embedding layer through local syntax based in the lower contextual layers to longer range semantics such coreference at the upper layers. Together, these results suggest that unsupervised biLMs, independent of architecture, are learning much more about the structure of language than previously appreciated.","citations":["df2b0e26d0599ce3e70df8a9da02e51594e0e992"],"references":["5d833331b0e22ff359db05c62a8bca18c4f04b68","5ded2b8c64491b4a67f6d39ce473d4b9347a672e","3febb2bed8865945e7fddc99efd791887bb7e14f","bc8fa64625d9189f5801837e7b133e7fe3c581f7","10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb","02b3d1d162080d9aefd3fc30a0bcc9a843073b5d","0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38","cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","f37e1b62a767a307c046404ca96bc140b3e68cb5","204e3073870fae3d05bcbc2f6a8e263d9b72e776","0a10d64beb0931efdc24a28edaa91d539194b2e2","1e077413b25c4d34945cc2707e17e46ed4fe784a","033eb044ef6a865a53878397633876827b7a8f20"],"score":1.6599169150083282},{"paper_id":"0c47cad9729c38d9db1f75491b1ee4bd883a5d4e","paper_title":"Semi-Supervised Sequence Modeling with Cross-View Training","paper_year":2018,"paper_venue":"EMNLP","paper_citations":32,"paper_abstract":"Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models, mainly because they can take advantage of large amounts of unlabeled text. However, the supervised models only learn from task-specific labeled data during the main training phase. We therefore propose Cross-View Training (CVT), a semi-supervised learning algorithm that improves the representations of a Bi-LSTM sentence encoder using a mix of labeled and unlabeled data. On labeled examples, standard supervised learning is used. On unlabeled examples, CVT teaches auxiliary prediction modules that see restricted views of the input (e.g., only part of a sentence) to match the predictions of the full model seeing the whole input. Since the auxiliary modules and the full model share intermediate representations, this in turn improves the full model. Moreover, we show that CVT is particularly effective when combined with multi-task learning. We evaluate CVT on five sequence tagging tasks, machine translation, and dependency parsing, achieving state-of-the-art results.","citations":["df2b0e26d0599ce3e70df8a9da02e51594e0e992"],"references":["bc8fa64625d9189f5801837e7b133e7fe3c581f7","1a07186bc10592f0330655519ad91652125cd907","6e795c6e9916174ae12349f5dc3f516570c17ce8","cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c","4aa9f5150b46320f534de4747a2dd0cd7f3fe292","93499a7c7f699b6630a86fad964536f9423bb6d0","0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38","f37e1b62a767a307c046404ca96bc140b3e68cb5","1e077413b25c4d34945cc2707e17e46ed4fe784a","10a4db59e81d26b2e0e896d3186ef81b4458b93f","5d833331b0e22ff359db05c62a8bca18c4f04b68","87f40e6f3022adbc1f1905e3e506abad05a9964f","26e743d5bd465f49b9538deaf116c15e61b7951f","3febb2bed8865945e7fddc99efd791887bb7e14f","39dba6f22d72853561a4ed684be265e179a39e4f","2538e3eb24d26f31482c479d95d2e26c0e79b990","10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb","8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4"],"score":1.589859849190535},{"paper_id":"f37e1b62a767a307c046404ca96bc140b3e68cb5","paper_title":"Glove: Global Vectors for Word Representation","paper_year":2014,"paper_venue":"EMNLP","paper_citations":999,"paper_abstract":"Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.","citations":["10a4db59e81d26b2e0e896d3186ef81b4458b93f","452059171226626718eb677358836328f884298e","af5c4b80fbf847f69a202ba5a780a3dd18c1a027"],"references":["3cfbb77e5a0e24772cfdb2eb3d4f35dead54b118","87f40e6f3022adbc1f1905e3e506abad05a9964f","0d67362a5630ec3b7562327acc278c1c996454b5","2538e3eb24d26f31482c479d95d2e26c0e79b990","dac72f2c509aee67524d3321f77e97e8eff51de6","1a07186bc10592f0330655519ad91652125cd907","10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb","209ba9f612f34583a23b75c0e8dadc410c400bb2","23694a80bf1b9b38215be3e23068dd75296bc90f","05aba481e8a221df5d8775a3bb749001e7f2525e","c4fd9c86b2b41df51a6fe212406dda81b1997fd4","0a10d64beb0931efdc24a28edaa91d539194b2e2"],"score":1.5993293473379713}],[{"paper_id":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","paper_title":"Improving Language Understanding by Generative Pre-Training","paper_year":2018,"paper_venue":"","paper_citations":377,"paper_abstract":"Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).","citations":["0c47cad9729c38d9db1f75491b1ee4bd883a5d4e","ac11062f1f368d97f4c826c317bf50dcc13fdb59"],"references":["a97dc52807d80454e78d255f9fbd7b0fab56bd03","843959ffdccf31c6694d135fad07425924f785b1","bc8fa64625d9189f5801837e7b133e7fe3c581f7","1a07186bc10592f0330655519ad91652125cd907","6e795c6e9916174ae12349f5dc3f516570c17ce8","97fb4e3d45bb098e27e0071448b6152217bd35a5","93b8da28d006415866bf48f9a6e06b5242129195","ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c","d1505c6123c102e53eb19dff312cb25cea840b72","4aa9f5150b46320f534de4747a2dd0cd7f3fe292","0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38","f37e1b62a767a307c046404ca96bc140b3e68cb5","bc1d609520290e0460c49b685675eb5a57fa5935","1e077413b25c4d34945cc2707e17e46ed4fe784a","a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096","204e3073870fae3d05bcbc2f6a8e263d9b72e776","1510cf4b8abea80b9f352325ca4c132887de21a0","0e6824e137847be0599bb0032e37042ed2ef5045","f04df4e20a18358ea2f689b4c129781628ef7fc1","4361e64f2d12d63476fdc88faf72a0f70d9a2ffb","87f40e6f3022adbc1f1905e3e506abad05a9964f","3febb2bed8865945e7fddc99efd791887bb7e14f","2538e3eb24d26f31482c479d95d2e26c0e79b990","5ded2b8c64491b4a67f6d39ce473d4b9347a672e","475354f10798f110d34792b6d88f31d6d5cb099e","687bac2d3320083eb4530bf18bb8f8f721477600"],"score":0.6555204365138089},{"paper_id":"2538e3eb24d26f31482c479d95d2e26c0e79b990","paper_title":"Natural Language Processing (almost) from Scratch","paper_year":2011,"paper_venue":"J. Mach. Learn. Res.","paper_citations":999,"paper_abstract":"We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.","citations":["3febb2bed8865945e7fddc99efd791887bb7e14f","0c47cad9729c38d9db1f75491b1ee4bd883a5d4e","0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38"],"references":["aa9efc8b2737eac0675ba5abb5feab8305482c12","783480acff435bfbc15ffcdb4f15eccddaa0c810","2c5135a0531bc5ad7dd890f018e67a40529f5bcb","dac72f2c509aee67524d3321f77e97e8eff51de6"],"score":0.7319141196065042},{"paper_id":"24158c9fc293c8a998ac552b1188404a877da292","paper_title":"Neural Architectures for Named Entity Recognition","paper_year":2016,"paper_venue":"HLT-NAACL","paper_citations":972,"paper_abstract":"State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available. In this paper, we introduce two new neural architectures---one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers. Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora. Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers.","citations":["421fc2556836a6b441de806d7b393a35b6eaea58","3febb2bed8865945e7fddc99efd791887bb7e14f","0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38","8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4","59761abc736397539bdd01ad7f9d91c8607c0457"],"references":["10a4db59e81d26b2e0e896d3186ef81b4458b93f","8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4"],"score":0.6827995225115789},{"paper_id":"8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4","paper_title":"End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF","paper_year":2016,"paper_venue":"ACL","paper_citations":619,"paper_abstract":"State-of-the-art sequence labeling systems traditionally require large amounts of taskspecific knowledge in the form of handcrafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data preprocessing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks \u2014 Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both datasets \u2014 97.55% accuracy for POS tagging and 91.21% F1 for NER.","citations":["421fc2556836a6b441de806d7b393a35b6eaea58","3febb2bed8865945e7fddc99efd791887bb7e14f","0c47cad9729c38d9db1f75491b1ee4bd883a5d4e","24158c9fc293c8a998ac552b1188404a877da292","0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38"],"references":["1eb09fecd75eb27825dce4f964b97f4f5cc399d7","87f40e6f3022adbc1f1905e3e506abad05a9964f","10a4db59e81d26b2e0e896d3186ef81b4458b93f","24158c9fc293c8a998ac552b1188404a877da292","aa9efc8b2737eac0675ba5abb5feab8305482c12","f37e1b62a767a307c046404ca96bc140b3e68cb5"],"score":0.5760829867862983},{"paper_id":"10a4db59e81d26b2e0e896d3186ef81b4458b93f","paper_title":"Named Entity Recognition with Bidirectional LSTM-CNNs","paper_year":2015,"paper_venue":"Transactions of the Association for Computational Linguistics","paper_citations":433,"paper_abstract":"Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form of feature engineering and lexicons to achieve high performance. In this paper, we present a novel neural network architecture that automatically detects word- and character-level features using a hybrid bidirectional LSTM and CNN architecture, eliminating the need for most feature engineering. We also propose a novel method of encoding partial lexicon matches in neural networks and compare it to existing approaches. Extensive evaluation shows that, given only tokenized text and publicly available word embeddings, our system is competitive on the CoNLL-2003 dataset and surpasses the previously reported state of the art performance on the OntoNotes 5.0 dataset by 2.13 F1 points. By using two lexicons constructed from publicly-available sources, we establish new state of the art performance with an F1 score of 91.62 on CoNLL-2003 and 86.28 on OntoNotes, surpassing systems that employ heavy feature engineering, proprietary lexicons, and rich entity linking information.","citations":["3febb2bed8865945e7fddc99efd791887bb7e14f","0c47cad9729c38d9db1f75491b1ee4bd883a5d4e","24158c9fc293c8a998ac552b1188404a877da292","0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38","8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4"],"references":["1eb09fecd75eb27825dce4f964b97f4f5cc399d7","87f40e6f3022adbc1f1905e3e506abad05a9964f","2538e3eb24d26f31482c479d95d2e26c0e79b990","dac72f2c509aee67524d3321f77e97e8eff51de6","1a07186bc10592f0330655519ad91652125cd907","aa9efc8b2737eac0675ba5abb5feab8305482c12","f37e1b62a767a307c046404ca96bc140b3e68cb5","03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f"],"score":0.596796942552177},{"paper_id":"aa9efc8b2737eac0675ba5abb5feab8305482c12","paper_title":"Design Challenges and Misconceptions in Named Entity Recognition","paper_year":2009,"paper_venue":"CoNLL","paper_citations":821,"paper_abstract":"We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset.","citations":["10a4db59e81d26b2e0e896d3186ef81b4458b93f","2538e3eb24d26f31482c479d95d2e26c0e79b990","dac72f2c509aee67524d3321f77e97e8eff51de6","0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38","8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4"],"references":["783480acff435bfbc15ffcdb4f15eccddaa0c810"],"score":0.598676407550967}]],[[{"paper_id":"783480acff435bfbc15ffcdb4f15eccddaa0c810","paper_title":"Class-Based n-gram Models of Natural Language","paper_year":1992,"paper_venue":"Computational Linguistics","paper_citations":999,"paper_abstract":"We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics.","citations":["9fa8d73e572c3ca824a04a5f551b602a17831bc5","209ba9f612f34583a23b75c0e8dadc410c400bb2","df2b0e26d0599ce3e70df8a9da02e51594e0e992","dac72f2c509aee67524d3321f77e97e8eff51de6"],"references":[],"score":1.4818012861598482},{"paper_id":"dac72f2c509aee67524d3321f77e97e8eff51de6","paper_title":"Word Representations: A Simple and General Method for Semi-Supervised Learning","paper_year":2010,"paper_venue":"ACL","paper_citations":999,"paper_abstract":"If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize.com/projects/wordreprs/","citations":["59761abc736397539bdd01ad7f9d91c8607c0457","3cfbb77e5a0e24772cfdb2eb3d4f35dead54b118","0183b3e9d84c15c7048e6c2149ed86257ccdc6cb","87f40e6f3022adbc1f1905e3e506abad05a9964f","4e88de2930a4435f737c3996287a90ff87b95c59","3febb2bed8865945e7fddc99efd791887bb7e14f","0157dcd6122c20b5afc359a799b2043453471f7f","10a4db59e81d26b2e0e896d3186ef81b4458b93f","3ed9b9e2972e429cbba3d9f8a5edff2f4f8619fe","df2b0e26d0599ce3e70df8a9da02e51594e0e992","649d03490ef72c5274e3bccd03d7a299d2f8da91","2538e3eb24d26f31482c479d95d2e26c0e79b990","f37e1b62a767a307c046404ca96bc140b3e68cb5","c4fd9c86b2b41df51a6fe212406dda81b1997fd4","03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f"],"references":["3a0e788268fafb23ab20da0e98bb578b06830f7d","1005645c05585c2042e3410daeed638b55e2474d","783480acff435bfbc15ffcdb4f15eccddaa0c810","1a07186bc10592f0330655519ad91652125cd907","23694a80bf1b9b38215be3e23068dd75296bc90f","aa9efc8b2737eac0675ba5abb5feab8305482c12"],"score":1.6037113737336153},{"paper_id":"1005645c05585c2042e3410daeed638b55e2474d","paper_title":"A Scalable Hierarchical Distributed Language Model","paper_year":2008,"paper_venue":"NIPS","paper_citations":591,"paper_abstract":"Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the non-hierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models.","citations":["0183b3e9d84c15c7048e6c2149ed86257ccdc6cb","87f40e6f3022adbc1f1905e3e506abad05a9964f","0157dcd6122c20b5afc359a799b2043453471f7f","3ed9b9e2972e429cbba3d9f8a5edff2f4f8619fe","df2b0e26d0599ce3e70df8a9da02e51594e0e992","0d67362a5630ec3b7562327acc278c1c996454b5","dac72f2c509aee67524d3321f77e97e8eff51de6","a4cec122a08216fe8a3bc19b22e78fbaea096256","02b3d1d162080d9aefd3fc30a0bcc9a843073b5d","bc1d609520290e0460c49b685675eb5a57fa5935","1510cf4b8abea80b9f352325ca4c132887de21a0","2b669398c4cf2ebe04375c8b1beae20f4ac802fa","c4fd9c86b2b41df51a6fe212406dda81b1997fd4"],"references":["783480acff435bfbc15ffcdb4f15eccddaa0c810","23694a80bf1b9b38215be3e23068dd75296bc90f"],"score":1.5514170617463914},{"paper_id":"59761abc736397539bdd01ad7f9d91c8607c0457","paper_title":"context2vec: Learning Generic Context Embedding with Bidirectional LSTM","paper_year":2016,"paper_venue":"CoNLL","paper_citations":124,"paper_abstract":"Context representations are central to various NLP tasks, such as word sense disambiguation, named entity recognition, coreference resolution, and many more. In this work we present a neural model for efficiently learning a generic context embedding function from large corpora, using bidirectional LSTM. With a very simple application of our context representations, we manage to surpass or nearly reach state-of-the-art results on sentence completion, lexical substitution and word sense disambiguation tasks, while substantially outperforming the popular context representation of averaged word embeddings. We release our code and pretrained models, suggesting they could be useful in a wide variety of NLP tasks.","citations":["3febb2bed8865945e7fddc99efd791887bb7e14f","df2b0e26d0599ce3e70df8a9da02e51594e0e992","0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38"],"references":["0183b3e9d84c15c7048e6c2149ed86257ccdc6cb","cfa2646776405d50533055ceb1b7f050e9014dcb","87f40e6f3022adbc1f1905e3e506abad05a9964f","3ed9b9e2972e429cbba3d9f8a5edff2f4f8619fe","dac72f2c509aee67524d3321f77e97e8eff51de6","6e795c6e9916174ae12349f5dc3f516570c17ce8","24158c9fc293c8a998ac552b1188404a877da292","02b3d1d162080d9aefd3fc30a0bcc9a843073b5d","2b669398c4cf2ebe04375c8b1beae20f4ac802fa","f37e1b62a767a307c046404ca96bc140b3e68cb5","0a10d64beb0931efdc24a28edaa91d539194b2e2"],"score":1.6265571860179926},{"paper_id":"1a07186bc10592f0330655519ad91652125cd907","paper_title":"A unified architecture for natural language processing: deep neural networks with multitask learning","paper_year":2008,"paper_venue":"ICML","paper_citations":999,"paper_abstract":"We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.","citations":["3cfbb77e5a0e24772cfdb2eb3d4f35dead54b118","0183b3e9d84c15c7048e6c2149ed86257ccdc6cb","50d53cc562225549457cbc782546bfbe1ac6f0cf","892e53fe5cd39f037cb2a961499f42f3002595dd","dac72f2c509aee67524d3321f77e97e8eff51de6","0c47cad9729c38d9db1f75491b1ee4bd883a5d4e","a4cec122a08216fe8a3bc19b22e78fbaea096256","209ba9f612f34583a23b75c0e8dadc410c400bb2","687bac2d3320083eb4530bf18bb8f8f721477600"],"references":["2c5135a0531bc5ad7dd890f018e67a40529f5bcb","23694a80bf1b9b38215be3e23068dd75296bc90f"],"score":1.5993521557049024},{"paper_id":"87f40e6f3022adbc1f1905e3e506abad05a9964f","paper_title":"Distributed Representations of Words and Phrases and their Compositionality","paper_year":2013,"paper_venue":"NIPS","paper_citations":999,"paper_abstract":"The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \\"Canada\\" and \\"Air\\" cannot be easily combined to obtain \\"Air Canada\\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.","citations":["0c47cad9729c38d9db1f75491b1ee4bd883a5d4e","622b38e59cf9fe035f258964b8d6fd3238463e5e"],"references":["dac72f2c509aee67524d3321f77e97e8eff51de6","1005645c05585c2042e3410daeed638b55e2474d","23694a80bf1b9b38215be3e23068dd75296bc90f","1a07186bc10592f0330655519ad91652125cd907"],"score":1.5466990342247593}],[{"paper_id":"c4fd9c86b2b41df51a6fe212406dda81b1997fd4","paper_title":"Linguistic Regularities in Continuous Space Word Representations","paper_year":2013,"paper_venue":"HLT-NAACL","paper_citations":999,"paper_abstract":"Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, \u201cKing Man + Woman\u201d results in a vector very close to \u201cQueen.\u201d We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems.","citations":["0183b3e9d84c15c7048e6c2149ed86257ccdc6cb","3cfbb77e5a0e24772cfdb2eb3d4f35dead54b118","0157dcd6122c20b5afc359a799b2043453471f7f","3ed9b9e2972e429cbba3d9f8a5edff2f4f8619fe","6aa3d8bcca2ebdc52ef7cd786204c338f9d609f2","209ba9f612f34583a23b75c0e8dadc410c400bb2","1510cf4b8abea80b9f352325ca4c132887de21a0","f37e1b62a767a307c046404ca96bc140b3e68cb5","bc1d609520290e0460c49b685675eb5a57fa5935"],"references":["23694a80bf1b9b38215be3e23068dd75296bc90f","1005645c05585c2042e3410daeed638b55e2474d","dac72f2c509aee67524d3321f77e97e8eff51de6","1a07186bc10592f0330655519ad91652125cd907"],"score":0.60919455472756},{"paper_id":"cfa2646776405d50533055ceb1b7f050e9014dcb","paper_title":"Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions","paper_year":2011,"paper_venue":"EMNLP","paper_citations":793,"paper_abstract":"We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model\'s ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.","citations":["0183b3e9d84c15c7048e6c2149ed86257ccdc6cb","204a4a70428f3938d2c538a4d74c7ae0416306d8","26e743d5bd465f49b9538deaf116c15e61b7951f","a4cec122a08216fe8a3bc19b22e78fbaea096256","1510cf4b8abea80b9f352325ca4c132887de21a0","2b669398c4cf2ebe04375c8b1beae20f4ac802fa","59761abc736397539bdd01ad7f9d91c8607c0457","03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f","687bac2d3320083eb4530bf18bb8f8f721477600"],"references":["dac72f2c509aee67524d3321f77e97e8eff51de6","1a07186bc10592f0330655519ad91652125cd907","23694a80bf1b9b38215be3e23068dd75296bc90f"],"score":0.5592927261760056},{"paper_id":"2b669398c4cf2ebe04375c8b1beae20f4ac802fa","paper_title":"Improving Word Representations via Global Context and Multiple Word Prototypes","paper_year":2012,"paper_venue":"ACL","paper_citations":784,"paper_abstract":"Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one representation per word. This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models.","citations":["3cfbb77e5a0e24772cfdb2eb3d4f35dead54b118","50d53cc562225549457cbc782546bfbe1ac6f0cf","4e88de2930a4435f737c3996287a90ff87b95c59","0157dcd6122c20b5afc359a799b2043453471f7f","209ba9f612f34583a23b75c0e8dadc410c400bb2","1510cf4b8abea80b9f352325ca4c132887de21a0","59761abc736397539bdd01ad7f9d91c8607c0457","687bac2d3320083eb4530bf18bb8f8f721477600"],"references":["1005645c05585c2042e3410daeed638b55e2474d","cfa2646776405d50533055ceb1b7f050e9014dcb","dac72f2c509aee67524d3321f77e97e8eff51de6","1a07186bc10592f0330655519ad91652125cd907","23694a80bf1b9b38215be3e23068dd75296bc90f"],"score":0.6164008645105562},{"paper_id":"3ed9b9e2972e429cbba3d9f8a5edff2f4f8619fe","paper_title":"Neural Word Embedding as Implicit Matrix Factorization","paper_year":2014,"paper_venue":"NIPS","paper_citations":771,"paper_abstract":"We analyze skip-gram with negative-sampling (SGNS), a word embedding method introduced by Mikolov et al., and show that it is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual information (PMI) of the respective word and context pairs, shifted by a global constant. We find that another embedding method, NCE, is implicitly factorizing a similar matrix, where each cell is the (shifted) log conditional probability of a word given its context. We show that using a sparse Shifted Positive PMI word-context matrix to represent words improves results on two word similarity tasks and one of two analogy tasks. When dense low-dimensional vectors are preferred, exact factorization with SVD can achieve solutions that are at least as good as SGNS\'s solutions for word similarity tasks. On analogy questions SGNS remains superior to SVD. We conjecture that this stems from the weighted nature of SGNS\'s factorization.","citations":["26e743d5bd465f49b9538deaf116c15e61b7951f","59761abc736397539bdd01ad7f9d91c8607c0457","6aa3d8bcca2ebdc52ef7cd786204c338f9d609f2"],"references":["3a0e788268fafb23ab20da0e98bb578b06830f7d","1005645c05585c2042e3410daeed638b55e2474d","0183b3e9d84c15c7048e6c2149ed86257ccdc6cb","3cfbb77e5a0e24772cfdb2eb3d4f35dead54b118","87f40e6f3022adbc1f1905e3e506abad05a9964f","2538e3eb24d26f31482c479d95d2e26c0e79b990","dac72f2c509aee67524d3321f77e97e8eff51de6","1a07186bc10592f0330655519ad91652125cd907","23694a80bf1b9b38215be3e23068dd75296bc90f","c4fd9c86b2b41df51a6fe212406dda81b1997fd4","0a10d64beb0931efdc24a28edaa91d539194b2e2"],"score":0.4346848905007367},{"paper_id":"3a0e788268fafb23ab20da0e98bb578b06830f7d","paper_title":"From Frequency to Meaning: Vector Space Models of Semantics","paper_year":2010,"paper_venue":"J. Artif. Intell. Res.","paper_citations":999,"paper_abstract":"Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.","citations":["0183b3e9d84c15c7048e6c2149ed86257ccdc6cb","3ed9b9e2972e429cbba3d9f8a5edff2f4f8619fe","6aa3d8bcca2ebdc52ef7cd786204c338f9d609f2","209ba9f612f34583a23b75c0e8dadc410c400bb2","1510cf4b8abea80b9f352325ca4c132887de21a0","687bac2d3320083eb4530bf18bb8f8f721477600"],"references":["1a07186bc10592f0330655519ad91652125cd907","2c5135a0531bc5ad7dd890f018e67a40529f5bcb"],"score":0.434157978451},{"paper_id":"0157dcd6122c20b5afc359a799b2043453471f7f","paper_title":"Exploiting Similarities among Languages for Machine Translation","paper_year":2013,"paper_venue":"ArXiv","paper_citations":649,"paper_abstract":"Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs.","citations":["1510cf4b8abea80b9f352325ca4c132887de21a0","3cfbb77e5a0e24772cfdb2eb3d4f35dead54b118","6e795c6e9916174ae12349f5dc3f516570c17ce8","a4cec122a08216fe8a3bc19b22e78fbaea096256"],"references":["1005645c05585c2042e3410daeed638b55e2474d","87f40e6f3022adbc1f1905e3e506abad05a9964f","dac72f2c509aee67524d3321f77e97e8eff51de6","1a07186bc10592f0330655519ad91652125cd907","209ba9f612f34583a23b75c0e8dadc410c400bb2","23694a80bf1b9b38215be3e23068dd75296bc90f","2b669398c4cf2ebe04375c8b1beae20f4ac802fa","c4fd9c86b2b41df51a6fe212406dda81b1997fd4"],"score":0.5375225633769497},{"paper_id":"3cfbb77e5a0e24772cfdb2eb3d4f35dead54b118","paper_title":"Don\'t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors","paper_year":2014,"paper_venue":"ACL","paper_citations":736,"paper_abstract":"Context-predicting models (more commonly known as embeddings or neural language models) are the new kids on the distributional semantics block. Despite the buzz surrounding these models, the literature is still lacking a systematic comparison of the predictive models with classic, count-vector-based distributional semantic approaches. In this paper, we perform such an extensive evaluation, on a wide range of lexical semantics tasks and across many parameter settings. The results, to our own surprise, show that the buzz is fully justified, as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts.","citations":["26e743d5bd465f49b9538deaf116c15e61b7951f","f37e1b62a767a307c046404ca96bc140b3e68cb5","3ed9b9e2972e429cbba3d9f8a5edff2f4f8619fe","6aa3d8bcca2ebdc52ef7cd786204c338f9d609f2"],"references":["3a0e788268fafb23ab20da0e98bb578b06830f7d","87f40e6f3022adbc1f1905e3e506abad05a9964f","0157dcd6122c20b5afc359a799b2043453471f7f","2538e3eb24d26f31482c479d95d2e26c0e79b990","dac72f2c509aee67524d3321f77e97e8eff51de6","1a07186bc10592f0330655519ad91652125cd907","23694a80bf1b9b38215be3e23068dd75296bc90f","2b669398c4cf2ebe04375c8b1beae20f4ac802fa","c4fd9c86b2b41df51a6fe212406dda81b1997fd4","0a10d64beb0931efdc24a28edaa91d539194b2e2"],"score":0.5518945847927526},{"paper_id":"0183b3e9d84c15c7048e6c2149ed86257ccdc6cb","paper_title":"Dependency-Based Word Embeddings","paper_year":2014,"paper_venue":"ACL","paper_citations":567,"paper_abstract":"While continuous word embeddings are gaining popularity, current models are based solely on linear contexts. In this work, we generalize the skip-gram model with negative sampling introduced by Mikolov et al. to include arbitrary contexts. In particular, we perform experiments with dependency-based contexts, and show that they produce markedly different embeddings. The dependencybased embeddings are less topical and exhibit more functional similarity than the original skip-gram embeddings.","citations":["3ed9b9e2972e429cbba3d9f8a5edff2f4f8619fe","59761abc736397539bdd01ad7f9d91c8607c0457","6aa3d8bcca2ebdc52ef7cd786204c338f9d609f2"],"references":["3a0e788268fafb23ab20da0e98bb578b06830f7d","1005645c05585c2042e3410daeed638b55e2474d","cfa2646776405d50533055ceb1b7f050e9014dcb","87f40e6f3022adbc1f1905e3e506abad05a9964f","783480acff435bfbc15ffcdb4f15eccddaa0c810","2538e3eb24d26f31482c479d95d2e26c0e79b990","dac72f2c509aee67524d3321f77e97e8eff51de6","1a07186bc10592f0330655519ad91652125cd907","23694a80bf1b9b38215be3e23068dd75296bc90f","c4fd9c86b2b41df51a6fe212406dda81b1997fd4","0a10d64beb0931efdc24a28edaa91d539194b2e2"],"score":0.4334292803112526},{"paper_id":"0a10d64beb0931efdc24a28edaa91d539194b2e2","paper_title":"Efficient Estimation of Word Representations in Vector Space","paper_year":2013,"paper_venue":"ICLR","paper_citations":999,"paper_abstract":"We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.","citations":["0e6824e137847be0599bb0032e37042ed2ef5045","ac11062f1f368d97f4c826c317bf50dcc13fdb59","a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096","204a4a70428f3938d2c538a4d74c7ae0416306d8"],"references":["23694a80bf1b9b38215be3e23068dd75296bc90f"],"score":0.5631092812946097},{"paper_id":"23694a80bf1b9b38215be3e23068dd75296bc90f","paper_title":"A Neural Probabilistic Language Model","paper_year":2000,"paper_venue":"NIPS","paper_citations":999,"paper_abstract":"A goal of statistical language modeling is to learn the joint probability function of sequences of words. This is intrinsically difficult because of the curse of dimensionality: we propose to fight it with its own weapons. In the proposed approach one learns simultaneously (1) a distributed representation for each word (i.e. a similarity between words) along with (2) the probability function for word sequences, expressed with these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar to words forming an already seen sentence. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach very significantly improves on a state-of-the-art trigram model.","citations":["87f40e6f3022adbc1f1905e3e506abad05a9964f","cfa2646776405d50533055ceb1b7f050e9014dcb","0157dcd6122c20b5afc359a799b2043453471f7f","3ed9b9e2972e429cbba3d9f8a5edff2f4f8619fe","0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38","c4fd9c86b2b41df51a6fe212406dda81b1997fd4"],"references":["783480acff435bfbc15ffcdb4f15eccddaa0c810"],"score":0.5467084773113217},{"paper_id":"6aa3d8bcca2ebdc52ef7cd786204c338f9d609f2","paper_title":"Improving Distributional Similarity with Lessons Learned from Word Embeddings","paper_year":2015,"paper_venue":"Transactions of the Association for Computational Linguistics","paper_citations":628,"paper_abstract":"Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distributional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter optimizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.","citations":["26e743d5bd465f49b9538deaf116c15e61b7951f"],"references":["3a0e788268fafb23ab20da0e98bb578b06830f7d","0183b3e9d84c15c7048e6c2149ed86257ccdc6cb","3cfbb77e5a0e24772cfdb2eb3d4f35dead54b118","87f40e6f3022adbc1f1905e3e506abad05a9964f","3ed9b9e2972e429cbba3d9f8a5edff2f4f8619fe","1a07186bc10592f0330655519ad91652125cd907","23694a80bf1b9b38215be3e23068dd75296bc90f","f37e1b62a767a307c046404ca96bc140b3e68cb5","c4fd9c86b2b41df51a6fe212406dda81b1997fd4","0a10d64beb0931efdc24a28edaa91d539194b2e2"],"score":0.5069075183529714},{"paper_id":"209ba9f612f34583a23b75c0e8dadc410c400bb2","paper_title":"Parsing with Compositional Vector Grammars","paper_year":2013,"paper_venue":"ACL","paper_citations":604,"paper_abstract":"Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness. Instead, we introduce a Compositional Vector Grammar (CVG), which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations. The CVG improves the PCFG of the Stanford Parser by 3.8% to obtain an F1 score of 90.4%. It is fast to train and implemented approximately as an efficient reranker it is about 20% faster than the current Stanford factored parser. The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments.","citations":["0157dcd6122c20b5afc359a799b2043453471f7f","f37e1b62a767a307c046404ca96bc140b3e68cb5"],"references":["3a0e788268fafb23ab20da0e98bb578b06830f7d","783480acff435bfbc15ffcdb4f15eccddaa0c810","dac72f2c509aee67524d3321f77e97e8eff51de6","1a07186bc10592f0330655519ad91652125cd907","23694a80bf1b9b38215be3e23068dd75296bc90f","2b669398c4cf2ebe04375c8b1beae20f4ac802fa","05aba481e8a221df5d8775a3bb749001e7f2525e","c4fd9c86b2b41df51a6fe212406dda81b1997fd4"],"score":0.5758993500219718}]]],"clusterNames":["natural language inference","neural network","deep learning","question answering","entity recognition","word representation"],"importance":[0.4040927166733688,0.07646917823086252,0.05177757291451488,0.1494846351717006,0.20133111879366866,0.11684477821588447]}')},148:function(e){e.exports=JSON.parse('{"zh":{"move":"\u79fb\u52a8","like":"\u8d5e","dislike":"\u8e29","citation":"\u5f15\u7528","influence":"\u5f71\u54cd"}}')},164:function(e){e.exports=JSON.parse('{"zh":{"like":"\u559c\u6b22","dislike":"\u4e0d\u559c\u6b22","share":"\u5206\u4eab","qr code":"\u4e8c\u7ef4\u7801","font size":"\u5b57\u4f53\u5927\u5c0f","larger font":"\u5b57\u4f53\u589e\u5927","smaller font":"\u5b57\u4f53\u51cf\u5c0f","zoom":"\u89c6\u56fe","zoom in":"\u653e\u5927","zoom out":"\u7f29\u5c0f","download":"\u4e0b\u8f7d","full picture":"\u539f\u56fe","snapshot":"\u5feb\u7167","control":"\u63a7\u5236","display sub branch":"\u663e\u793a\u6b21\u7ea7\u5206\u652f","hide sub branch":"\u9690\u85cf\u6b21\u7ea7\u5206\u652f","enable text span":"\u542f\u7528\u6587\u672c\u8de8\u8d8a","disable text span":"\u7981\u7528\u6587\u672c\u8de8\u8d8a","load JSON":"\u52a0\u8f7dJSON","toolbox":"\u5de5\u5177\u7bb1"}}')},166:function(e,a,t){e.exports=t(327)},171:function(e,a,t){},172:function(e,a,t){},173:function(e,a,t){},178:function(e,a){},180:function(e,a){},215:function(e,a){},216:function(e,a){},260:function(e,a,t){},261:function(e,a,t){},282:function(e,a,t){},283:function(e,a,t){},327:function(e,a,t){"use strict";t.r(a);var n=t(0),r=t.n(n),i=t(146),o=t.n(i),c=(t(171),t(6)),s=t(7),d=t(9),f=t(8),b=t(10),l=(t(172),t(147)),p=t(45),h=t(92),u=t(151),m=t(153),g=t(150),y=t(152),v=t(149),w=t(154),x=t(155),_=t(3),k=t.n(_),T=(t(173),t(148)),S=t(11),L=t.n(S),E=k()("green").luminance(.3).desaturate(1),N=k()("red").luminance(.3).desaturate(2),C=k()("blue").luminance(.3).desaturate(1),W=k()("orange").luminance(.3),M=k()("purple").luminance(.3),A=k()("grey").luminance(.3),z=k()("grey").luminance(.1),I=function(e){function a(e){var t;return Object(c.a)(this,a),(t=Object(d.a)(this,Object(f.a)(a).call(this,e))).state={hover:!1},t}return Object(b.a)(a,e),Object(s.a)(a,[{key:"onHover",value:function(e){this.setState({hover:e}),this.props.onHover&&this.props.onHover(e)}},{key:"render",value:function(){var e=this;return r.a.createElement("circle",{className:"era-node-circle",cx:this.props.node.x,cy:this.props.node.y,r:this.props.radius,onMouseOver:function(){e.onHover(!0)},onMouseLeave:function(){e.onHover(!1)},stroke:this.props.node.color,strokeWidth:this.props.strokeWidth,fill:this.state.hover?this.props.node.color:"white"})}}]),a}(r.a.Component),q=function(e){function a(e){var t;return Object(c.a)(this,a),(t=Object(d.a)(this,Object(f.a)(a).call(this,e))).state={expand:-1},t}return Object(b.a)(a,e),Object(s.a)(a,[{key:"onHover",value:function(e){e||-1===this.state.expand||this.setState({expand:-1})}},{key:"render",value:function(){var e=this,a=this.props.lang||"en",t=T[a],n=function(e){return t&&t[e.toLowerCase()]?t[e.toLowerCase()]:e},i=this.props.node.textColor,o=1.25*this.props.lineHeight,c=this.props.pins.map((function(a,t){var c=e.state.expand===t,s=c?a.fullTextPieces:a.textPieces,d=a.abstractPieces.length*e.props.secondaryLineHeight,f=(s.length-1)*e.props.lineHeight+e.props.editButtonMarginTop+c*d,b=c?e.props.fullTextWidth:e.props.textWidth,l=function(t,i,c,s,d){return r.a.createElement("g",{transform:"translate(".concat(b-o*i,", ").concat(f,")")},r.a.createElement("g",{className:"paper-edit-icon",style:{transformOrigin:"".concat(o/2,"px ").concat(o/2,"px")},onClick:"link-switch"===s?function(){return e.props.onSwitchLinksVisibility(a.id)}:function(){return e.props.onEdit(s,a)}},r.a.createElement(t,{className:"paper-edit-icon",fill:c,width:o,height:o}),r.a.createElement("rect",{className:"paper-edit-icon",width:o,height:o,fill:"transparent"}),r.a.createElement("text",{textAnchor:"middle",x:o/2,y:o+e.props.secondaryLineHeight/2,fill:c,fontSize:e.props.secondaryLineHeight/2},n(d))))},p=a.edits&&a.edits.rate>0,h=a.edits&&a.edits.rate<0,_="left"===e.props.scaleOrigin?0:"middle"===e.props.scaleOrigin?b/2:b;return r.a.createElement("g",{key:t,transform:"translate(".concat(e.props.textLeadingMargin+e.props.radius,", ").concat(a.y-e.props.node.y,")")},r.a.createElement("g",{className:"paper-view-group-inner",style:{transformOrigin:"".concat(_,"px ").concat(-e.props.lineHeight,"px")},onMouseOver:function(){return e.onHover(!0)},onMouseLeave:function(){return e.onHover(!1)}},r.a.createElement("rect",{className:"paper-text-background",x:-e.props.lineHeight,y:2.5*-e.props.lineHeight,width:b+2*e.props.lineHeight,height:4*e.props.lineHeight+f+o,fill:"white",filter:"url(#blur-filter)"}),r.a.createElement("text",{className:"paper-text",fontSize:e.props.fontSize,fill:i,onClick:function(){return e.setState({expand:c?-1:t})}},s.map((function(a,t){return r.a.createElement("tspan",{key:t,x:"0",y:t*e.props.lineHeight},a)}))),c&&r.a.createElement("text",{className:"paper-abstract-inner",fontSize:e.props.secondaryFontSize,fill:z},a.abstractPieces.map((function(a,t){return r.a.createElement("tspan",{key:t,x:"0",y:s.length*e.props.lineHeight+t*e.props.secondaryLineHeight},a)}))),r.a.createElement("g",{className:"paper-edit-icon-group"},e.props.editable&&l(v.a,6,C,"to-exchange","Move"),l(p?g.a:u.a,4.5,E,p?"thumb-delete":"thumb-up","Like"),l(h?y.a:m.a,3,N,h?"thumb-delete":"thumb-down","Dislike"),e.props.editable&&(a.references.length>0||a.citations.length>0)&&l(w.a,1.5,e.props.linksVisibility[a.id]?A:W,"link-switch","Citation"),a.level&&r.a.createElement("g",{transform:"translate(0, ".concat(f,")")},r.a.createElement("rect",{className:"paper-edit-icon",width:o*a.level,height:o,fill:"transparent"}),r.a.createElement("g",{className:"paper-edit-icon",style:{transformOrigin:"".concat(o/2,"px ").concat(o/2,"px")}},L.a.range(0,a.level).map((function(e){return r.a.createElement("g",{transform:"translate(".concat(e*o,", 0)"),key:e},r.a.createElement(x.a,{className:"paper-edit-icon",fill:M,width:o,height:o}))})),r.a.createElement("text",{textAnchor:"middle",x:a.level*o/2,y:o+e.props.secondaryLineHeight/2,fill:M,fontSize:e.props.secondaryLineHeight/2},n("Influence")))))))}));return r.a.createElement("g",{className:"era-node-text-group",transform:"translate(".concat(this.props.x,", ").concat(this.props.y,")")},c.reverse())}}]),a}(r.a.Component),O=function(e){function a(){return Object(c.a)(this,a),Object(d.a)(this,Object(f.a)(a).apply(this,arguments))}return Object(b.a)(a,e),Object(s.a)(a,[{key:"generateLinkPath",value:function(e,a){var t=e.x,n=e.y,r=a.x,i=a.y,o=t-this.props.radius-this.props.nodePaddingLeft,c="M ".concat(t," ").concat(n);return c+=n===i?" L ".concat(r," ").concat(i):" C ".concat(o," ").concat(n,", ").concat(o," ").concat(n,", ").concat(o," ").concat((n+i)/2," S ").concat(o," ").concat(i,", ").concat(r," ").concat(i)}},{key:"generateArrowPath",value:function(e,a,t){var n=a.x,r=a.y,i=n>=e.x?n-this.props.radius:n+this.props.radius,o=r-this.props.radius/2,c=r+this.props.radius/2;if(t){var s=n>=e.x?n-1.2*this.props.radius:n+1.2*this.props.radius;return"M ".concat(n," ").concat(r," L ").concat(s," ").concat(o," L ").concat(i," ").concat(r," L ").concat(s," ").concat(c," L ").concat(n," ").concat(r)}var d=n>=e.x?n+.2*this.props.radius:n-.2*this.props.radius;return"M ".concat(i," ").concat(r," L ").concat(d," ").concat(o," L ").concat(n," ").concat(r," L ").concat(d," ").concat(c," L ").concat(i," ").concat(r)}},{key:"render",value:function(){var e=this,a=this.props.node.textColor,t=this.props.node.pins.map((function(t,n){return e.props.linksVisibility[t.id]&&r.a.createElement("g",{key:t.id},r.a.createElement("circle",{cx:t.x,cy:t.y,r:.5*e.props.lineHeight,fill:a}),[].concat(Object(h.a)(t.references),Object(h.a)(t.citations)).map((function(n,i){return e.props.nodesLookup[n]&&r.a.createElement("g",{key:i},r.a.createElement("path",{d:e.generateLinkPath(t,e.props.nodesLookup[n]),strokeWidth:2,stroke:a,strokeDasharray:10,fill:"transparent"}),r.a.createElement("path",{d:e.generateArrowPath(t,e.props.nodesLookup[n],t.references.indexOf(n)>=0),fill:a}))})))}));return r.a.createElement("g",null,t)}}]),a}(r.a.Component),P=t(156),R=t.n(P);t(260);function D(){return(D=Object.assign||function(e){for(var a=1;a<arguments.length;a++){var t=arguments[a];for(var n in t)Object.prototype.hasOwnProperty.call(t,n)&&(e[n]=t[n])}return e}).apply(this,arguments)}function F(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var j=r.a.createElement("path",{d:"M520.650142 80.061167L369.746208 410.281909l269.859169-71.853003 1.641632-0.631397-120.596867-257.736342z",fill:"#E9624C"}),B=r.a.createElement("path",{d:"M256.852386 657.03194l385.909977 121.859662 100.392157-222.756937-76.272784-163.279319-333.251449 96.098656-76.777901 168.077938z",fill:"#2A698D"}),H=r.a.createElement("path",{d:"M92.689111 1015.791836h865.266741L773.966704 622.052534l-102.286349 232.227895-442.735726-136.381798L92.689111 1015.791836z",fill:"#233D7E"}),U=r.a.createElement("path",{d:"M484.281662 0H0v1015.791836h6.187693L483.650265 1.641633l0.631397-1.641633zM555.376989 0l0.505117 1.136515 483.271427 1014.655321h4.293501V0H555.376989z",fill:"#3C3837"}),G=function(e){var a=e.svgRef,t=e.title,n=F(e,["svgRef","title"]);return r.a.createElement("svg",D({viewBox:"0 0 1024 1024",ref:a},n),t?r.a.createElement("title",null,t):null,j,B,H,U)},V=r.a.forwardRef((function(e,a){return r.a.createElement(G,D({svgRef:a},e))}));t.p;function Q(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function J(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?Q(t,!0).forEach((function(a){Object(p.a)(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):Q(t).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}var X=function(e){function a(e){var t;return Object(c.a)(this,a),(t=Object(d.a)(this,Object(f.a)(a).call(this,e))).EraMinRatio=t.props.EraMinRatio||.05,t.lastEraRatio=t.props.lastEraRatio||.2,t.strokeWidth=4,t.labelTextFontSize=64,t.labelTextLineHeight=72,t.nodeRadius=20,t.nodeTextLeadingMargin=20,t.nodeTextWidth=260,t.nodeFullSpan=2,t.horizonMarginTop=32,t.horizonMarginBottom=48,t.averageFontWidthRatio=.6,t.nodePaddingLeft=20,t.nodePaddingRight=20,t.nodePaddingTop=32,t.nodePaddingBottom=12,t.nodeEditButtonMarginTop=10,t.nodeOffsetX=t.nodePaddingLeft+t.nodeRadius,t.nodeOffsetY=t.nodePaddingTop+t.nodeRadius,t.nodeWidth=t.nodePaddingLeft+2*t.nodeRadius+t.nodeTextLeadingMargin+t.nodeTextWidth+t.nodePaddingRight,t.pinHeight=function(e,a){return(e.textPieces.length+.4)*a},t.nodeHeight=function(e){var a=L.a.sum(e.pins.map((function(e){return t.pinHeight(e,t.nodeTextLineHeight)})));return t.nodePaddingTop+t.nodeRadius+Math.max(t.nodeRadius,a)+t.nodePaddingBottom},t.state={toExchange:null,focusEraIndex:-1,linksVisibility:{}},t}return Object(b.a)(a,e),Object(s.a)(a,[{key:"onSwitchLinksVisibility",value:function(e){var a=this.state.linksVisibility;a[e]=!(!0===a[e]),this.setState({linksVisibility:a})}},{key:"render",value:function(){var e=this,a=function(e){var a=e.paper_id,t=e.paper_year,n=e.paper_venue.trim(),r=e.paper_title.trim(),i=e.citations,o=e.references,c=e.score,s="".concat(t),d=/^(19|20)\d{2}\b/.exec(n);return null==d&&n.length>0?s="".concat(t," ").concat(n):null!=d&&(s="".concat(n)),{id:a,year:t,venue:n,title:r,citations:i,references:o,text:"[".concat(s,"] ").concat(r).replace("\t"," ").replace("\n"," "),abstract:e.paper_abstract?e.paper_abstract.trim().replace("\t"," "):"",score:c}};this.clusterNames=this.props.data.clusterNames.map((function(e){return e.split(" ").map(L.a.capitalize).join(" ")})),this.hideSubBranch=this.props.hideSubBranch,this.disableTextBranchSpan=this.props.disableTextBranchSpan,this.disableTextClusterSpan=this.props.disableTextClusterSpan,this.nodeFontExtraSize=this.props.fontExtraSize||0,this.nodeTextFontSize=20+this.nodeFontExtraSize,this.nodeTextSecondaryFontSize=16+this.nodeFontExtraSize,this.nodeTextLineHeight=20+this.nodeFontExtraSize,this.nodeTextSecondaryLineHeight=18+this.nodeFontExtraSize,this.nodeTextCustomFold=function(a,t,n){var r=Math.floor(((t-1)*e.nodeWidth+e.nodeTextWidth)/(n*e.averageFontWidthRatio));return(a.match(new RegExp("([^\\n]{1,".concat(r,"})(\\s|$)"),"g"))||[]).filter((function(e){return e.length>0}))},this.nodeTextFold=function(a,t){return e.nodeTextCustomFold(a,t,e.nodeTextFontSize)},this.nodeTextSecondaryFold=function(a,t){return e.nodeTextCustomFold(a,t,e.nodeTextSecondaryFontSize)};var t=this.props.data.importance,n=L.a.max(t),i=L.a.min(t),o=t.map((function(a){return((a-i)/(n-i)+1)/2*e.strokeWidth})),c={root:a(this.props.data.root),branches:L.a.range(0,2*this.props.data.branches.length).map((function(){return[]}))};this.props.data.branches.forEach((function(t,n){return t.forEach((function(t,r){return t.forEach((function(t){var i=a(t);i.isSub=r,i.edits=e.props.userEdits[i.id],i.clusterID=i.edits?i.edits.clusterID:n,i.branchID=2*i.clusterID+r,e.hideSubBranch&&r||c.branches[i.branchID].push(i)}))}))})),c.branches.forEach((function(e){return e.sort((function(e,a){return e.year===a.year?a.citations.length-e.citations.length:a.year-e.year}))}));var s=L.a.flatten(c.branches).length;L.a.flatten(c.branches).sort((function(e,a){return a.score-e.score})).forEach((function(e,a){e.scoreRank=a,e.level=a<.1*s?3:a<.3*s?2:a<.6*s?1:0}));for(var d=[],f=L.a.flatten(c.branches).map((function(e){return e.year})).sort().reverse(),b=f[0],l=1,p=this.EraMinRatio*f.length,h=this.lastEraRatio*f.length,u=1;u<f.length;u++)f[u]===f[u-1]||l<p||u>f.length-h?l+=1:(d.push({from:f[u-1],to:b,cnt:l}),b=f[u],l=1);d.push({from:f[f.length-1],to:b,cnt:l});var m=function(e,a){return e.filter((function(e){return e.year>=a.from&&e.year<=a.to}))},g=this.props.data.branches.length,y=2*g,v=k.a.scale()(.5),w=k.a.cubehelix().start(200).rotations(3).gamma(.7).lightness([.2,.6]).scale().correctLightness().colors(g),x=c.branches.map((function(e,a){return k()(w[Math.floor(a/2)]).luminance(a%2===0?.25:.5)})),_=x.map((function(e){return k()(e).darken()})),T={defs:[],nodes:{},edges:[]},S=function(e,a,t,n,i,o){return T.edges.push(r.a.createElement("line",{key:T.edges.length,x1:e,y1:a,x2:t,y2:n,strokeWidth:o,stroke:i}))},E=function(e,a,t,n,r){return S(e,a,e,t,n,r)},N=function(e,a,t,n,r){return S(e,t,a,t,n,r)},C=function(e,a,t,n,i,o){var c=R.a.generate(8);return T.defs.push(r.a.createElement("defs",{key:c},r.a.createElement("linearGradient",{id:c,x1:t,y1:n,x2:i,y2:o,gradientUnits:"userSpaceOnUse"},r.a.createElement("stop",{offset:"20%",stopColor:e}),r.a.createElement("stop",{offset:"80%",stopColor:a})))),"url('#".concat(c,"')")};T.nodes.root={x:this.nodeWidth*(c.branches.length-1)/2+this.nodeOffsetX,y:this.nodeOffsetY,color:v,textColor:k()(v).darken(),pins:[J({},c.root,{textPieces:this.nodeTextCustomFold(c.root.text,3,1.5*this.nodeTextFontSize),fullTextPieces:this.nodeTextCustomFold(c.root.text,3,1.5*this.nodeTextFontSize),abstractPieces:this.nodeTextCustomFold(c.root.abstract,3,1.5*this.nodeTextSecondaryFontSize),edits:this.props.userEdits[c.root.id]})],span:3,fullSpan:3,fontSize:1.5*this.nodeTextFontSize,secondaryFontSize:1.5*this.nodeTextSecondaryFontSize,lineHeight:1.5*this.nodeTextLineHeight,secondaryLineHeight:1.5*this.nodeTextSecondaryLineHeight},T.nodes.root.height=this.nodeHeight(T.nodes.root),T.nodes.root.pins[0].x=T.nodes.root.x,T.nodes.root.pins[0].y=T.nodes.root.y,T.nodes.branches=c.branches.map((function(a,t){return d.map((function(n,r){return{x:e.nodeWidth*t+e.nodeOffsetX,y:0,color:x[t],textColor:_[t],pins:m(a,n),era:n,eraID:r,clusterID:Math.floor(t/2),branchID:t,fontSize:e.nodeTextFontSize,secondaryFontSize:e.nodeTextSecondaryFontSize,lineHeight:e.nodeTextLineHeight,secondaryLineHeight:e.nodeTextSecondaryLineHeight,height:0,edgeStrokeWidth:o[Math.floor(t/2)]}}))})),T.nodes.branches.forEach((function(a,t){return a.forEach((function(a,n){0!==a.pins.length&&(a.span=!(t<y-1&&0===T.nodes.branches[t+1][n].pins.length)||e.disableTextBranchSpan||e.disableTextClusterSpan&&t%2!==0?1:2,a.fullSpan=t<y-1?e.nodeFullSpan:1,a.pins.forEach((function(t){t.textPieces=e.nodeTextFold(t.text,a.span),t.fullTextPieces=e.nodeTextFold(t.text,a.fullSpan),t.abstractPieces=e.nodeTextSecondaryFold(t.abstract,a.fullSpan)})),a.height=e.nodeHeight(a))}))}));var W=T.nodes.root.height+this.horizonMarginTop,M=W+this.horizonMarginBottom,A=d.map((function(a,t){T.nodes.branches.forEach((function(a){return a[t].y=M+e.nodeOffsetY}));var n=T.nodes.branches.reduce((function(e,a){return Math.max(e,a[t].height||0)}),0);return M+=n,n})),z={};T.nodes.branches.forEach((function(a){return a.forEach((function(a){return a.pins.forEach((function(t,n){t.x=a.x,t.y=a.y+a.pins.slice(0,n).reduce((function(a,t){return a+(t.textPieces.length+.4)*e.nodeTextLineHeight}),0),z[t.id]=t}))}))}));var P=T.nodes.root,D=T.nodes.branches[0][0],F=T.nodes.branches[y-2][0];E(P.x,P.y,W,v,this.strokeWidth),N(D.x,F.x,W,v,this.strokeWidth),T.nodes.branches.forEach((function(a,t){var n=a.filter((function(e){return e.pins.length>0}));if(0!==n.length||t%2!==1){var r=t%2===0?0:n[0].eraID,i=n.length>0?n[n.length-1].eraID:0;if(t%2===0){var o=T.nodes.branches[t+1].filter((function(e){return e.pins.length>0}));o.length>0&&(i=Math.max(i,o[0].eraID))}for(var c=!e.disableTextBranchSpan&&!(e.disableTextClusterSpan&&t%2===0),s=r+1;s<=i;s++){var d=a[s],f=t>0?T.nodes.branches[t-1][s]:null,b=c&&0===d.pins.length&&(t>0&&f.pins.length>0||s===i)?d.y-e.nodeRadius-e.nodeTextLineHeight:d.y;d=a[s-1],f=t>0?T.nodes.branches[t-1][s-1]:null;var l=c&&0===d.pins.length&&t>0&&f.pins.length>0?d.y-e.nodeOffsetY+f.height-e.nodePaddingBottom+e.nodeTextLineHeight:d.y;E(d.x,b,l,d.color,d.edgeStrokeWidth)}if(t%2===0){var p=a[0],h=t>0?T.nodes.branches[t-1][0]:null,u=c&&0===p.pins.length&&t>0&&h.pins.length>0?p.y-e.nodeRadius-e.nodeTextLineHeight:p.y;E(p.x,W,u,C(v,p.color,p.x,W,p.x,u),p.edgeStrokeWidth)}else{var m=a[r],g=T.nodes.branches[t-1][r],y=m.y-e.nodeRadius-e.nodeTextLineHeight,w=m.y;E(m.x,w,y,m.color,m.edgeStrokeWidth),N(m.x,g.x,y,C(m.color,g.color,m.x,y,g.x,y),m.edgeStrokeWidth)}}}));var j=function(a,t,n){var r=e.props.userEdits;r[t.id]||"thumb-up"!==a&&"thumb-down"!==a&&"exchange"!==a||(r[t.id]={rate:0,clusterID:t.clusterID}),"thumb-up"===a&&r[t.id].rate<=0?(r[t.id].rate=1,e.props.onEditChange(r)):"thumb-down"===a&&r[t.id].rate>=0?(r[t.id].rate=-1,e.props.onEditChange(r)):"thumb-delete"===a&&r[t.id]&&0!==r[t.id].rate?(r[t.id].rate=0,e.props.onEditChange(r)):"to-exchange"===a&&null===e.state.toExchange?(e.setState({toExchange:t}),e.props.onEditChange(r)):"exchange"===a&&(r[t.id].clusterID=n,e.setState({toExchange:null}),e.props.onEditChange(r))},B=T.nodes.branches.map((function(e){return e[e.length-1]})).reduce((function(e,a){return Math.max(e,L.a.max(a.pins.map((function(e){return e.y+2*(e.fullTextPieces.length*a.lineHeight+e.abstractPieces.length*a.secondaryLineHeight)})))||0)}),M),H=L.a.flattenDeep(T.nodes.branches).sort((function(e,a){return e.eraID===a.eraID?a.branchID-e.branchID:a.eraID-e.eraID}));H.push(T.nodes.root);var U=this.nodeWidth*c.branches.length,G=this.clusterNames.map((function(e){return e.split(" ")})),Q=G.map((function(a,t){return r.a.createElement("text",{key:t},a.reverse().map((function(a,t){return r.a.createElement("tspan",{key:t,x:"0",y:-t*e.labelTextLineHeight},a)})))})),X=G.reduce((function(e,a){return Math.max(e,a.length)}),0)*this.labelTextLineHeight;M+=X+this.labelTextLineHeight;var K=Math.max(1.5*this.labelTextLineHeight,B-M),Y=w.map((function(e){return k()(e).luminance(.9)})),Z=w.map((function(e){return k()(e).luminance(.7)})),$=w.map((function(e,a){var t=T.nodes.branches[2*a][d.length-1].x;return C(k()(e).luminance(.9),"white",t,M,t,M+K)})),ee=w.map((function(e){return k()(e).luminance(.5)})),ae=w.map((function(e){return k()(e).luminance(.2)})),te=w.map((function(e,a){var t=T.nodes.branches[2*a][d.length-1].x;return C(k()(e).luminance(.5),"white",t,M,t,M+K)}));return r.a.createElement("svg",{className:"mrt",id:this.props.id,width:"100%",viewBox:"0 0 ".concat(U," ").concat(M+K)},T.defs,r.a.createElement("filter",{id:"blur-filter"},r.a.createElement("feGaussianBlur",{stdDeviation:this.nodeTextLineHeight,in:"SourceGraphic"})),r.a.createElement("g",{className:"mrt-background"},r.a.createElement("rect",{x:"0",y:"0",width:U,height:W,fill:k()(v).luminance(.9)})),Q.map((function(a,t){return r.a.createElement("g",{className:"mrt-background",key:t,opacity:null===e.state.toExchange?1:0},r.a.createElement("rect",{x:e.nodeWidth*t*2,y:W,width:2*e.nodeWidth,height:M-W,fill:Y[t]}),r.a.createElement("rect",{x:e.nodeWidth*t*2,y:M,width:2*e.nodeWidth,height:K,fill:$[t]}),r.a.createElement("g",{transform:"translate(".concat(e.nodeWidth*t*2+e.nodeOffsetX,", ").concat(M-e.labelTextLineHeight/2,")"),fill:Z[t],fontSize:e.labelTextFontSize},a))})),d.map((function(a,t){return r.a.createElement("g",{key:t,className:"mrt-era-background",transform:"translate(0, ".concat(T.nodes.branches[0][t].y-e.nodeRadius-e.nodePaddingTop+A[t],")")},r.a.createElement("rect",{className:"mrt-era-background",x:"0",y:-A[t],width:U,height:A[t],opacity:t===e.state.focusEraIndex?.1:0}),r.a.createElement("text",{className:"mrt-era-background",fontSize:e.labelTextFontSize,x:e.nodePaddingLeft,y:-e.labelTextFontSize/2,opacity:t===e.state.focusEraIndex?.2:0},a.from===a.to?a.from:"".concat(a.from," - ").concat(a.to)))})),T.nodes.branches.map((function(a,t){if(t%2!==0)return r.a.createElement("text",{key:t});var n=a.filter((function(e){return e.pins.length>0})),i=T.nodes.branches[t+1].filter((function(e){return e.pins.length>0}));if(0===n.length&&0===i.length)return r.a.createElement("text",{key:t});var o=2*e.nodeTextFontSize,c=(0===n.length||i.length>0&&i[0].eraID<=n[0].eraID?i[0].y-e.nodeRadius-e.nodeTextLineHeight/2:n[0].y-e.nodeTextLineHeight)-o/2,s=a[0].x+e.nodeRadius+e.nodeTextLeadingMargin,d=k()(x[t]).darken(2);return r.a.createElement("text",{key:t,x:s,y:c,fill:d,fontSize:o},e.clusterNames[Math.floor(t/2)])})),T.edges,H.map((function(a,t){return a.pins.length>0&&r.a.createElement(I,{key:t,node:a,radius:e.nodeRadius,lineHeight:e.nodeTextLineHeight,color:a.color,strokeWidth:e.strokeWidth,onHover:function(t){return e.setState(J({},e.state,{focusEraIndex:t?a.eraID:-1}))}})})),r.a.createElement("g",{className:"mrt-links"},H.map((function(a,t){return a.pins.length>0&&a!==T.nodes.root&&r.a.createElement(O,{key:t,linksVisibility:e.state.linksVisibility,node:a,nodesLookup:z,nodePaddingLeft:e.nodePaddingLeft,radius:e.nodeRadius,lineHeight:e.nodeTextLineHeight})}))),r.a.createElement("g",{className:"mrt-node-text-container"},H.map((function(a,t){return a.pins.length>0&&r.a.createElement(q,{key:t,node:a,pins:a.pins,x:a.x,y:a.y,radius:e.nodeRadius,lineHeight:a.lineHeight,secondaryLineHeight:a.secondaryLineHeight,textWidth:(a.span-1)*e.nodeWidth+e.nodeTextWidth,fullTextWidth:(a.fullSpan-1)*e.nodeWidth+e.nodeTextWidth,color:a.color,fontSize:a.fontSize,secondaryFontSize:a.secondaryFontSize,strokeWidth:e.strokeWidth,onEdit:j,textLeadingMargin:e.nodeTextLeadingMargin,editable:"undefined"!==typeof a.clusterID,editButtonMarginTop:e.nodeEditButtonMarginTop,scaleOrigin:a.clusterID===g-1?"right":a.branchID===y-3?"middle":"left",linksVisibility:e.state.linksVisibility,onSwitchLinksVisibility:function(a){return e.onSwitchLinksVisibility(a)},lang:e.props.lang})}))),Q.map((function(a,t){var n=null!==e.state.toExchange&&t===e.state.toExchange.clusterID;return r.a.createElement("g",{className:"mrt-background",key:t,opacity:null===e.state.toExchange?0:1,visibility:null===e.state.toExchange?"hidden":"none",onClick:function(){return j("exchange",e.state.toExchange,t)}},r.a.createElement("rect",{className:"mrt-background-card",x:e.nodeWidth*t*2,y:W,width:2*e.nodeWidth,height:M-W,fill:ee[t]}),r.a.createElement("rect",{className:"mrt-background-card",x:e.nodeWidth*t*2,y:M,width:2*e.nodeWidth,height:K,fill:te[t]}),r.a.createElement("g",{className:"mrt-background-text",style:{textDecoration:n?"underline":""},transform:"translate(".concat(e.nodeWidth*t*2+e.nodeOffsetX,", ").concat(M-e.labelTextLineHeight/2,")"),fill:ae[t],fontSize:e.labelTextFontSize},a))})),r.a.createElement("g",{opacity:"0.5",transform:"translate(".concat(U,", ").concat(M+K-.5*this.labelTextLineHeight,")")},r.a.createElement(V,{x:3.35*-this.labelTextFontSize,y:1.78*-this.labelTextFontSize,height:.8*this.labelTextFontSize,width:.8*this.labelTextFontSize}),r.a.createElement("text",{x:.1*-this.labelTextFontSize,y:.05*-this.labelTextFontSize,textAnchor:"end",fontSize:.75*this.labelTextFontSize,fill:k()("grey").luminance(.3).hex()},(this.props.authors||[]).join(", ")),r.a.createElement("text",{x:.1*-this.labelTextFontSize,y:1*-this.labelTextFontSize,textAnchor:"end",fontSize:.7*this.labelTextFontSize,fill:k()("grey").luminance(.3).hex()},"AMiner")))}}]),a}(r.a.Component),K=(t(261),t(89)),Y=t.n(K),Z=t(157),$=t.n(Z),ee=(t(282),t(329)),ae=(t(283),function(e){function a(){return Object(c.a)(this,a),Object(d.a)(this,Object(f.a)(a).apply(this,arguments))}return Object(b.a)(a,e),Object(s.a)(a,[{key:"onClick",value:function(){this.props.onClick&&this.props.onClick()}},{key:"render",value:function(){var e=this,a=this.props.luminance||(this.props.primary?.4:.2),t=k()(this.props.color).luminance(a),n=t.luminance(a/2),i="twoTone"!==this.props.theme?r.a.createElement(ee.a,{className:"toolicon",type:this.props.type,theme:this.props.theme,onClick:function(){return e.onClick()},style:{color:t.hex()}}):r.a.createElement(ee.a,{className:"toolicon",type:this.props.type,theme:this.props.theme,onClick:function(){return e.onClick()},twoToneColor:t.hex()});return r.a.createElement("div",{className:"tool tooltip ".concat(this.props.primary?"primary":"secondary"," ").concat(this.props.className)},i,r.a.createElement("span",{className:"tooltip-text",style:{color:n.hex()}},this.props.tooltipText),this.props.children)}}]),a}(r.a.Component)),te=t(164),ne=function(e){function a(){return Object(c.a)(this,a),Object(d.a)(this,Object(f.a)(a).apply(this,arguments))}return Object(b.a)(a,e),Object(s.a)(a,[{key:"render",value:function(){var e=this;setTimeout((function(){$.a.toCanvas(document.getElementById("mrt-share-qrcode-canvas"),window.location.href,(function(e){e&&console.error(e)}))}),500);var a=this.props.lang||"en",t=te[a],n=function(e){return t&&t[e.toLowerCase()]?t[e.toLowerCase()]:e};return r.a.createElement("div",null,r.a.createElement("div",{className:"toolgroup horizontal"},r.a.createElement("div",{className:"toolgroup secondary vertical"},r.a.createElement(ae,{type:"heart",theme:this.props.like?"filled":"twoTone",color:"red",tooltipText:n(this.props.like?"Dislike":"Like"),primary:!0,onClick:function(){return e.props.onLike()}})),r.a.createElement("div",{className:"toolgroup secondary vertical"},r.a.createElement(ae,{type:"share-alt",theme:"outlined",color:"green",tooltipText:n("Share"),primary:!0}),r.a.createElement(ae,{className:"qrcode-icon",type:"qrcode",theme:"outlined",color:"green",tooltipText:n("QR Code")},r.a.createElement("canvas",{className:"qrcode",id:"mrt-share-qrcode-canvas"}))),r.a.createElement("div",{className:"toolgroup secondary vertical"},r.a.createElement(ae,{type:"font-size",theme:"outlined",color:"pink",tooltipText:n("Font Size"),primary:!0}),r.a.createElement(ae,{type:"zoom-in",theme:"outlined",color:"pink",tooltipText:n("Larger Font"),onClick:function(){return e.props.scaleFont(!0)}}),r.a.createElement(ae,{type:"zoom-out",theme:"outlined",color:"pink",tooltipText:n("Smaller Font"),onClick:function(){return e.props.scaleFont(!1)}})),r.a.createElement("div",{className:"toolgroup secondary vertical"},r.a.createElement(ae,{type:"search",theme:"outlined",color:"aquamarine",tooltipText:n("Zoom"),primary:!0}),r.a.createElement(ae,{type:"zoom-in",theme:"outlined",color:"aquamarine",tooltipText:n("Zoom In"),onClick:function(){return e.props.zoom(!0)}}),r.a.createElement(ae,{type:"zoom-out",theme:"outlined",color:"aquamarine",tooltipText:n("Zoom Out"),onClick:function(){return e.props.zoom(!1)}})),r.a.createElement("div",{className:"toolgroup secondary vertical"},r.a.createElement(ae,{type:"download",theme:"outlined",color:"blue",tooltipText:n("Download"),primary:!0}),r.a.createElement(ae,{type:"file-image",theme:"twoTone",color:"blue",tooltipText:n("Full Picture"),onClick:function(){return e.props.capture(!0)}}),r.a.createElement(ae,{type:"camera",theme:"twoTone",color:"blue",tooltipText:n("Snapshot"),onClick:function(){return e.props.capture(!1)}})),r.a.createElement("div",{className:"toolgroup secondary vertical"},r.a.createElement(ae,{type:"control",theme:"outlined",color:"teal",tooltipText:n("Control"),primary:!0}),r.a.createElement(ae,{type:"eye".concat(this.props.hideSubBranch?"":"-invisible"),theme:"twoTone",color:"teal",onClick:function(){return e.props.onHideSubBranch()},tooltipText:n(this.props.hideSubBranch?"Display Sub Branch":"Hide Sub Branch")}),r.a.createElement(ae,{type:"column-width",theme:"outlined",color:"teal",onClick:function(){return e.props.onDisableTextClusterSpan()},tooltipText:n(this.props.disableTextClusterSpan?"Enable Text Span":"Disable Text Span")}),this.props.onLoadJson&&r.a.createElement(ae,{type:"folder-open",theme:"outlined",color:"teal",onClick:function(){return document.getElementById("mrt-file-load-input").click()},tooltipText:n("Load JSON")}),r.a.createElement("input",{id:"mrt-file-load-input",type:"file",hidden:!0,onChange:function(a){return e.props.onLoadJson(a)}})),r.a.createElement("div",{className:"toolgroup vertical"},r.a.createElement(ae,{className:"toolgroup",type:"appstore",theme:"outlined",color:"purple",tooltipText:n("Toolbox"),primary:!0}))))}}]),a}(r.a.Component);function re(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function ie(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?re(t,!0).forEach((function(a){Object(p.a)(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):re(t).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}var oe=function(e){function a(e){var t;return Object(c.a)(this,a),(t=Object(d.a)(this,Object(f.a)(a).call(this,e))).state={like:t.props.like||!1,viewerScale:100,hideSubBranch:!1,disableTextClusterSpan:!1,fontExtraSize:0},t.generated=!1,t}return Object(b.a)(a,e),Object(s.a)(a,[{key:"capture",value:function(e){if(e)Y.a.saveSvgAsPng(document.getElementById("mrt-viewer"),"master-reading-tree.png");else{var a=document.getElementById("mrt-viewer").viewBox.baseVal.width,t=document.body.clientWidth;Y.a.saveSvgAsPng(document.getElementById("mrt-viewer"),"master-reading-tree-snapshot.png",{scale:t/a})}}},{key:"zoom",value:function(e){this.setState(ie({},this.state,{viewerScale:Math.min(Math.max(this.state.viewerScale+(e?10:-10),100),1e3)}))}},{key:"scaleFont",value:function(e){this.setState(ie({},this.state,{fontExtraSize:Math.max(0,Math.min(10,this.state.fontExtraSize+(e?2:-2)))}))}},{key:"onLoadJson",value:function(e){var a=this;if(0!==e.target.files.length){var t=new FileReader;t.onload=function(e){a.props.onLoadJson&&a.props.onLoadJson(JSON.parse(e.target.result))},t.readAsText(e.target.files[0])}}},{key:"render",value:function(){var e=this;return r.a.createElement("div",{className:"mrt-container",style:{width:"".concat(this.state.viewerScale,"%")}},r.a.createElement(ne,Object(p.a)({lang:this.props.lang,onLike:function(){return e.props.onLike()},like:this.props.like,onHideSubBranch:function(){return e.setState({hideSubBranch:!e.state.hideSubBranch})},hideSubBranch:this.state.hideSubBranch,onDisableTextClusterSpan:function(){return e.setState({disableTextClusterSpan:!e.state.disableTextClusterSpan})},disableTextClusterSpan:this.state.disableTextClusterSpan,onLoadJson:this.props.onLoadJson?function(a){return e.onLoadJson(a)}:void 0,scaleFont:function(a){return e.scaleFont(a)},zoom:function(a){return e.zoom(a)},capture:function(a){return e.capture(a)}},"lang",this.props.lang)),r.a.createElement(X,{id:"mrt-viewer",data:this.props.data,userEdits:this.props.userEdits,hideSubBranch:this.state.hideSubBranch,disableTextClusterSpan:this.state.disableTextClusterSpan,fontExtraSize:this.state.fontExtraSize,authors:this.props.authors,onEditChange:this.props.onEditChange,lang:this.props.lang}))}}]),a}(r.a.Component),ce=function(e){function a(e){var t;return Object(c.a)(this,a),(t=Object(d.a)(this,Object(f.a)(a).call(this,e))).state={data:l,like:!1,userEdits:{}},t}return Object(b.a)(a,e),Object(s.a)(a,[{key:"render",value:function(){var e=this;return r.a.createElement("div",{className:"App"},r.a.createElement(oe,{data:this.state.data,authors:["Somefive","Rainatum"],onLoadJson:function(a){return e.setState({data:a})},onLike:function(){return e.setState({like:!e.state.like})},like:this.state.like,onEditChange:function(a){return e.setState({userEdits:a})},userEdits:this.state.userEdits,lang:"en"}))}}]),a}(r.a.Component);Boolean("localhost"===window.location.hostname||"[::1]"===window.location.hostname||window.location.hostname.match(/^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/));o.a.render(r.a.createElement(ce,null),document.getElementById("root")),"serviceWorker"in navigator&&navigator.serviceWorker.ready.then((function(e){e.unregister()}))}},[[166,1,2]]]);
//# sourceMappingURL=main.42179aaa.chunk.js.map