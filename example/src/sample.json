{"root": {"paper_id": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "paper_year": 2018, "paper_venue": "NAACL-HLT", "paper_authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "paper_citations": 997, "paper_abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. \nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "citations": [], "references": ["93b8da28d006415866bf48f9a6e06b5242129195", "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "26e743d5bd465f49b9538deaf116c15e61b7951f", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "007ab5528b3bd310a80d553cccad4b78dc496b02", "1e077413b25c4d34945cc2707e17e46ed4fe784a", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "081651b38ff7533550a3adfc1c00da333a8fe86c", "7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "b9de9599d7241459db9213b5cdd7059696f5ef8d", "4361e64f2d12d63476fdc88faf72a0f70d9a2ffb", "8c1b00128e74f1cd92aede3959690615695d5101", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "421fc2556836a6b441de806d7b393a35b6eaea58", "e0222a1ae6874f7fff128c3da8769ab95963da04", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "1a07186bc10592f0330655519ad91652125cd907", "3c78c6df5eb1695b6a399e346dde880af27d1016", "27e98e09cf09bc13c913d01676e5f32624011050", "475354f10798f110d34792b6d88f31d6d5cb099e", "843959ffdccf31c6694d135fad07425924f785b1", "766ce989b8b8b984f7a4691fd8c9af4bdb2b74cd", "38211dc39e41273c0007889202c69f841e02248a", "128cb6b891aee1b5df099acb48e2efecfcff689f", "59761abc736397539bdd01ad7f9d91c8607c0457", "3febb2bed8865945e7fddc99efd791887bb7e14f", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "687bac2d3320083eb4530bf18bb8f8f721477600", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "1510cf4b8abea80b9f352325ca4c132887de21a0", "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "bc1d609520290e0460c49b685675eb5a57fa5935", "af5c4b80fbf847f69a202ba5a780a3dd18c1a027", "9fa8d73e572c3ca824a04a5f551b602a17831bc5", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "2c5135a0531bc5ad7dd890f018e67a40529f5bcb", "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "1005645c05585c2042e3410daeed638b55e2474d", "783480acff435bfbc15ffcdb4f15eccddaa0c810", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "26b47e35fe6e4260fdf7b7cc98f279a73c277494", "a97dc52807d80454e78d255f9fbd7b0fab56bd03", "05dd7254b632376973f3a1b4d39485da17814df5", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb", "0e6824e137847be0599bb0032e37042ed2ef5045", "dac72f2c509aee67524d3321f77e97e8eff51de6"], "score": 1.0000000000000002, "embeddings": [0.8539482348163329, 0.038089813056682194, -0.03787621004142873, -0.3488360063004663, -0.00910197138548498, -0.04376638685636092, 0.03652325189315867, -0.03560260311069311, -0.13699346785286678, 0.12148407015439225, -0.12360012296355442, -0.10552727794783517, -0.055553415009155165, -0.024312066438872227, 0.005012187985952592, -0.027825226353320312, 0.0334960464198825, -0.002203567890811548, -0.0187200075187746, 0.029425450481682724, 0.05104159076905506, 0.02135436324822071, 0.07920679618875512, -0.07687431369818758, -0.07668511455670217, -0.02343221173212624, -0.08799043063341475, 0.08841159073005975, -0.11638038242916215, -0.14959521831917133, 0.028939997513310823, -0.029126500219044914]}, "branches": [[[{"paper_id": "4361e64f2d12d63476fdc88faf72a0f70d9a2ffb", "paper_title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units", "paper_year": 2017, "paper_venue": "ArXiv", "paper_authors": ["Dan Hendrycks", "Kevin Gimpel"], "paper_citations": 45, "paper_abstract": "We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic regularizer which randomly applies the identity or zero map to a neuron's input. This stochastic regularizer is comparable to nonlinearities aided by dropout, but it removes the need for a traditional nonlinearity. The connection between the GELU and the stochastic regularizer suggests a new probabilistic understanding of nonlinearities. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all tasks.", "citations": ["cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "df2b0e26d0599ce3e70df8a9da02e51594e0e992"], "references": ["6c8b30f63f265c32e26d999aa1fef5286b8308ad", "061356704ec86334dbbc073985375fe13cd39088", "272216c1f097706721096669d85b2843c23fa77d"], "score": 1.4359975810264478, "embeddings": [0.4295984310994161, 0.04431043306600404, -0.22035019527776428, -0.03139159656493396, -0.31199005081417586, -0.1315945055790083, -0.1730380382304113, 0.19411907025014405, -0.027155875374571776, -0.0012568722171969404, -0.4360884773643336, -0.17843694216218747, -0.2403531497822639, -0.13184213097189687, -0.12507474645974057, 0.12563224967263759, -0.23534047218901047, -0.21735806565971322, 0.07389537943266623, -0.15354777842390946, -0.030654532591766707, 0.027271442381790495, 0.032208585423530584, 0.017194027540563678, 0.16841537991980055, -0.0898831446188209, -0.20517162511660253, -0.06582562359131196, 0.033050815966187835, 0.1450299443367979, 0.09806138237209104, 0.055090933516656664]}, {"paper_id": "081651b38ff7533550a3adfc1c00da333a8fe86c", "paper_title": "How transferable are features in deep neural networks?", "paper_year": 2014, "paper_venue": "NIPS", "paper_authors": ["Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson"], "paper_citations": 999, "paper_abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.", "citations": ["1e077413b25c4d34945cc2707e17e46ed4fe784a", "df2b0e26d0599ce3e70df8a9da02e51594e0e992"], "references": ["abd1c342495432171beb7ca8fd9551ef13cbd0ff", "38211dc39e41273c0007889202c69f841e02248a"], "score": 1.4142894406817803, "embeddings": [0.3642173120479671, -0.06752501327405219, -0.2635001234765823, -0.021946766023952426, -0.4110122496943443, -0.41344608086265133, 0.05810398352714939, -0.19379248825692147, 0.16121845115142303, 0.21416290393000828, 0.007116255528609102, -0.05349833956286815, -0.1931623678519307, -0.0011166602133886746, 0.17453496127939638, -0.11680342219853622, 0.26869901740607127, 0.11420059408350464, -0.2764809774734813, -0.013783337879719732, 0.19626503384418723, -0.011084744331138175, 0.06768806389771481, 0.03988330590098283, 0.034108001836525195, 0.11935133434701096, -0.0769554011324633, 0.08216968336640003, -0.10429750444975894, 0.07825754948158657, 0.027627663071292854, 0.057862750861154076]}, {"paper_id": "38211dc39e41273c0007889202c69f841e02248a", "paper_title": "ImageNet: A large-scale hierarchical image database", "paper_year": 2009, "paper_venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition", "paper_authors": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Fei-Fei Li"], "paper_citations": 1031, "paper_abstract": "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \u201cImageNet\u201d, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.", "citations": ["79dc84a3bf76f1cb983902e2591d913cee5bdb0e", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "02b3d1d162080d9aefd3fc30a0bcc9a843073b5d", "061356704ec86334dbbc073985375fe13cd39088", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "05aba481e8a221df5d8775a3bb749001e7f2525e", "05dd7254b632376973f3a1b4d39485da17814df5", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "081651b38ff7533550a3adfc1c00da333a8fe86c", "71b7178df5d2b112d07e45038cb5637208659ff7", "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "a4cec122a08216fe8a3bc19b22e78fbaea096256"], "references": ["d87ceda3042f781c341ac17109d1e94a717f5f60"], "score": 1.3471047554687703, "embeddings": [0.5179664007764153, -0.11644415479925088, -0.14847278720071255, 0.27140641643505137, -0.5461129488272183, -0.3533229159151885, 0.010067659909925843, -0.2769137914844583, 0.0778926890387257, -0.01752622848858767, 0.14034644508080724, -0.05548371851415184, 0.026047062380811045, 0.005382753869220893, -0.05656344238013463, 0.05641419710970328, 0.07604921451216555, -0.022475789426073305, -0.13799790850540142, 0.06691476973846097, 0.06435334558278841, -0.1001628017098366, -0.022152187763051185, -0.13636752217191034, 0.012029059320005017, 0.004130804746291811, 0.04093432468854922, 0.020488244590872246, 0.13689055567365593, 0.012928357855200147, 0.010210009098958174, 0.044827719559686355]}], [{"paper_id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "paper_title": "Deep Residual Learning for Image Recognition", "paper_year": 2015, "paper_venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "paper_authors": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "paper_citations": 1053, "paper_abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.", "citations": ["204e3073870fae3d05bcbc2f6a8e263d9b72e776", "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "e0222a1ae6874f7fff128c3da8769ab95963da04", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "1e077413b25c4d34945cc2707e17e46ed4fe784a", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd"], "references": ["abd1c342495432171beb7ca8fd9551ef13cbd0ff", "061356704ec86334dbbc073985375fe13cd39088", "0b3cfbf79d50dae4a16584533227bb728e3522aa", "71b7178df5d2b112d07e45038cb5637208659ff7"], "score": 0.4426986214661845, "embeddings": [0.5507549746739178, 0.03257206695124365, -0.4059397327713816, 0.09879325834270418, -0.2906773778395037, -0.1258429203153382, 0.08048734054115993, 0.061089486284809785, 0.14091085360850886, 0.05773734683323349, -0.05797980073590971, 0.22086514444752905, -0.1293715731766479, -0.018851261231999913, 0.27214579129769817, -0.165109566214474, -0.02637997010779522, -0.036243948813937374, 0.05915688005351836, 0.0578332715297687, -0.2731086438036483, -0.023131507240614774, -0.0343924455133304, 0.12462423536545578, -0.257689456475104, -0.09692186043644135, -0.02449175983991835, -0.11888511373696149, 0.026213442176267417, -0.056389939865616946, -0.008867129290353194, -0.14698132527202348]}, {"paper_id": "061356704ec86334dbbc073985375fe13cd39088", "paper_title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "paper_year": 2014, "paper_venue": "ICLR", "paper_authors": ["Karen Simonyan", "Andrew Zisserman"], "paper_citations": 1049, "paper_abstract": "Abstract: In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.", "citations": ["2c03df8b48bf3fa39054345bafabfeff15bfd11d", "146f6f6ed688c905fb6e346ad02332efd5464616", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "a4cec122a08216fe8a3bc19b22e78fbaea096256", "4361e64f2d12d63476fdc88faf72a0f70d9a2ffb"], "references": ["abd1c342495432171beb7ca8fd9551ef13cbd0ff", "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "38211dc39e41273c0007889202c69f841e02248a"], "score": 0.31484382949429224, "embeddings": [0.45215493362986486, -0.06092884563289333, -0.3442459096498214, 0.3385382182590633, -0.5392977915214691, -0.28178438713826154, -0.09545954604153137, 0.027283374944303544, 0.040464515120068094, 0.015966429962925518, -0.038176542093229265, -0.02258043713418192, -0.08501894707302708, -0.14167443189472895, -0.05072220978920882, 0.01234111129268386, -0.13138906914595944, 0.05421620103760218, 0.11743504714527325, -0.09190420421712513, -0.10180243854543199, -0.04833972011424481, -0.023093719266140185, -0.07619543698703901, 0.13616359707274578, -0.13917852556620977, -0.07483659649744784, -0.06348829133356032, -0.010346853377349714, -0.18199871042055585, 0.007472756567307203, -0.04935525256391422]}, {"paper_id": "71b7178df5d2b112d07e45038cb5637208659ff7", "paper_title": "Microsoft COCO: Common Objects in Context", "paper_year": 2014, "paper_venue": "ECCV", "paper_authors": ["Tsung-Yi Lin", "Michael Maire", "Serge J. Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C. Lawrence Zitnick"], "paper_citations": 1028, "paper_abstract": "We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.", "citations": ["2c03df8b48bf3fa39054345bafabfeff15bfd11d", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "146f6f6ed688c905fb6e346ad02332efd5464616", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "0e6824e137847be0599bb0032e37042ed2ef5045", "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "bc1d609520290e0460c49b685675eb5a57fa5935"], "references": ["d87ceda3042f781c341ac17109d1e94a717f5f60", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "38211dc39e41273c0007889202c69f841e02248a"], "score": 0.3301955142774451, "embeddings": [0.4741790900638613, -0.14978037657601143, -0.25501149547395907, 0.2885681546745378, -0.5340275291154761, -0.13892172475703554, 0.14851968237907515, -0.12491533432673593, -0.22480149355267398, 0.05955776150518808, 0.14776852341925684, 0.10602119191329684, 0.142723365707986, 0.05714508548947013, -0.10003699517997863, -0.0665826714070319, -0.139237122211352, -0.11700281933921737, -0.007890729129483553, 0.07098099449253376, 0.016589691058452632, 0.07871438095350501, 0.009070629480564741, 0.14082271578954775, -0.199143651605738, -0.05632771598369826, 0.08050109586801876, -0.06812730531290519, -0.030986651679473922, -0.05132826687552848, -0.11615387316094261, 0.03788176232497227]}, {"paper_id": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "paper_title": "ImageNet Large Scale Visual Recognition Challenge", "paper_year": 2014, "paper_venue": "International Journal of Computer Vision", "paper_authors": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael S. Bernstein", "Alexander C. Berg", "Fei-Fei Li"], "paper_citations": 1018, "paper_abstract": "The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5\u00a0years of the challenge, and propose future directions and improvements.", "citations": ["26e743d5bd465f49b9538deaf116c15e61b7951f", "a4cec122a08216fe8a3bc19b22e78fbaea096256", "061356704ec86334dbbc073985375fe13cd39088"], "references": ["38211dc39e41273c0007889202c69f841e02248a", "061356704ec86334dbbc073985375fe13cd39088", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "71b7178df5d2b112d07e45038cb5637208659ff7", "0a10d64beb0931efdc24a28edaa91d539194b2e2"], "score": 0.1986621744004882, "embeddings": [0.38250276279134754, -0.14592695784361473, -0.24489099104156814, 0.32976191353549933, -0.5598439631858108, -0.32951020735300335, -0.00483709649600148, -0.0929723591676342, -0.12194919602256275, -0.04237334171008227, 0.2312516112379731, 0.008551613932176839, 0.0905522322704638, -0.06008015629949848, -0.1554594211853693, 0.1085500896835238, -0.0025750739767592765, 0.13794418731770303, 0.21444093081375556, -0.029296760232002787, -0.05494434847288015, -0.023086676439768587, -0.06792113929261573, -0.11815591142158152, 0.02573094147504982, 0.04201181436533168, 0.11355490642068311, 0.06675763674402417, 0.01730064640254305, 0.015560123941788509, -0.005067973574319003, -0.08176435166437804]}, {"paper_id": "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "paper_title": "ImageNet Classification with Deep Convolutional Neural Networks", "paper_year": 2012, "paper_venue": "NIPS", "paper_authors": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "paper_citations": 1090, "paper_abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.", "citations": ["39dba6f22d72853561a4ed684be265e179a39e4f", "0b544dfe355a5070b60986319a3f51fb45d1348e", "79dc84a3bf76f1cb983902e2591d913cee5bdb0e", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "02b3d1d162080d9aefd3fc30a0bcc9a843073b5d", "061356704ec86334dbbc073985375fe13cd39088", "6c8b30f63f265c32e26d999aa1fef5286b8308ad", "146f6f6ed688c905fb6e346ad02332efd5464616", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "081651b38ff7533550a3adfc1c00da333a8fe86c", "71b7178df5d2b112d07e45038cb5637208659ff7", "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "272216c1f097706721096669d85b2843c23fa77d"], "references": ["38211dc39e41273c0007889202c69f841e02248a"], "score": 0.3586280018867734, "embeddings": [0.5674229906383166, -0.052343375526522774, -0.3891921787211839, 0.32973143702967955, -0.4714831716727234, -0.26430014856214457, -0.029436471195534882, -0.1515367507902726, 0.17334258566576233, 0.006422681922114415, -0.029469661605996213, -0.010791608661113058, -0.039736321181215443, -0.01999771642722529, -0.028396381168678285, -0.07878912193943119, -0.006743313241851298, 0.07354646493674007, -0.05781570945995186, -0.08407527392896258, 0.013080633928241051, 0.10612942811782362, -0.10029463291570413, 0.06352124918895058, 0.007943024283053584, 0.028555680000824875, 0.12413430633052432, -0.011484625365656085, 0.028521565492671846, 0.009760084249142707, -0.015725453119410208, 0.008953516414785341]}]], [[{"paper_id": "8c1b00128e74f1cd92aede3959690615695d5101", "paper_title": "QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension", "paper_year": 2018, "paper_venue": "ICLR", "paper_authors": ["Adams Wei Yu", "David Dohan", "Minh-Thang Luong", "Rui Zhao", "Kai Chen", "Mohammad Norouzi", "Quoc V. Le"], "paper_citations": 164, "paper_abstract": "Current end-to-end machine reading and question answering (Q\\&A) models are primarily based on recurrent neural networks (RNNs) with attention. Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs. We propose a new Q\\&A model that does not require recurrent networks: It consists exclusively of attention and convolutions, yet achieves equivalent or better performance than existing models. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference. The speed-up gain allows us to train the model with much more data. We hence combine our model with data generated by backtranslation from a neural machine translation model. This data augmentation technique not only enhances the training examples but also diversifies the phrasing of the sentences, which results in immediate accuracy improvements. Our single model achieves 84.6 F1 score on the test set, which is significantly better than the best published F1 score of 81.8.", "citations": ["26b47e35fe6e4260fdf7b7cc98f279a73c277494", "9784fbf77295860b2e412137b86356d70b25e3c0", "df2b0e26d0599ce3e70df8a9da02e51594e0e992"], "references": ["3c78c6df5eb1695b6a399e346dde880af27d1016", "5171157c2c09a85ad6558c5c03da6b75b0cf5fe6", "1ee46c3b71ebe336d0b278de9093cfca7af7390b", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "0b3cfbf79d50dae4a16584533227bb728e3522aa", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "071b16f25117fb6133480c6259227d54fc2a5ea0", "007ab5528b3bd310a80d553cccad4b78dc496b02", "93499a7c7f699b6630a86fad964536f9423bb6d0", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "d1505c6123c102e53eb19dff312cb25cea840b72", "05dd7254b632376973f3a1b4d39485da17814df5", "272216c1f097706721096669d85b2843c23fa77d"], "score": 1.523296376847107, "embeddings": [0.6429584777122473, 0.6908581961568434, 0.20437120802991304, 0.12618516743482258, 0.06702197994106406, 0.006493064853210489, 0.02947992698123918, -0.016058659667061336, -0.01347864420846931, 0.03304277229745318, 0.04120471369352598, 0.052620423817229714, -0.00730696138474231, 0.00016973772997018455, 0.046889516690676536, 0.06982264462296217, -0.01489752018154867, 0.014241043696551482, -0.050116042129961784, 0.027776305899473726, -0.026969684601267033, -0.014963848550572235, 0.003511029678120521, -0.03799575313857233, -0.03290183862200804, -0.07218776991636369, -0.08614238258860266, 0.046484455177798485, 0.0347242783353988, 0.08003285763428615, 0.03552803669645276, -0.047298622831108486]}, {"paper_id": "27e98e09cf09bc13c913d01676e5f32624011050", "paper_title": "U-Net: Machine Reading Comprehension with Unanswerable Questions", "paper_year": 2018, "paper_venue": "ArXiv", "paper_authors": ["Fu Sun", "Linyang Li", "Xipeng Qiu", "Yang P. Liu"], "paper_citations": 8, "paper_abstract": "Machine reading comprehension with unanswerable questions is a new challenging task for natural language processing. A key subtask is to reliably predict whether the question is unanswerable. In this paper, we propose a unified model, called U-Net, with three important components: answer pointer, no-answer pointer, and answer verifier. We introduce a universal node and thus process the question and its context passage as a single contiguous sequence of tokens. The universal node encodes the fused information from both the question and passage, and plays an important role to predict whether the question is answerable and also greatly improves the conciseness of the U-Net. Different from the state-of-art pipeline models, U-Net can be learned in an end-to-end fashion. The experimental results on the SQuAD 2.0 dataset show that U-Net can effectively predict the unanswerability of questions and achieves an F1 score of 71.7 on SQuAD 2.0.", "citations": ["df2b0e26d0599ce3e70df8a9da02e51594e0e992"], "references": ["d1505c6123c102e53eb19dff312cb25cea840b72", "3c78c6df5eb1695b6a399e346dde880af27d1016", "5171157c2c09a85ad6558c5c03da6b75b0cf5fe6", "3febb2bed8865945e7fddc99efd791887bb7e14f", "1ee46c3b71ebe336d0b278de9093cfca7af7390b", "0b3cfbf79d50dae4a16584533227bb728e3522aa", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "071b16f25117fb6133480c6259227d54fc2a5ea0", "e0222a1ae6874f7fff128c3da8769ab95963da04", "007ab5528b3bd310a80d553cccad4b78dc496b02", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "26b47e35fe6e4260fdf7b7cc98f279a73c277494", "05dd7254b632376973f3a1b4d39485da17814df5", "6c8b30f63f265c32e26d999aa1fef5286b8308ad", "272216c1f097706721096669d85b2843c23fa77d"], "score": 1.5040317261724683, "embeddings": [0.6080929038005325, 0.6445724965722561, 0.22470165634922462, 0.026530715927609513, -0.0012509321303218622, -0.11796440312969637, 0.0014739030923529158, 0.0648227184762851, 0.022520196190246033, -0.12813569082545492, -0.1561410489040135, 0.02789643184995701, 0.0029357050316492273, 0.0742104593909695, -0.04491247461672371, -0.018628224819631233, -0.058897901195880835, -0.055050342414562185, -0.0006664598025203055, -0.07110997587667878, 0.048782645685365396, -0.0755034638616703, 0.058424665088599136, 0.016140525591274442, -0.10876130988274024, 0.2069534325404911, 0.09469657002282092, -0.0504486630454502, 0.027415700017765134, 0.06664802236351551, 0.016946917796223145, -0.0224964943320158]}, {"paper_id": "26b47e35fe6e4260fdf7b7cc98f279a73c277494", "paper_title": "Multi-granularity hierarchical attention fusion networks for reading comprehension and question answering", "paper_year": 2018, "paper_venue": "ACL", "paper_authors": ["Wei Wang", "Chen Wu", "Ming Yan"], "paper_citations": 35, "paper_abstract": "This paper describes a novel hierarchical attention network for reading comprehension style question answering, which aims to answer questions for a given narrative paragraph. In the proposed method, attention and fusion are conducted horizontally and vertically across layers at different levels of granularity between question and paragraph. Specifically, it first encode the question and paragraph with fine-grained language embeddings, to better capture the respective representations at semantic level. Then it proposes a multi-granularity fusion approach to fully fuse information from both global and attended representations. Finally, it introduces a hierarchical attention network to focuses on the answer span progressively with multi-level softalignment. Extensive experiments on the large-scale SQuAD and TriviaQA datasets validate the effectiveness of the proposed method. At the time of writing the paper (Jan. 12th 2018), our model achieves the first position on the SQuAD leaderboard for both single and ensemble models. We also achieves state-of-the-art results on TriviaQA, AddSent and AddOne-Sent datasets.", "citations": ["27e98e09cf09bc13c913d01676e5f32624011050", "df2b0e26d0599ce3e70df8a9da02e51594e0e992"], "references": ["8c1b00128e74f1cd92aede3959690615695d5101", "3c78c6df5eb1695b6a399e346dde880af27d1016", "5171157c2c09a85ad6558c5c03da6b75b0cf5fe6", "3febb2bed8865945e7fddc99efd791887bb7e14f", "1ee46c3b71ebe336d0b278de9093cfca7af7390b", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "b1e20420982a4f923c08652941666b189b11b7fe", "007ab5528b3bd310a80d553cccad4b78dc496b02", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "d1505c6123c102e53eb19dff312cb25cea840b72", "05dd7254b632376973f3a1b4d39485da17814df5"], "score": 1.5173089222997262, "embeddings": [0.5495015060946561, 0.7151105021048291, 0.34614091682461656, -0.001954058257950165, 0.005504995229968321, -0.15001155985891423, 0.06194733480107959, -0.044689576717230714, -0.016504845973798938, -0.039898160486893784, -0.01283056644742461, -0.007338911151005417, 0.02880612458674652, 0.07017826407049559, -0.0783439124551065, 0.03574118677543577, 0.04627879334535722, 0.015326428225101137, -0.03311312663530808, 0.07308160621646469, 0.03971303192701368, 0.003726166852859494, 0.011948391586659047, 0.012124332691035642, 0.008867910716933889, -0.03451707693338563, -0.05985263717026067, 0.053512026770448376, -0.0179995435270619, -0.05004578659098827, 0.016775399506402743, -0.04130191708869729]}, {"paper_id": "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "paper_title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension", "paper_year": 2017, "paper_venue": "ACL", "paper_authors": ["Mandar Joshi", "Eunsol Choi", "Daniel S. Weld", "Luke Zettlemoyer"], "paper_citations": 214, "paper_abstract": "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study. Data and code available at -- this http URL", "citations": ["8c1b00128e74f1cd92aede3959690615695d5101", "3c78c6df5eb1695b6a399e346dde880af27d1016", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "9784fbf77295860b2e412137b86356d70b25e3c0", "26b47e35fe6e4260fdf7b7cc98f279a73c277494"], "references": ["1ee46c3b71ebe336d0b278de9093cfca7af7390b", "b1e20420982a4f923c08652941666b189b11b7fe", "007ab5528b3bd310a80d553cccad4b78dc496b02", "146f6f6ed688c905fb6e346ad02332efd5464616", "d1505c6123c102e53eb19dff312cb25cea840b72", "05dd7254b632376973f3a1b4d39485da17814df5"], "score": 1.4336764188650868, "embeddings": [0.49148167847767454, 0.6777919705826885, 0.3138691887135486, 0.11506553109006379, 0.0008997630399350013, -0.118499449912022, 0.06219186438193529, -0.10463324696845488, -0.06724321517446132, 0.0980039539359606, 0.13162284530480284, -0.03602439587381616, 0.0356834064974459, -0.02021451144654855, -0.063381573864844, 0.11054926765017412, -0.04411020348469048, 0.10408506335436438, -0.04722624041611796, 0.04562210761256494, 0.04920154794611529, 0.126038610842976, 0.045864078289568365, 0.1046571843246737, 0.03425516809011176, -0.17353172784081283, -0.12405999940408144, 0.022917560148471634, -0.031717187164548945, -0.011217847395431024, 0.05469538663589487, -0.08933916080852351]}, {"paper_id": "3c78c6df5eb1695b6a399e346dde880af27d1016", "paper_title": "Simple and Effective Multi-Paragraph Reading Comprehension", "paper_year": 2017, "paper_venue": "ACL", "paper_authors": ["Christopher Clark", "Matt Gardner"], "paper_citations": 96, "paper_abstract": "We consider the problem of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Our proposed solution trains models to produce well calibrated confidence scores for their results on individual paragraphs. We sample multiple paragraphs from the documents during training, and use a shared-normalization training objective that encourages the model to produce globally correct output. We combine this method with a state-of-the-art pipeline for training models on document QA data. Experiments demonstrate strong performance on several document QA datasets. Overall, we are able to achieve a score of 71.3 F1 on the web portion of TriviaQA, a large improvement from the 56.7 F1 of the previous best system.", "citations": ["8c1b00128e74f1cd92aede3959690615695d5101", "27e98e09cf09bc13c913d01676e5f32624011050", "3febb2bed8865945e7fddc99efd791887bb7e14f", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "26b47e35fe6e4260fdf7b7cc98f279a73c277494"], "references": ["0b544dfe355a5070b60986319a3f51fb45d1348e", "1ee46c3b71ebe336d0b278de9093cfca7af7390b", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "007ab5528b3bd310a80d553cccad4b78dc496b02", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "d1505c6123c102e53eb19dff312cb25cea840b72", "05dd7254b632376973f3a1b4d39485da17814df5"], "score": 1.528922496011142, "embeddings": [0.5554156324475059, 0.6726278486266797, 0.3214611252939752, 0.01873454548122488, 0.04221827514212038, -0.17721149595885283, 0.09589760252300045, -0.06223605594258205, -0.05560876810958509, -0.017125211189726555, 0.024285376329238703, -0.03450433676205509, 0.03949992615726444, 0.03225899435823041, -0.08353613526490716, 0.03072193998613198, -0.029002744335811664, 0.0799076939409812, -0.02926946480091885, 0.07152651380918792, 0.05674375970559047, 0.06616936282965415, 0.05426778062766905, 0.029739728725328783, -0.04263148769962031, 0.047214026177054357, -0.09097295439301782, 0.0070916322704736605, -0.019111693915183324, -0.029296444472947148, 0.16074724295494616, -0.09689724525031708]}, {"paper_id": "e0222a1ae6874f7fff128c3da8769ab95963da04", "paper_title": "Reinforced Mnemonic Reader for Machine Reading Comprehension", "paper_year": 2017, "paper_venue": "IJCAI", "paper_authors": ["Minghao Hu", "Yuxing Peng", "Zhen Huang", "Xipeng Qiu", "Furu Wei", "Ming Zhou"], "paper_citations": 46, "paper_abstract": "In this paper, we introduce the Reinforced Mnemonic Reader for machine reading comprehension tasks, which enhances previous attentive readers in two aspects. First, a reattention mechanism is proposed to refine current attentions by directly accessing to past attentions that are temporally memorized in a multi-round alignment architecture, so as to avoid the problems of attention redundancy and attention deficiency. Second, a new optimization approach, called dynamic-critical reinforcement learning, is introduced to extend the standard supervised method. It always encourages to predict a more acceptable answer so as to address the convergence suppression problem occurred in traditional reinforcement learning algorithms. Extensive experiments on the Stanford Question Answering Dataset (SQuAD) show that our model achieves state-of-the-art results. Meanwhile, our model outperforms previous systems by over 6% in terms of both Exact Match and F1 metrics on two adversarial SQuAD datasets.", "citations": ["9784fbf77295860b2e412137b86356d70b25e3c0", "27e98e09cf09bc13c913d01676e5f32624011050", "df2b0e26d0599ce3e70df8a9da02e51594e0e992"], "references": ["5171157c2c09a85ad6558c5c03da6b75b0cf5fe6", "1ee46c3b71ebe336d0b278de9093cfca7af7390b", "3febb2bed8865945e7fddc99efd791887bb7e14f", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "071b16f25117fb6133480c6259227d54fc2a5ea0", "007ab5528b3bd310a80d553cccad4b78dc496b02", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "6c8b30f63f265c32e26d999aa1fef5286b8308ad", "272216c1f097706721096669d85b2843c23fa77d"], "score": 1.4797213606084068, "embeddings": [0.6071458924218682, 0.5104162971253361, 0.08788067595462744, 0.028983900977512404, -0.058468192636957815, -0.06629252369560938, -0.04242399252575696, 0.13666293026190088, 0.07312093632496326, -0.15518899414562462, -0.19929814044439564, 0.12914296932898398, -0.1436607354613915, 0.06081419177078179, 0.047225248914104886, -0.08040628283139245, -0.05416392237622789, -0.11317935050481923, 0.06266606791302351, -0.15088414481036472, -0.04981087310606242, -0.15153279920870386, 0.05869428346790247, 0.0011737765275935891, -0.24796116860047324, 0.17116361535566788, 0.1717287601726078, -0.10617778825906289, -0.01042567317917959, 0.10039130915998197, -0.04536391213063673, -0.0043378724122077866]}, {"paper_id": "007ab5528b3bd310a80d553cccad4b78dc496b02", "paper_title": "Bidirectional Attention Flow for Machine Comprehension", "paper_year": 2016, "paper_venue": "ICLR", "paper_authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "paper_citations": 611, "paper_abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "citations": ["8c1b00128e74f1cd92aede3959690615695d5101", "3c78c6df5eb1695b6a399e346dde880af27d1016", "3febb2bed8865945e7fddc99efd791887bb7e14f", "93b8da28d006415866bf48f9a6e06b5242129195", "27e98e09cf09bc13c913d01676e5f32624011050", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "e0222a1ae6874f7fff128c3da8769ab95963da04", "9784fbf77295860b2e412137b86356d70b25e3c0", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "26b47e35fe6e4260fdf7b7cc98f279a73c277494"], "references": ["1ee46c3b71ebe336d0b278de9093cfca7af7390b", "0b3cfbf79d50dae4a16584533227bb728e3522aa", "071b16f25117fb6133480c6259227d54fc2a5ea0", "b1e20420982a4f923c08652941666b189b11b7fe", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "d1505c6123c102e53eb19dff312cb25cea840b72", "05dd7254b632376973f3a1b4d39485da17814df5", "6c8b30f63f265c32e26d999aa1fef5286b8308ad"], "score": 1.5509678930883157, "embeddings": [0.649591380453491, 0.6812324963878791, 0.2678663541289739, 0.002755640410141749, -0.025028075852356182, -0.07400331242050144, 0.03130193814512828, 0.011948645464958156, 0.021705322543143987, -0.05211415127642602, -0.0033882878371074947, 0.02987381507290164, 0.0030107115574173125, 0.07133111230323498, -0.0035694514767786885, 0.02073264279816767, -0.019639779262747896, 0.059308712136828964, 0.007832793076522791, -0.025991836923527505, -0.006927961650664274, -0.06077063208448072, 0.009129421484915377, 0.00906795856082653, -0.02989772421739737, 0.038604576030055286, 0.04348207136789869, -0.03555212885392713, 0.003727756792822842, 0.017695059559116644, 0.1004708235823662, -0.02881183555756739]}, {"paper_id": "05dd7254b632376973f3a1b4d39485da17814df5", "paper_title": "SQuAD: 100, 000+ Questions for Machine Comprehension of Text", "paper_year": 2016, "paper_venue": "EMNLP", "paper_authors": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang"], "paper_citations": 926, "paper_abstract": "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. \nThe dataset is freely available at this https URL", "citations": ["8c1b00128e74f1cd92aede3959690615695d5101", "3c78c6df5eb1695b6a399e346dde880af27d1016", "5171157c2c09a85ad6558c5c03da6b75b0cf5fe6", "3febb2bed8865945e7fddc99efd791887bb7e14f", "93b8da28d006415866bf48f9a6e06b5242129195", "27e98e09cf09bc13c913d01676e5f32624011050", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "9784fbf77295860b2e412137b86356d70b25e3c0", "007ab5528b3bd310a80d553cccad4b78dc496b02", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "26b47e35fe6e4260fdf7b7cc98f279a73c277494"], "references": ["1ee46c3b71ebe336d0b278de9093cfca7af7390b", "38211dc39e41273c0007889202c69f841e02248a", "b1e20420982a4f923c08652941666b189b11b7fe", "d1505c6123c102e53eb19dff312cb25cea840b72", "0b44fcbeea9415d400c5f5789d6b892b6f98daff"], "score": 1.513463504383514, "embeddings": [0.5990322569456137, 0.6659077141191724, 0.27362213795873114, -0.002124466457965752, -0.05207424252469466, -0.15376691745843415, 0.05480091292722275, -0.1316936424325852, 0.04220734957987448, 0.01728474307634504, 0.08652797214825678, 0.014681773428303386, 0.01357836675706646, 0.027189290980374786, -0.045696812119278334, 0.18316804396728353, 0.036015531934927535, 0.0757638633297528, -0.04635452947479752, 0.04075616515533337, -0.012954272406197885, -0.02138505764171029, -0.0426956819330998, -0.008504825467407943, 0.009733649298002663, 0.033849116751788866, -0.04297906009978585, -0.047728352732593314, 0.08690597134521087, -0.05097825765639184, 0.024072775837518327, 0.018493287542858325]}], [{"paper_id": "9784fbf77295860b2e412137b86356d70b25e3c0", "paper_title": "The Natural Language Decathlon: Multitask Learning as Question Answering", "paper_year": 2018, "paper_venue": "ArXiv", "paper_authors": ["Bryan McCann", "Nitish Shirish Keskar", "Caiming Xiong", "Richard Socher"], "paper_citations": 67, "paper_abstract": "Presented on August 28, 2018 at 12:15 p.m. in the Pettit Microelectronics Research Center, Room 102 A/B.", "citations": ["93b8da28d006415866bf48f9a6e06b5242129195"], "references": ["5171157c2c09a85ad6558c5c03da6b75b0cf5fe6", "93b8da28d006415866bf48f9a6e06b5242129195", "071b16f25117fb6133480c6259227d54fc2a5ea0", "007ab5528b3bd310a80d553cccad4b78dc496b02", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "d7da009f457917aa381619facfa5ffae9329a6e9", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "8c1b00128e74f1cd92aede3959690615695d5101", "e0222a1ae6874f7fff128c3da8769ab95963da04", "93499a7c7f699b6630a86fad964536f9423bb6d0", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "1a07186bc10592f0330655519ad91652125cd907", "4e88de2930a4435f737c3996287a90ff87b95c59", "27e5bd13d581ef682b96038dce4c18f260122352", "39dba6f22d72853561a4ed684be265e179a39e4f", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "128cb6b891aee1b5df099acb48e2efecfcff689f", "687bac2d3320083eb4530bf18bb8f8f721477600", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "0b3cfbf79d50dae4a16584533227bb728e3522aa", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "d1505c6123c102e53eb19dff312cb25cea840b72", "2538e3eb24d26f31482c479d95d2e26c0e79b990", "05dd7254b632376973f3a1b4d39485da17814df5", "0a10d64beb0931efdc24a28edaa91d539194b2e2"], "score": 0.6099798086694943, "embeddings": [0.7779698852041, 0.32078643268330187, 0.09761463622589772, -0.04354088306407855, -0.062103058389185124, 0.30294802850220504, -0.05240763904639499, 0.03683466146636921, 0.06214725575684525, -0.0022470604884807007, 0.21133926049315144, 0.1333485949661713, -0.0715893178334801, -0.026425897424385333, 0.07441259635625457, 0.024833381512170346, 0.06880416650038855, 0.039864427647873624, 0.04440932625017511, -0.15302932896612595, -0.05625218465347666, -0.13600236217199427, -0.08495908944921694, -0.09586613242678234, -0.016456109043151385, -0.08780929883111876, 0.01747264513386545, -0.09230052656162095, -0.06813663100784965, 0.06926010790291075, 0.04905054636091276, 0.06302345354842145]}, {"paper_id": "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "paper_title": "Gated Self-Matching Networks for Reading Comprehension and Question Answering", "paper_year": 2017, "paper_venue": "ACL", "paper_authors": ["Wenhui Wang", "Nan Yang", "Furu Wei", "Baobao Chang", "Ming Zhou"], "paper_citations": 245, "paper_abstract": "In this paper, we present the gated selfmatching networks for reading comprehension style question answering, which aims to answer questions from a given passage. We first match the question and passage with gated attention-based recurrent networks to obtain the question-aware passage representation. Then we propose a self-matching attention mechanism to refine the representation by matching the passage against itself, which effectively encodes information from the whole passage. We finally employ the pointer networks to locate the positions of answers from the passages. We conduct extensive experiments on the SQuAD dataset. The single model achieves 71.3% on the evaluation metrics of exact match on the hidden test set, while the ensemble model further boosts the results to 75.9%. At the time of submission of the paper, our model holds the first place on the SQuAD leaderboard for both single and ensemble model.", "citations": ["8c1b00128e74f1cd92aede3959690615695d5101", "3c78c6df5eb1695b6a399e346dde880af27d1016", "3febb2bed8865945e7fddc99efd791887bb7e14f", "27e98e09cf09bc13c913d01676e5f32624011050", "e0222a1ae6874f7fff128c3da8769ab95963da04", "9784fbf77295860b2e412137b86356d70b25e3c0", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "26b47e35fe6e4260fdf7b7cc98f279a73c277494"], "references": ["5171157c2c09a85ad6558c5c03da6b75b0cf5fe6", "1ee46c3b71ebe336d0b278de9093cfca7af7390b", "0b544dfe355a5070b60986319a3f51fb45d1348e", "0b3cfbf79d50dae4a16584533227bb728e3522aa", "071b16f25117fb6133480c6259227d54fc2a5ea0", "b1e20420982a4f923c08652941666b189b11b7fe", "007ab5528b3bd310a80d553cccad4b78dc496b02", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "47a87c2cbdd928bb081974d308b3d9cf678d257e", "d1505c6123c102e53eb19dff312cb25cea840b72", "05dd7254b632376973f3a1b4d39485da17814df5", "6c8b30f63f265c32e26d999aa1fef5286b8308ad"], "score": 0.5102287506030445, "embeddings": [0.6506513010656263, 0.6564660534122962, 0.20131039068351278, 0.09035375799087693, 0.039306302521306465, -0.055319660121902633, 0.025088135303924702, 0.02570430102071927, 0.05262181517660274, -0.10961633557130399, -0.06271407993702735, 0.018249733980344286, -0.014084427704847887, 0.15517069159387598, -0.03715245732934157, -0.08963367679204644, 0.0355405322545372, 0.025217049024359065, 0.03541052415921942, -0.03017201172423548, -0.057099052699313244, -0.10978425050292452, -0.038238348267379665, -0.020611263230371043, 0.012861418762655903, 0.07735649883106453, 0.07826311680018587, -0.005385693428115273, -0.02387546268714929, 0.012201106798645231, 0.008207374146084198, 0.06344634912430026]}, {"paper_id": "5171157c2c09a85ad6558c5c03da6b75b0cf5fe6", "paper_title": "Dynamic Coattention Networks For Question Answering", "paper_year": 2016, "paper_venue": "ICLR", "paper_authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "paper_citations": 307, "paper_abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "citations": ["8c1b00128e74f1cd92aede3959690615695d5101", "27e98e09cf09bc13c913d01676e5f32624011050", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "e0222a1ae6874f7fff128c3da8769ab95963da04", "9784fbf77295860b2e412137b86356d70b25e3c0", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "26b47e35fe6e4260fdf7b7cc98f279a73c277494"], "references": ["1ee46c3b71ebe336d0b278de9093cfca7af7390b", "0b3cfbf79d50dae4a16584533227bb728e3522aa", "b1e20420982a4f923c08652941666b189b11b7fe", "93499a7c7f699b6630a86fad964536f9423bb6d0", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "d1505c6123c102e53eb19dff312cb25cea840b72", "05dd7254b632376973f3a1b4d39485da17814df5", "6c8b30f63f265c32e26d999aa1fef5286b8308ad", "272216c1f097706721096669d85b2843c23fa77d"], "score": 0.4432770728438928, "embeddings": [0.6069605857205372, 0.6726551597321506, 0.21496744513377067, 0.11638187302573397, -0.011914847903111172, -0.03705263209998405, -0.009818628926924599, 0.08131844574373812, 0.04666307086233217, -0.04984737774197081, -0.047716074044598905, 0.0973962014931262, -0.09221178000511163, 0.08378084957880148, 0.010395933921074511, 0.01526720230907266, 0.008295682574077716, -0.06806669400149082, 0.006989508993405626, -0.08688875578517201, -0.05645628672774004, -0.06525090651490226, -0.04541284684644808, -0.06827205129274908, -0.012218953114410154, 0.061715256605215964, 0.07205690173132644, -0.04878656388942352, 0.026323764440098003, 0.0640295427601638, -0.16125541499831583, 0.10035189513917621]}, {"paper_id": "b1e20420982a4f923c08652941666b189b11b7fe", "paper_title": "A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task", "paper_year": 2016, "paper_venue": "ACL", "paper_authors": ["Danqi Chen", "Jason Bolton", "Christopher D. Manning"], "paper_citations": 261, "paper_abstract": "Enabling a computer to understand a document so that it can answer comprehension questions is a central, yet unsolved goal of NLP. A key factor impeding its solution by machine learned systems is the limited availability of human-annotated data. Hermann et al. (2015) seek to solve this problem by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points, and show that a neural network can then be trained to give good performance on this task. In this paper, we conduct a thorough examination of this new reading comprehension task. Our primary aim is to understand what depth of language understanding is required to do well on this task. We approach this from one side by doing a careful hand-analysis of a small subset of the problems and from the other by showing that simple, carefully designed systems can obtain accuracies of 72.4% and 75.8% on these two datasets, exceeding current state-of-the-art results by over 5% and approaching what we believe is the ceiling for performance on this task.1", "citations": ["5171157c2c09a85ad6558c5c03da6b75b0cf5fe6", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "007ab5528b3bd310a80d553cccad4b78dc496b02", "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "26b47e35fe6e4260fdf7b7cc98f279a73c277494", "05dd7254b632376973f3a1b4d39485da17814df5"], "references": ["0b544dfe355a5070b60986319a3f51fb45d1348e", "1ee46c3b71ebe336d0b278de9093cfca7af7390b", "93499a7c7f699b6630a86fad964536f9423bb6d0", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "d1505c6123c102e53eb19dff312cb25cea840b72"], "score": 0.38667992133093165, "embeddings": [0.5162161303256368, 0.6986743585544272, 0.3117317608635824, 0.1640769891308418, 0.05779039377837945, -0.09836264360954043, 0.08008641640102374, -0.03491012483374934, -0.04609237296569177, 0.0522387671687845, 0.1290583078623282, 0.034060270134439824, -0.04856094542697847, 0.07110169510941086, -0.041898810074728766, 0.03156322711250551, 0.014657741241306692, 0.08094458362239092, -0.01413742828916447, 0.07684191903091449, -0.05525646934205919, 0.11986627133754259, -0.033653028278194316, -0.01920229509008496, 0.11448433066735299, -0.07648222576212999, -0.026564170036006432, -0.0007032564897989224, 0.022089095124104008, 0.011294864796049514, -0.07149815831319316, 0.10300966543653141]}, {"paper_id": "d1505c6123c102e53eb19dff312cb25cea840b72", "paper_title": "Teaching Machines to Read and Comprehend", "paper_year": 2015, "paper_venue": "NIPS", "paper_authors": ["Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "paper_citations": 974, "paper_abstract": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.", "citations": ["8c1b00128e74f1cd92aede3959690615695d5101", "3c78c6df5eb1695b6a399e346dde880af27d1016", "5171157c2c09a85ad6558c5c03da6b75b0cf5fe6", "1ee46c3b71ebe336d0b278de9093cfca7af7390b", "27e98e09cf09bc13c913d01676e5f32624011050", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "9784fbf77295860b2e412137b86356d70b25e3c0", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "b1e20420982a4f923c08652941666b189b11b7fe", "007ab5528b3bd310a80d553cccad4b78dc496b02", "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "26b47e35fe6e4260fdf7b7cc98f279a73c277494", "05dd7254b632376973f3a1b4d39485da17814df5"], "references": ["0b3cfbf79d50dae4a16584533227bb728e3522aa", "766ce989b8b8b984f7a4691fd8c9af4bdb2b74cd", "071b16f25117fb6133480c6259227d54fc2a5ea0", "03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f", "2538e3eb24d26f31482c479d95d2e26c0e79b990"], "score": 0.48639581969114715, "embeddings": [0.6032173745625254, 0.6625127186766004, 0.3457659818399608, 0.06436893343675153, 0.03616027991372596, -0.07963049073628167, 0.0365039457928725, 0.0067198686426703175, 0.014357096441413083, 0.103110896086398, 0.10657014439670214, -0.07517797703924223, 0.12536925351466188, -0.0444048582755694, 0.033486965127378296, 0.011844696576914953, -0.008981652078194884, -0.009672232943021027, -0.024245391064361046, -0.03533297036038173, -0.06745580750483691, -0.013626973558696607, -0.039208153578886275, 0.014749592762624124, 0.053275528976995806, -0.019006470166220747, -0.019055342835083162, -0.0458084436024591, -0.007984780079460806, 0.04715908968019607, -0.035061535145565204, -0.002177171765361539]}, {"paper_id": "1ee46c3b71ebe336d0b278de9093cfca7af7390b", "paper_title": "The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations", "paper_year": 2015, "paper_venue": "ICLR", "paper_authors": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "paper_citations": 293, "paper_abstract": "Abstract: We introduce a new test of how well language models capture meaning in children's books. Unlike standard language modelling benchmarks, it distinguishes the task of predicting syntactic function words from that of predicting lower-frequency words, which carry greater semantic content. We compare a range of state-of-the-art models, each with a different way of encoding what has been previously read. We show that models which store explicit representations of long-term contexts outperform state-of-the-art neural language models at predicting semantic content words, although this advantage is not observed for syntactic function words. Interestingly, we find that the amount of text encoded in a single memory representation is highly influential to the performance: there is a sweet-spot, not too big and not too small, between single words and full sentences that allows the most meaningful information in a text to be effectively retained and recalled. Further, the attention over such window-based memories can be trained effectively through self-supervision. We then assess the generality of this principle by applying it to the CNN QA benchmark, which involves identifying named entities in paraphrased summaries of news articles, and achieve state-of-the-art performance.", "citations": ["8c1b00128e74f1cd92aede3959690615695d5101", "3c78c6df5eb1695b6a399e346dde880af27d1016", "5171157c2c09a85ad6558c5c03da6b75b0cf5fe6", "27e98e09cf09bc13c913d01676e5f32624011050", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "e0222a1ae6874f7fff128c3da8769ab95963da04", "b1e20420982a4f923c08652941666b189b11b7fe", "007ab5528b3bd310a80d553cccad4b78dc496b02", "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "26b47e35fe6e4260fdf7b7cc98f279a73c277494", "05dd7254b632376973f3a1b4d39485da17814df5"], "references": ["93499a7c7f699b6630a86fad964536f9423bb6d0", "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "d1505c6123c102e53eb19dff312cb25cea840b72", "146f6f6ed688c905fb6e346ad02332efd5464616"], "score": 0.4138090611570779, "embeddings": [0.5250313516536793, 0.7031304721038473, 0.3444775618321453, 0.195825260707009, 0.047559483519046354, -0.12997971244353856, 0.05033000835476591, -0.03618642088157868, -0.04242122779119659, 0.02296956990571383, 0.002564284295225504, 0.04088742053809823, -0.07790677637035576, 0.009896103648713642, -0.037257452341752, 0.0511229363725346, -0.054507901212620245, 0.02713925090651814, -0.02874043138371723, -0.01636448158355489, 0.008195381757960783, 0.0623504527252856, 0.03311516081336834, 0.09632671573283728, -0.01274955109911995, -0.033029092966318487, 0.006853835592439461, 0.022205396514435656, -0.02656220782851227, -0.04279437369701772, -0.1088965590152013, -0.02104662651827652]}]], [[{"paper_id": "1e077413b25c4d34945cc2707e17e46ed4fe784a", "paper_title": "Universal Language Model Fine-tuning for Text Classification", "paper_year": 2018, "paper_venue": "ACL", "paper_authors": ["Jeremy Howard", "Sebastian Ruder"], "paper_citations": 332, "paper_abstract": "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.", "citations": ["ac11062f1f368d97f4c826c317bf50dcc13fdb59", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "df2b0e26d0599ce3e70df8a9da02e51594e0e992"], "references": ["87f40e6f3022adbc1f1905e3e506abad05a9964f", "3febb2bed8865945e7fddc99efd791887bb7e14f", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "081651b38ff7533550a3adfc1c00da333a8fe86c", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292"], "score": 1.65285391172832, "embeddings": [0.6019173522259769, -0.13094509388286615, -0.2279597947216819, -0.3704049277935908, -0.08110640526977243, -0.2523795472058548, 0.2592545077344796, 0.1367581637829005, 0.14843026606462054, 0.22348574516079994, -0.0038525094868291287, 0.14291794389836202, -0.1501637700878586, 0.08776948153837258, 0.23984626634027112, -0.07331810262858038, 0.1705499250924141, 0.06755186095525406, -0.01347005925850472, -0.03274952873139933, -0.020552179371469786, -0.00829989793653948, 0.06593296598545206, 0.15040215334515686, 0.03998450156367347, 0.0007888631272130694, -0.08748898294673671, -0.10624002646944052, -0.0047143899675542375, 0.04085152499203481, -0.030036601877068678, -0.06421946531820324]}, {"paper_id": "93b8da28d006415866bf48f9a6e06b5242129195", "paper_title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "paper_year": 2018, "paper_venue": "ICLR", "paper_authors": ["Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman"], "paper_citations": 159, "paper_abstract": "For natural language understanding (NLU) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark (GLUE), a tool for evaluating and analyzing the performance of models across a diverse range of existing NLU tasks. GLUE is model-agnostic, but it incentivizes sharing knowledge across tasks because certain tasks have very limited training data. We further provide a hand-crafted diagnostic test suite that enables detailed linguistic analysis of NLU models. We evaluate baselines based on current methods for multi-task and transfer learning and find that they do not immediately give substantial improvements over the aggregate performance of training a separate model per task, indicating room for improvement in developing general and robust NLU systems.", "citations": ["df2b0e26d0599ce3e70df8a9da02e51594e0e992", "9784fbf77295860b2e412137b86356d70b25e3c0", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "af5c4b80fbf847f69a202ba5a780a3dd18c1a027"], "references": ["071b16f25117fb6133480c6259227d54fc2a5ea0", "26e743d5bd465f49b9538deaf116c15e61b7951f", "007ab5528b3bd310a80d553cccad4b78dc496b02", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "27e5bd13d581ef682b96038dce4c18f260122352", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "272216c1f097706721096669d85b2843c23fa77d", "3febb2bed8865945e7fddc99efd791887bb7e14f", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "475354f10798f110d34792b6d88f31d6d5cb099e", "128cb6b891aee1b5df099acb48e2efecfcff689f", "9784fbf77295860b2e412137b86356d70b25e3c0", "687bac2d3320083eb4530bf18bb8f8f721477600", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "1510cf4b8abea80b9f352325ca4c132887de21a0", "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "2538e3eb24d26f31482c479d95d2e26c0e79b990", "05dd7254b632376973f3a1b4d39485da17814df5", "5d833331b0e22ff359db05c62a8bca18c4f04b68"], "score": 1.7730152805522583, "embeddings": [0.7698740064254925, 0.04832052466825163, -0.02505293234404805, -0.4189717888825176, -0.20090935297321927, 0.3163006189721835, 0.017081340095136524, 0.02471960757421391, -0.03972226191867936, -0.041650378778714796, 0.07253614952521965, -0.05313036535023284, -0.01387622081872331, -0.0216833919527725, 0.016216164080320596, 0.052628326938034806, -0.11576585042380443, 0.09561324752736249, -0.09217546338637281, 0.03748218926637256, -0.03741889009915652, -0.03437861761877435, -0.08784738910647168, -0.07547875406185009, 0.020445934630395363, 0.08071400814576477, 0.007096193463180062, -0.006828493980104114, 0.12335509120086731, -0.04041069697557827, 0.00636264553339785, 0.005234876721131745]}, {"paper_id": "421fc2556836a6b441de806d7b393a35b6eaea58", "paper_title": "Contextual String Embeddings for Sequence Labeling", "paper_year": 2018, "paper_venue": "COLING", "paper_authors": ["Alan Akbik", "Duncan Blythe", "Roland Vollgraf"], "paper_citations": 85, "paper_abstract": "Recent advances in language modeling using recurrent neural networks have made it viable to model language as distributions over characters. By learning to predict the next character on the basis of previous characters, such models have been shown to automatically internalize linguistic concepts such as words, sentences, subclauses and even sentiment. In this paper, we propose to leverage the internal states of a trained character language model to produce a novel type of word embedding which we refer to as contextual string embeddings. Our proposed embeddings have the distinct properties that they (a) are trained without any explicit notion of words and thus fundamentally model words as sequences of characters, and (b) are contextualized by their surrounding text, meaning that the same word will have different embeddings depending on its contextual use. We conduct a comparative evaluation against previous embeddings and find that our embeddings are highly useful for downstream tasks: across four classic sequence labeling tasks we consistently outperform the previous state-of-the-art. In particular, we significantly outperform previous work on English and German named entity recognition (NER), allowing us to report new state-of-the-art F1-scores on the CONLL03 shared task. We release all code and pre-trained language models in a simple-to-use framework to the research community, to enable reproduction of these experiments and application of our proposed embeddings to other tasks: https://github.com/zalandoresearch/flair", "citations": ["df2b0e26d0599ce3e70df8a9da02e51594e0e992"], "references": ["87f40e6f3022adbc1f1905e3e506abad05a9964f", "39dba6f22d72853561a4ed684be265e179a39e4f", "3febb2bed8865945e7fddc99efd791887bb7e14f", "0b3cfbf79d50dae4a16584533227bb728e3522aa", "02b3d1d162080d9aefd3fc30a0bcc9a843073b5d", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "652d159bf64a70194127722d19841daa99a69b64", "5d833331b0e22ff359db05c62a8bca18c4f04b68"], "score": 1.569804902801678, "embeddings": [0.6010967922276224, -0.06122350298709937, -0.26945038905524965, -0.18340715910480757, 0.35524665616698625, -0.14081411736563965, 0.21024915955906223, -0.12844606235226316, 0.02431145716595673, -0.2584825262312039, -0.012189762923378747, -0.07122061016880488, 0.15815592824864005, 0.018405648017768436, 0.07724122052829284, -0.010554048792208058, -0.07745810570807439, -0.013435297590686042, -0.10509777496292247, -0.23917205907704217, 0.2142742196915986, 0.10295439815265812, 0.012506529147468757, -0.15158142585166687, 0.020039616574549834, -0.12387066407239675, 0.13647109150185435, -0.11577416555008645, -0.09438016009132152, 0.049075171822133674, 0.0448290112370663, -0.05226084506878784]}, {"paper_id": "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "paper_title": "Dissecting Contextual Word Embeddings: Architecture and Representation", "paper_year": 2018, "paper_venue": "EMNLP", "paper_authors": ["Matthew E. Peters", "Mark Neumann", "Luke Zettlemoyer", "Wen-tau Yih"], "paper_citations": 43, "paper_abstract": "Contextual word representations derived from pre-trained bidirectional language models (biLMs) have recently been shown to provide significant improvements to the state of the art for a wide range of NLP tasks. However, many questions remain as to how and why these models are so effective. In this paper, we present a detailed empirical study of how the choice of neural architecture (e.g. LSTM, CNN, or self attention) influences both end task accuracy and qualitative properties of the representations that are learned. We show there is a tradeoff between speed and accuracy, but all architectures learn high quality contextual representations that outperform word embeddings for four challenging NLP tasks. Additionally, all architectures learn representations that vary with network depth, from exclusively morphological based at the word embedding layer through local syntax based in the lower contextual layers to longer range semantics such coreference at the upper layers. Together, these results suggest that unsupervised biLMs, independent of architecture, are learning much more about the structure of language than previously appreciated.", "citations": ["df2b0e26d0599ce3e70df8a9da02e51594e0e992"], "references": ["5d833331b0e22ff359db05c62a8bca18c4f04b68", "3febb2bed8865945e7fddc99efd791887bb7e14f", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "02b3d1d162080d9aefd3fc30a0bcc9a843073b5d", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "1e077413b25c4d34945cc2707e17e46ed4fe784a", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "0c7f52c753a65ceaf3755e20b906ffd0c05c994a", "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb", "0a10d64beb0931efdc24a28edaa91d539194b2e2"], "score": 1.6281103973362394, "embeddings": [0.6167666035074768, -0.05163279596924145, -0.300717727151608, -0.4109566150308136, 0.09943025939507358, -0.2620448539259567, 0.13528628473502227, -0.020427854297486964, 0.22855747770497623, -0.12655741584153216, 0.06610356890716446, 0.13444552500532495, -0.0661877774118154, -0.10334432874817655, 0.03932545193152355, 0.12870194297852916, 0.07016204330946942, -0.08030990126045226, 0.1844215464329788, 0.1918248600764111, -0.08873430287533404, -0.006941581619727736, 0.00698591754114089, 0.11591020767921256, -0.11914979278081289, -0.08254206148613531, -0.01689607558210892, -0.0008167999621317533, 0.009407757593263621, 0.04179080971719018, 0.09389920707965742, 0.030600028123514894]}, {"paper_id": "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "paper_title": "Neural Network Acceptability Judgments", "paper_year": 2018, "paper_venue": "ArXiv", "paper_authors": ["Alex Warstadt", "Amanpreet Singh", "Samuel R. Bowman"], "paper_citations": 34, "paper_abstract": "In this work, we explore the ability of artificial neural networks to judge the grammatical acceptability of a sentence. Machine learning research of this kind is well placed to answer important open questions about the role of prior linguistic bias in language acquisition by providing a test for the Poverty of the Stimulus Argument. In service of this goal, we introduce the Corpus of Linguistic Acceptability (CoLA), a set of 10,657 English sentences labeled as grammatical or ungrammatical by expert linguists. We train several recurrent neural networks to do binary acceptability classification. These models set a baseline for the task. Error-analysis testing the models on specific grammatical phenomena reveals that they learn some systematic grammatical generalizations like subject-verb-object word order without any grammatical supervision. We find that neural sequence models show promise on the acceptability classification task. However, human-like performance across a wide range of grammatical constructions remains far off.", "citations": ["93b8da28d006415866bf48f9a6e06b5242129195", "df2b0e26d0599ce3e70df8a9da02e51594e0e992"], "references": ["39dba6f22d72853561a4ed684be265e179a39e4f", "0b544dfe355a5070b60986319a3f51fb45d1348e", "3febb2bed8865945e7fddc99efd791887bb7e14f", "93b8da28d006415866bf48f9a6e06b5242129195", "0b3cfbf79d50dae4a16584533227bb728e3522aa", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "a4cec122a08216fe8a3bc19b22e78fbaea096256", "272216c1f097706721096669d85b2843c23fa77d"], "score": 1.6034321363231587, "embeddings": [0.670589888736154, -0.021878166002757556, -0.17078287186321778, -0.021903675762251218, -0.029290392277269255, 0.15511350748615352, 0.029724841455287754, 0.20453607909825053, -0.13536346233143196, -0.08618220511469857, -0.037202205239922646, 0.014903600991875918, 0.12698455859677854, -0.04453299542799264, -0.06105261891391353, -0.14058109950826303, -0.15658102154556802, 0.09368170773552492, -0.16738312589390764, 0.06048968038553947, 0.13688611771628303, 0.09562221847805544, -0.007325790742504871, -0.22218051213273132, -0.034315296581248496, 0.32886730518516366, -0.14023152769048594, -0.15154827668725143, 0.21543484418098413, -0.15809270529851305, 0.1338427109863282, -0.006338982701788832]}, {"paper_id": "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "paper_title": "Semi-Supervised Sequence Modeling with Cross-View Training", "paper_year": 2018, "paper_venue": "EMNLP", "paper_authors": ["Kevin Clark", "Minh-Thang Luong", "Christopher D. Manning", "Quoc V. Le"], "paper_citations": 32, "paper_abstract": "Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models, mainly because they can take advantage of large amounts of unlabeled text. However, the supervised models only learn from task-specific labeled data during the main training phase. We therefore propose Cross-View Training (CVT), a semi-supervised learning algorithm that improves the representations of a Bi-LSTM sentence encoder using a mix of labeled and unlabeled data. On labeled examples, standard supervised learning is used. On unlabeled examples, CVT teaches auxiliary prediction modules that see restricted views of the input (e.g., only part of a sentence) to match the predictions of the full model seeing the whole input. Since the auxiliary modules and the full model share intermediate representations, this in turn improves the full model. Moreover, we show that CVT is particularly effective when combined with multi-task learning. We evaluate CVT on five sequence tagging tasks, machine translation, and dependency parsing, achieving state-of-the-art results.", "citations": ["df2b0e26d0599ce3e70df8a9da02e51594e0e992"], "references": ["26e743d5bd465f49b9538deaf116c15e61b7951f", "071b16f25117fb6133480c6259227d54fc2a5ea0", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "1e077413b25c4d34945cc2707e17e46ed4fe784a", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "93499a7c7f699b6630a86fad964536f9423bb6d0", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "1a07186bc10592f0330655519ad91652125cd907", "39dba6f22d72853561a4ed684be265e179a39e4f", "3febb2bed8865945e7fddc99efd791887bb7e14f", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "0b3cfbf79d50dae4a16584533227bb728e3522aa", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "2538e3eb24d26f31482c479d95d2e26c0e79b990", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb"], "score": 1.6626059752089497, "embeddings": [0.744732840987097, -0.11084248826775396, -0.1601992614601399, -0.28847665176740717, 0.15595901072855042, -0.06070788282430344, 0.209353201675672, 0.19362867534870518, 0.10113561587491982, 0.15486697740458147, 0.1020196840335583, 0.14590039891766068, -0.0007249048054293065, 0.09317051011023675, 0.0969372267698963, -0.006188566502371234, 0.05384445952174417, 0.0746562348883706, 0.06977142514363657, -0.034979391613220556, -0.008558966093589191, 0.08974049284039971, 0.07216563669924207, -0.08144986024368327, 0.17020017321794098, -0.03539826867282888, 0.09861138368642933, -0.10651965866774508, 0.18883057532418335, 0.07485136219120576, -0.050274441784172, 0.03520467259845537]}, {"paper_id": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "paper_title": "Improving Language Understanding by Generative Pre-Training", "paper_year": 2018, "paper_venue": "", "paper_authors": ["Alec Radford"], "paper_citations": 378, "paper_abstract": "Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).", "citations": ["ac11062f1f368d97f4c826c317bf50dcc13fdb59", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "df2b0e26d0599ce3e70df8a9da02e51594e0e992"], "references": ["93b8da28d006415866bf48f9a6e06b5242129195", "1e077413b25c4d34945cc2707e17e46ed4fe784a", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "4361e64f2d12d63476fdc88faf72a0f70d9a2ffb", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "1a07186bc10592f0330655519ad91652125cd907", "272216c1f097706721096669d85b2843c23fa77d", "475354f10798f110d34792b6d88f31d6d5cb099e", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "3febb2bed8865945e7fddc99efd791887bb7e14f", "843959ffdccf31c6694d135fad07425924f785b1", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "687bac2d3320083eb4530bf18bb8f8f721477600", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "1510cf4b8abea80b9f352325ca4c132887de21a0", "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "2538e3eb24d26f31482c479d95d2e26c0e79b990", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "d1505c6123c102e53eb19dff312cb25cea840b72", "a97dc52807d80454e78d255f9fbd7b0fab56bd03", "0e6824e137847be0599bb0032e37042ed2ef5045", "bc1d609520290e0460c49b685675eb5a57fa5935"], "score": 1.848792586178666, "embeddings": [0.7787033462331335, -0.13934233883047845, -0.07304412176619511, -0.39423532494187485, -0.10944070051835739, 0.07498554106515962, 0.12162190321961362, 0.2511065728574791, -0.09089376099150799, 0.17859024803289583, -0.12608586833055135, -0.07321267201070253, -0.022028879532123364, 0.04963982216870537, 0.022893261910344187, 0.12453864689722201, -0.009240781127639446, -0.02647720062087735, -0.015615243970778892, 0.010728655057271937, -0.0474789181049137, -0.0006402051016351992, -0.039530137234018305, 0.06290757717021818, 0.045636648881379055, -0.05700373065802117, -0.08801862754725083, -0.04139008740474004, -0.07111004147401091, -0.03275771423934815, -0.07270895314492921, 0.04975192916416649]}, {"paper_id": "af5c4b80fbf847f69a202ba5a780a3dd18c1a027", "paper_title": "SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference", "paper_year": 2018, "paper_venue": "EMNLP", "paper_authors": ["Rowan Zellers", "Yonatan Bisk", "Roy Schwartz", "Yejin Choi"], "paper_citations": 64, "paper_abstract": "Given a partial description like \"she opened the hood of the car,\" humans can reason about the situation and anticipate what might come next (\"then, she examined the engine\"). In this paper, we introduce the task of grounded commonsense inference, unifying natural language inference and commonsense reasoning. \nWe present SWAG, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations. To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data. To account for the aggressive adversarial filtering, we use state-of-the-art language models to massively oversample a diverse set of potential counterfactuals. Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88%), various competitive models struggle on our task. We provide comprehensive analysis that indicates significant opportunities for future research.", "citations": ["df2b0e26d0599ce3e70df8a9da02e51594e0e992"], "references": ["3febb2bed8865945e7fddc99efd791887bb7e14f", "93b8da28d006415866bf48f9a6e06b5242129195", "0b3cfbf79d50dae4a16584533227bb728e3522aa", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "27e5bd13d581ef682b96038dce4c18f260122352", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "0e6824e137847be0599bb0032e37042ed2ef5045"], "score": 1.6036821362611438, "embeddings": [0.601602682739451, 0.04184166256122149, -0.10200471244804624, -0.3510638069128929, -0.28041221154764706, 0.39972544020522077, -0.051360998896965465, 0.01727029393664633, -0.01686031236903678, -0.2096552957153965, -0.049068798189173646, 0.05743293119200284, 0.16364996974251, 0.027532481771268606, -0.1431620297364181, -0.23655958963481385, 0.06948534755530722, -0.04073785761516473, -0.05801854400755255, 0.15016168421607495, 0.17283482556531166, 0.06580768512760247, -0.09522343641636924, 0.12291906682848411, 0.06476775493244262, -0.048376897410641294, -0.006911926013648146, 0.024018842384442275, 0.02456842681805347, -0.0467557436551267, -0.0019938759721426177, -0.08154062891268707]}, {"paper_id": "3febb2bed8865945e7fddc99efd791887bb7e14f", "paper_title": "Deep contextualized word representations", "paper_year": 2018, "paper_venue": "NAACL-HLT", "paper_authors": ["Matthew E. Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer"], "paper_citations": 1004, "paper_abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pretrained network is crucial, allowing downstream models to mix different types of semi-supervision signals.", "citations": ["27e98e09cf09bc13c913d01676e5f32624011050", "93b8da28d006415866bf48f9a6e06b5242129195", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "e0222a1ae6874f7fff128c3da8769ab95963da04", "421fc2556836a6b441de806d7b393a35b6eaea58", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "1e077413b25c4d34945cc2707e17e46ed4fe784a", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "26b47e35fe6e4260fdf7b7cc98f279a73c277494", "af5c4b80fbf847f69a202ba5a780a3dd18c1a027"], "references": ["02b3d1d162080d9aefd3fc30a0bcc9a843073b5d", "007ab5528b3bd310a80d553cccad4b78dc496b02", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "0c7f52c753a65ceaf3755e20b906ffd0c05c994a", "272216c1f097706721096669d85b2843c23fa77d", "3c78c6df5eb1695b6a399e346dde880af27d1016", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "59761abc736397539bdd01ad7f9d91c8607c0457", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "687bac2d3320083eb4530bf18bb8f8f721477600", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "0b3cfbf79d50dae4a16584533227bb728e3522aa", "2538e3eb24d26f31482c479d95d2e26c0e79b990", "05dd7254b632376973f3a1b4d39485da17814df5", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "6c8b30f63f265c32e26d999aa1fef5286b8308ad", "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb", "dac72f2c509aee67524d3321f77e97e8eff51de6"], "score": 1.7918553841191236, "embeddings": [0.8011622970797914, 0.1478702966736858, -0.026989261767598225, -0.38960269764485367, 0.09424130357485501, -0.19549314609015492, 0.12313297947988247, 0.075781240012639, 0.13552401021729055, -0.17064491340397128, -0.051907098724565524, 0.06964187229142588, -0.004067314248678777, -0.0016422139493335395, -0.053549364070337675, 0.005997904982860702, -0.0364666383743635, -0.030350427222749245, 0.008299470059319155, -0.03684680089714187, 0.1153792664249582, -0.011553044309041723, 0.015119382454610192, 0.006599345925223986, -0.0011901991921502925, 0.11884871768093325, 0.06795212835418038, -0.04871222953370539, 0.06325761213478029, -0.0533793797630879, 0.09434132146904767, -0.05944840677627789]}, {"paper_id": "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "paper_title": "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data", "paper_year": 2017, "paper_venue": "EMNLP", "paper_authors": ["Alexis Conneau", "Douwe Kiela", "Holger Schwenk", "Lo\u00efc Barrault", "Antoine Bordes"], "paper_citations": 474, "paper_abstract": "Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.", "citations": ["a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "93b8da28d006415866bf48f9a6e06b5242129195", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "1e077413b25c4d34945cc2707e17e46ed4fe784a", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "af5c4b80fbf847f69a202ba5a780a3dd18c1a027", "bc1d609520290e0460c49b685675eb5a57fa5935"], "references": ["87f40e6f3022adbc1f1905e3e506abad05a9964f", "39dba6f22d72853561a4ed684be265e179a39e4f", "38211dc39e41273c0007889202c69f841e02248a", "0b3cfbf79d50dae4a16584533227bb728e3522aa", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "23694a80bf1b9b38215be3e23068dd75296bc90f", "26e743d5bd465f49b9538deaf116c15e61b7951f", "2538e3eb24d26f31482c479d95d2e26c0e79b990", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "4e88de2930a4435f737c3996287a90ff87b95c59", "1a07186bc10592f0330655519ad91652125cd907", "71b7178df5d2b112d07e45038cb5637208659ff7", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "1510cf4b8abea80b9f352325ca4c132887de21a0", "0e6824e137847be0599bb0032e37042ed2ef5045", "272216c1f097706721096669d85b2843c23fa77d"], "score": 1.7168021239747189, "embeddings": [0.7804574232401523, -0.19070697874701714, -0.10213509056985615, -0.21820584417079433, -0.23840100211329354, 0.14338957369795316, 0.16653036891035783, 0.17162042031445476, -0.16518150485220595, 0.060935646133736594, 0.03789084653294764, 0.14909035132439416, 0.10237266859921125, 0.06960376446453825, 0.03503707318925042, -0.11171942879026481, 0.02948892870119783, -0.017331462882890305, -0.11529896383923807, 0.030798373799507034, -0.029173924928719334, 0.08921294590827726, -0.10190549103911407, -0.01051749142455012, 0.0031980463476340916, 0.02667779131641138, -0.07047174211788243, -0.14233358467630297, 0.09851984888316799, -0.0027962937874513464, -0.08822295321543945, 0.005713702557951943]}, {"paper_id": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "paper_title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference", "paper_year": 2017, "paper_venue": "NAACL-HLT", "paper_authors": ["Adina Williams", "Nikita Nangia", "Samuel R. Bowman"], "paper_citations": 314, "paper_abstract": "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. In addition to being one of the largest corpora available for the task of NLI, at 433k examples, this corpus improves upon available resources in its coverage: it offers data from ten distinct genres of written and spoken English--making it possible to evaluate systems on nearly the full complexity of the language--and it offers an explicit setting for the evaluation of cross-genre domain adaptation.", "citations": ["93b8da28d006415866bf48f9a6e06b5242129195", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "9784fbf77295860b2e412137b86356d70b25e3c0", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "af5c4b80fbf847f69a202ba5a780a3dd18c1a027"], "references": ["0b3cfbf79d50dae4a16584533227bb728e3522aa", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "27e5bd13d581ef682b96038dce4c18f260122352", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "9fa8d73e572c3ca824a04a5f551b602a17831bc5", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "6c8b30f63f265c32e26d999aa1fef5286b8308ad", "272216c1f097706721096669d85b2843c23fa77d"], "score": 1.6087085271824306, "embeddings": [0.6464778013013988, 0.0900137290331737, -0.19285082421798103, -0.3632084628511101, -0.2658068890234478, 0.24964198342356086, -0.23693604741162572, -0.12037961406452634, 0.10916457602869628, -0.1796404027911146, -0.15163323557929712, 0.10711089561294604, 0.08894360999885448, -0.011518642230201987, -0.10112747146624675, -0.029491885877464805, 0.14364951962922334, -0.06124044563821056, 0.00659537688646903, 0.1125757864448675, 0.013296380997869816, 0.12823101586858776, -0.137031666687445, 0.11409531423609669, 0.03093664769182273, -0.01785993075138759, 0.01276787008453278, -0.028993842061442635, 0.043513224289694716, 0.08383422002014941, -0.07938460147702514, 0.020824275781730718]}, {"paper_id": "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "paper_title": "Learned in Translation: Contextualized Word Vectors", "paper_year": 2017, "paper_venue": "NIPS", "paper_authors": ["Bryan McCann", "James Bradbury", "Caiming Xiong", "Richard Socher"], "paper_citations": 244, "paper_abstract": "Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.", "citations": ["3febb2bed8865945e7fddc99efd791887bb7e14f", "93b8da28d006415866bf48f9a6e06b5242129195", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "9784fbf77295860b2e412137b86356d70b25e3c0", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "1e077413b25c4d34945cc2707e17e46ed4fe784a", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e"], "references": ["5171157c2c09a85ad6558c5c03da6b75b0cf5fe6", "071b16f25117fb6133480c6259227d54fc2a5ea0", "26e743d5bd465f49b9538deaf116c15e61b7951f", "007ab5528b3bd310a80d553cccad4b78dc496b02", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "061356704ec86334dbbc073985375fe13cd39088", "93499a7c7f699b6630a86fad964536f9423bb6d0", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "39dba6f22d72853561a4ed684be265e179a39e4f", "38211dc39e41273c0007889202c69f841e02248a", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "687bac2d3320083eb4530bf18bb8f8f721477600", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "79dc84a3bf76f1cb983902e2591d913cee5bdb0e", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "2538e3eb24d26f31482c479d95d2e26c0e79b990", "05dd7254b632376973f3a1b4d39485da17814df5", "0a10d64beb0931efdc24a28edaa91d539194b2e2"], "score": 1.6566619686303246, "embeddings": [0.8187521764993311, 0.0874492285141973, -0.08669477910237197, -0.0937116194751229, -0.23850987264943402, -0.01957830966210624, 0.10657447071047392, 0.12488442798383254, 0.18563802351650252, 0.08273266204320225, 0.07906023619220123, 0.15129638339175214, -0.043477331117607015, 0.07742790342928527, 0.07981730138676194, 0.0014231048613335042, 0.12760145914900758, 0.0822696134305053, 0.05612833542911696, -0.024640085937599334, -0.16073668200320912, -0.15562144119805257, -0.11162103310624243, -0.054267124537792914, 0.10949438584797556, -0.04455370109756373, 0.07793274923626155, -0.07695825082975113, 0.1105013926173389, -0.014508580212501315, 0.07493774519152194, 0.022279554495240626]}, {"paper_id": "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "paper_title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation", "paper_year": 2017, "paper_venue": "SemEval@ACL", "paper_authors": ["Daniel Matthew Cer", "Mona T. Diab", "Eneko Agirre", "I\u00f1igo Lopez-Gazpio", "Lucia Specia"], "paper_citations": 156, "paper_abstract": "Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in all language tracks. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the STS Benchmark is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017).", "citations": ["cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "93b8da28d006415866bf48f9a6e06b5242129195", "df2b0e26d0599ce3e70df8a9da02e51594e0e992"], "references": ["87f40e6f3022adbc1f1905e3e506abad05a9964f", "0b3cfbf79d50dae4a16584533227bb728e3522aa", "26e743d5bd465f49b9538deaf116c15e61b7951f", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "1510cf4b8abea80b9f352325ca4c132887de21a0", "d87ceda3042f781c341ac17109d1e94a717f5f60", "0a10d64beb0931efdc24a28edaa91d539194b2e2"], "score": 1.643353078429314, "embeddings": [0.6550959818702851, -0.18202392599057104, 0.037347026931299966, -0.30638166741976697, -0.19580943229233183, 0.18058551961708325, 0.233799849714195, 0.06548317262210082, -0.3460805329905463, -0.018487386362608806, 0.08250835644671164, 0.1081796520360723, 0.13330188493713138, 0.16708126161689843, 0.1149465638895241, 0.0602577490438159, -0.03823409996516454, -0.07034322484089026, 0.050625821765366424, 0.05544760199066985, -0.04582887582283409, 0.009619624661912431, -0.12586507884928677, -0.10695903062961555, 0.11610164095231865, 0.07442462163472396, -0.11328472712516605, 0.0696935196545325, -0.009755833397216333, 0.1790780671708717, -0.02982117574892668, 0.010523265659944044]}, {"paper_id": "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "paper_title": "Semi-supervised sequence tagging with bidirectional language models", "paper_year": 2017, "paper_venue": "ACL", "paper_authors": ["Matthew E. Peters", "Waleed Ammar", "Chandra Bhagavatula", "Russell Power"], "paper_citations": 145, "paper_abstract": "Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks. However, in most cases, the recurrent network that operates on word-level representations to produce context sensitive representations is trained on relatively little labeled data. In this paper, we demonstrate a general semi-supervised approach for adding pre- trained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks. We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers.", "citations": ["3febb2bed8865945e7fddc99efd791887bb7e14f", "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "421fc2556836a6b441de806d7b393a35b6eaea58", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "1e077413b25c4d34945cc2707e17e46ed4fe784a", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e"], "references": ["87f40e6f3022adbc1f1905e3e506abad05a9964f", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "23694a80bf1b9b38215be3e23068dd75296bc90f", "0b3cfbf79d50dae4a16584533227bb728e3522aa", "02b3d1d162080d9aefd3fc30a0bcc9a843073b5d", "26e743d5bd465f49b9538deaf116c15e61b7951f", "59761abc736397539bdd01ad7f9d91c8607c0457", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "47a87c2cbdd928bb081974d308b3d9cf678d257e", "1510cf4b8abea80b9f352325ca4c132887de21a0", "2538e3eb24d26f31482c479d95d2e26c0e79b990", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "0c7f52c753a65ceaf3755e20b906ffd0c05c994a", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb", "272216c1f097706721096669d85b2843c23fa77d"], "score": 1.7075766507275656, "embeddings": [0.7187590440314231, -0.2012545619504554, -0.1618933695178561, -0.34310987269037896, 0.27845914789363213, -0.22676339946962695, 0.27481883020715075, 0.11090293464297439, 0.059808675865902904, -0.0456299853601377, 0.0632663367377178, 0.004475536925708475, -0.030535996500089417, 0.0007819145766287384, -0.07047365675193314, -0.03287668387593138, -0.08432907464926918, -0.034848935229911504, 0.053702071513384096, -0.0659439444179566, 0.05907943125176701, 0.0375423373776136, 0.11005685952014227, 0.022419660949609812, 0.07749257209526936, 0.013901941322428663, 0.023717813107119984, 0.05983541076055932, 0.044163295933771815, 0.08637271083919595, -0.1067041746646894, -0.03590244322323094]}, {"paper_id": "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "paper_title": "A Decomposable Attention Model for Natural Language Inference", "paper_year": 2016, "paper_venue": "EMNLP", "paper_authors": ["Ankur P. Parikh", "Oscar T\u00e4ckstr\u00f6m", "Dipanjan Das", "Jakob Uszkoreit"], "paper_citations": 410, "paper_abstract": "We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.", "citations": ["204e3073870fae3d05bcbc2f6a8e263d9b72e776", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "26b47e35fe6e4260fdf7b7cc98f279a73c277494", "af5c4b80fbf847f69a202ba5a780a3dd18c1a027"], "references": ["0b3cfbf79d50dae4a16584533227bb728e3522aa", "071b16f25117fb6133480c6259227d54fc2a5ea0", "05aba481e8a221df5d8775a3bb749001e7f2525e", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "6c8b30f63f265c32e26d999aa1fef5286b8308ad"], "score": 1.557533865724822, "embeddings": [0.6310707272447296, 0.2635160813259265, -0.05584491679903988, -0.10586891655168501, -0.17891389864343085, 0.23027469439140583, -0.14605603430204336, 0.047629575210053406, 0.1299839729965419, -0.24598423912728234, -0.24365687404593137, 0.028193351402470698, 0.12327871806201855, 0.10985489604673644, -0.025193757416634845, -0.18190223553407042, 0.24482725646963252, -0.09861707083939335, -0.03719522436284482, 0.16172817900921543, -0.0010699179561722222, -0.1311315853782539, -0.09673091967151422, -0.02321415221101488, 0.08620826301268968, -0.14930960030141346, 0.08772825200955026, 0.20781999517485214, 0.01887422887632741, 0.016630927835778672, 0.0723986512404043, -0.0023349092703019657]}, {"paper_id": "59761abc736397539bdd01ad7f9d91c8607c0457", "paper_title": "context2vec: Learning Generic Context Embedding with Bidirectional LSTM", "paper_year": 2016, "paper_venue": "CoNLL", "paper_authors": ["Oren Melamud", "Jacob Goldberger", "Ido Dagan"], "paper_citations": 124, "paper_abstract": "Context representations are central to various NLP tasks, such as word sense disambiguation, named entity recognition, coreference resolution, and many more. In this work we present a neural model for efficiently learning a generic context embedding function from large corpora, using bidirectional LSTM. With a very simple application of our context representations, we manage to surpass or nearly reach state-of-the-art results on sentence completion, lexical substitution and word sense disambiguation tasks, while substantially outperforming the popular context representation of averaged word embeddings. We release our code and pretrained models, suggesting they could be useful in a wide variety of NLP tasks.", "citations": ["3febb2bed8865945e7fddc99efd791887bb7e14f", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "df2b0e26d0599ce3e70df8a9da02e51594e0e992"], "references": ["87f40e6f3022adbc1f1905e3e506abad05a9964f", "0a10d64beb0931efdc24a28edaa91d539194b2e2", "02b3d1d162080d9aefd3fc30a0bcc9a843073b5d", "cfa2646776405d50533055ceb1b7f050e9014dcb", "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "272216c1f097706721096669d85b2843c23fa77d", "dac72f2c509aee67524d3321f77e97e8eff51de6"], "score": 1.6026684261281936, "embeddings": [0.6347139047232421, -0.24596473820970882, 0.13516209358366915, -0.1502081333378175, 0.15059504505035015, -0.17696796340722937, 0.19977426895183584, 0.016735646834350332, -0.17037163607030678, -0.2542064661083002, -0.08810804028787309, 0.1577693860766357, -0.11363572355771559, -0.34093139355968793, -0.11165129842583472, 0.04349206336149403, -0.009753236053303657, -0.011667708481504653, 0.04435687557227543, -0.07383609153359473, 0.16513396782664852, -0.001078314448904343, -0.07595948955896685, -0.055612236509696905, -0.08779220298992861, 0.08050099022551233, -0.049444349651715166, 0.10954231082412325, -0.06901036601730717, 0.1573038539801493, -0.04933902205126188, -0.1657061866179179]}, {"paper_id": "6e795c6e9916174ae12349f5dc3f516570c17ce8", "paper_title": "Skip-Thought Vectors", "paper_year": 2015, "paper_venue": "NIPS", "paper_authors": ["Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S. Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler"], "paper_citations": 846, "paper_abstract": "We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.", "citations": ["a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "93b8da28d006415866bf48f9a6e06b5242129195", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "59761abc736397539bdd01ad7f9d91c8607c0457", "9784fbf77295860b2e412137b86356d70b25e3c0", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "a97dc52807d80454e78d255f9fbd7b0fab56bd03", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "0e6824e137847be0599bb0032e37042ed2ef5045", "af5c4b80fbf847f69a202ba5a780a3dd18c1a027", "bc1d609520290e0460c49b685675eb5a57fa5935"], "references": ["39dba6f22d72853561a4ed684be265e179a39e4f", "0a10d64beb0931efdc24a28edaa91d539194b2e2", "0b544dfe355a5070b60986319a3f51fb45d1348e", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "0b3cfbf79d50dae4a16584533227bb728e3522aa", "79dc84a3bf76f1cb983902e2591d913cee5bdb0e", "071b16f25117fb6133480c6259227d54fc2a5ea0", "03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f", "061356704ec86334dbbc073985375fe13cd39088", "687bac2d3320083eb4530bf18bb8f8f721477600", "4e88de2930a4435f737c3996287a90ff87b95c59", "0e6824e137847be0599bb0032e37042ed2ef5045", "1510cf4b8abea80b9f352325ca4c132887de21a0", "71b7178df5d2b112d07e45038cb5637208659ff7", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "272216c1f097706721096669d85b2843c23fa77d"], "score": 1.6612555756885954, "embeddings": [0.8140991671772526, -0.23426953641909679, -0.03674649470282241, 0.04358961610383749, -0.14630821597817098, 0.21068778791324097, 0.2525725096105131, 0.18933042545010442, -0.11783072108345763, 0.053143511641019894, 0.010240747658063716, 0.06489534672882404, 0.15686968192221307, -0.10114433542345605, -0.14408906681822523, -0.042746110520128125, -0.05493791824588571, 0.05027362198730011, -0.039460657715080526, 0.018232465073457684, -0.008240216862366053, 0.018859843249385822, -0.006268269419728292, -0.02348919298935109, -0.04868121296666829, 0.030861549895192852, -0.04852559569732857, -0.06761455212279671, 0.05800276784753515, 0.06129879287387405, 0.05978848052710694, 0.07634662993726726]}, {"paper_id": "f04df4e20a18358ea2f689b4c129781628ef7fc1", "paper_title": "A large annotated corpus for learning natural language inference", "paper_year": 2015, "paper_venue": "EMNLP", "paper_authors": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning"], "paper_citations": 808, "paper_abstract": "Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.", "citations": ["3febb2bed8865945e7fddc99efd791887bb7e14f", "93b8da28d006415866bf48f9a6e06b5242129195", "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "9784fbf77295860b2e412137b86356d70b25e3c0", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "af5c4b80fbf847f69a202ba5a780a3dd18c1a027"], "references": ["0b3cfbf79d50dae4a16584533227bb728e3522aa", "687bac2d3320083eb4530bf18bb8f8f721477600", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "27e5bd13d581ef682b96038dce4c18f260122352", "6c8b30f63f265c32e26d999aa1fef5286b8308ad"], "score": 1.6266845446191716, "embeddings": [0.6628327882768827, 0.057746595136832925, -0.030344376859334346, -0.3950050435197306, -0.30914446280525415, 0.34984541356413806, -0.07988699907247758, 0.044201607037484505, 0.053118587275338865, -0.19846260444115685, -0.06837023636117058, 0.04988838146672739, 0.06612767294585291, 0.12167923128972975, -0.0056995589478747765, -0.11174879019365544, 0.12591747155150979, -0.04198836641690351, 0.014934681037531386, 0.05120155270883319, 0.027294907425952485, -0.03762788096319634, -0.18878241408319463, 0.06289420464871348, 0.15007940736153283, -0.05600560298353369, 0.03408104254407272, 0.009308313065232087, 0.029536666517526258, 0.027856184606488543, 0.02592831981324611, -0.03939057926694988]}, {"paper_id": "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "paper_title": "Semi-supervised Sequence Learning", "paper_year": 2015, "paper_venue": "NIPS", "paper_authors": ["Andrew M. Dai", "Quoc V. Le"], "paper_citations": 364, "paper_abstract": "We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a \"pretraining\" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.", "citations": ["3febb2bed8865945e7fddc99efd791887bb7e14f", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "1e077413b25c4d34945cc2707e17e46ed4fe784a", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "a97dc52807d80454e78d255f9fbd7b0fab56bd03"], "references": ["39dba6f22d72853561a4ed684be265e179a39e4f", "2c5135a0531bc5ad7dd890f018e67a40529f5bcb", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "0b3cfbf79d50dae4a16584533227bb728e3522aa", "687bac2d3320083eb4530bf18bb8f8f721477600", "27e38351e48fe4b7da2775bf94341738bc4da07e", "47a87c2cbdd928bb081974d308b3d9cf678d257e", "1510cf4b8abea80b9f352325ca4c132887de21a0", "abd1c342495432171beb7ca8fd9551ef13cbd0ff"], "score": 1.6107835550900496, "embeddings": [0.6686935677808991, -0.20319556370986536, -0.11765215082981753, -0.11444947315345347, 0.11052033798219853, -0.021138409518097714, 0.17964113878701668, 0.08923386967200023, 0.2325996598596799, 0.34093134563692584, -0.11073782440677296, -0.05881825433896754, 0.08938220649219017, 0.2616554037515956, -0.0030200941949977095, 0.015977643419543625, -0.0023605014583263904, 0.17124182681325023, 0.07484310721585175, -0.21874702590527875, 0.038996482681465126, -0.1262979079243832, 0.009273040274549208, 0.18582101565380302, 0.09496581987732745, 0.04322960252336048, -0.011360033375889596, 0.017977155625486296, 0.057030317643760986, -0.10208606018405167, -0.029776350325181906, -0.0904411049564939]}, {"paper_id": "f37e1b62a767a307c046404ca96bc140b3e68cb5", "paper_title": "Glove: Global Vectors for Word Representation", "paper_year": 2014, "paper_venue": "EMNLP", "paper_authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "paper_citations": 1129, "paper_abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.", "citations": ["5171157c2c09a85ad6558c5c03da6b75b0cf5fe6", "93b8da28d006415866bf48f9a6e06b5242129195", "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "007ab5528b3bd310a80d553cccad4b78dc496b02", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "8c1b00128e74f1cd92aede3959690615695d5101", "e0222a1ae6874f7fff128c3da8769ab95963da04", "421fc2556836a6b441de806d7b393a35b6eaea58", "4e88de2930a4435f737c3996287a90ff87b95c59", "3c78c6df5eb1695b6a399e346dde880af27d1016", "3febb2bed8865945e7fddc99efd791887bb7e14f", "27e98e09cf09bc13c913d01676e5f32624011050", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "59761abc736397539bdd01ad7f9d91c8607c0457", "9784fbf77295860b2e412137b86356d70b25e3c0", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "af5c4b80fbf847f69a202ba5a780a3dd18c1a027", "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "b1e20420982a4f923c08652941666b189b11b7fe", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "26b47e35fe6e4260fdf7b7cc98f279a73c277494", "bc1d609520290e0460c49b685675eb5a57fa5935"], "references": ["0d67362a5630ec3b7562327acc278c1c996454b5", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "23694a80bf1b9b38215be3e23068dd75296bc90f", "05aba481e8a221df5d8775a3bb749001e7f2525e", "1a07186bc10592f0330655519ad91652125cd907", "2538e3eb24d26f31482c479d95d2e26c0e79b990", "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb", "0a10d64beb0931efdc24a28edaa91d539194b2e2", "dac72f2c509aee67524d3321f77e97e8eff51de6"], "score": 1.8287138009893797, "embeddings": [0.8328193547454306, 0.13805513657467075, 0.14390168771901876, -0.28199019853087925, -0.014104207352858162, -0.02799436055704529, 0.06860553957734855, 0.1473629769370086, -0.07980001368245009, -0.18694295840848985, -0.008734020086537486, 0.12678272559358816, 0.01662650701816756, 0.01885064085679335, -0.018922307664151823, -0.053815496052895595, 0.12912424409506795, -0.05819635909158193, 0.031129621357662486, 0.054082039326510185, 0.09062746321887624, -0.07663763021801695, 0.009810384664871255, -0.18183024790123675, -0.016453442951917915, -0.04739293097826451, -0.06335202937192563, -0.055396601866986445, -0.08988291320738297, -0.07021876413694879, -0.0649020963736055, 0.01869849561128206]}, {"paper_id": "87f40e6f3022adbc1f1905e3e506abad05a9964f", "paper_title": "Distributed Representations of Words and Phrases and their Compositionality", "paper_year": 2013, "paper_venue": "NIPS", "paper_authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean"], "paper_citations": 1104, "paper_abstract": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.", "citations": ["0b544dfe355a5070b60986319a3f51fb45d1348e", "3febb2bed8865945e7fddc99efd791887bb7e14f", "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "26e743d5bd465f49b9538deaf116c15e61b7951f", "9784fbf77295860b2e412137b86356d70b25e3c0", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "421fc2556836a6b441de806d7b393a35b6eaea58", "59761abc736397539bdd01ad7f9d91c8607c0457", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "4e88de2930a4435f737c3996287a90ff87b95c59", "1e077413b25c4d34945cc2707e17e46ed4fe784a", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "bc1d609520290e0460c49b685675eb5a57fa5935"], "references": ["1a07186bc10592f0330655519ad91652125cd907", "1005645c05585c2042e3410daeed638b55e2474d", "23694a80bf1b9b38215be3e23068dd75296bc90f", "dac72f2c509aee67524d3321f77e97e8eff51de6"], "score": 1.7405417804138477, "embeddings": [0.7331074472496584, -0.26398568585930426, 0.07416464613455757, -0.26779587059386034, 0.12814422441325438, -0.06724651210278917, 0.24758780686253626, 0.11516799594242991, -0.26325340138806763, -0.04290971793710382, 0.02385454363079154, 0.13989541451378684, -0.03420261865142143, 0.010848350965656235, 0.14385973516716596, -0.04503536950211097, 0.08764451954925155, -0.0006262083342249539, -0.0226593107372002, -0.14231342441091296, 0.010113789502303499, 0.10819381014499396, 0.014932708643838168, -0.13335064208019706, 0.05953148264072994, -0.048840294530545, -0.0538512293366693, -0.08712648994645053, -0.14343600361893985, 0.0724473846152251, -0.06245705713215221, 0.021049432954480217]}, {"paper_id": "5d833331b0e22ff359db05c62a8bca18c4f04b68", "paper_title": "One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling", "paper_year": 2013, "paper_venue": "INTERSPEECH", "paper_authors": ["Ciprian Chelba", "Tomas Mikolov", "Michael Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson"], "paper_citations": 423, "paper_abstract": "We propose a new benchmark corpus to be used for measuring progress in statistical language modeling. With almost one billion words of training data, we hope this benchmark will be useful to quickly evaluate novel language modeling techniques, and to compare their contribution when combined with other advanced techniques. We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned Kneser-Ney 5-gram model achieves perplexity 67.6; a combination of techniques leads to 35% reduction in perplexity, or 10% reduction in cross-entropy (bits), over that baseline. \nThe benchmark is available as a code.google.com project; besides the scripts needed to rebuild the training/held-out data, it also makes available log-probability values for each word in each of ten held-out data sets, for each of the baseline n-gram models.", "citations": ["3febb2bed8865945e7fddc99efd791887bb7e14f", "93b8da28d006415866bf48f9a6e06b5242129195", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "02b3d1d162080d9aefd3fc30a0bcc9a843073b5d", "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "421fc2556836a6b441de806d7b393a35b6eaea58", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "b9de9599d7241459db9213b5cdd7059696f5ef8d"], "references": ["47a87c2cbdd928bb081974d308b3d9cf678d257e"], "score": 1.5850253468308444, "embeddings": [0.5454779548141978, -0.08550064866964516, -0.2988180521686732, -0.3123398275466462, 0.3732184432373497, -0.21249705994813456, 0.2602914785166123, -0.24582916025313412, 0.12008185681448626, -0.19557887855234823, 0.041470585838327106, -0.2630895806083263, -0.07547255302055307, 0.06728107650561267, -0.1096155055776529, -0.08293424365739697, -0.0949489501041399, 0.04680948793125652, 0.04831078160697954, -0.0064062786603860285, 0.003227094136923458, 0.004874502660656492, -0.05686536335498246, -0.08522524244168987, -0.023020101092298368, -0.09761075177012386, 0.07150350754421851, -0.055832737733040864, 0.0017060485358721099, -0.010063298194848752, -0.01961815854622815, 0.007193757818353337]}, {"paper_id": "128cb6b891aee1b5df099acb48e2efecfcff689f", "paper_title": "The Winograd Schema Challenge", "paper_year": 2011, "paper_venue": "AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning", "paper_authors": ["Hector J. Levesque", "Ernest Davis", "Leora Morgenstern"], "paper_citations": 198, "paper_abstract": "In this paper, we present an alternative to the Turing Test that has some conceptual and practical advantages. Like the original, it involves responding to typed English sentences, and English-speaking adults will have no difficulty with it. Unlike the original, the subject is not required to engage in a conversation and fool an interrogator into believing she is dealing with a person. Moreover, the test is arranged in such a way that having full access to a large corpus of English text might not help much. Finally, the interrogator or a third party will be able to decide unambiguously after a few minutes whether or not a subject has passed the test.", "citations": ["9784fbf77295860b2e412137b86356d70b25e3c0", "93b8da28d006415866bf48f9a6e06b5242129195", "df2b0e26d0599ce3e70df8a9da02e51594e0e992"], "references": ["27e5bd13d581ef682b96038dce4c18f260122352"], "score": 1.4209015914491616, "embeddings": [0.3859745411609701, 0.10728940552061225, 0.02972124345586007, -0.355378970168598, -0.2275646177151357, 0.4532950507874351, -0.1857177936975823, -0.22400123315178838, 0.044058007887383, -0.08347081824990814, 0.32476612024362045, -0.11668827628938269, -0.20245161453343916, -0.14144010161019482, 0.053172182655098946, 0.011059218303790768, -0.10604529316782746, 0.19816775882263352, 0.057855197009955284, -0.23032494456188532, 0.01908327183730046, -0.04707814334830989, 0.14835728662688247, -0.023394352836082, -0.15893370464982615, -0.020060624620070123, -0.04801113573094897, 0.07286912659023581, 0.0749949806412242, 0.025447390484431415, 0.008502297815748308, 0.08242655459611499]}, {"paper_id": "9fa8d73e572c3ca824a04a5f551b602a17831bc5", "paper_title": "Domain Adaptation with Structural Correspondence Learning", "paper_year": 2006, "paper_venue": "EMNLP", "paper_authors": ["John Blitzer", "Ryan T. McDonald", "Fernando C Pereira"], "paper_citations": 924, "paper_abstract": "Discriminative learning methods are widely used in natural language processing. These methods work best when their training and test data are drawn from the same distribution. For many NLP tasks, however, we are confronted with new domains in which labeled data is scarce or non-existent. In such cases, we seek to adapt existing models from a resource-rich source domain to a resource-poor target domain. We introduce structural correspondence learning to automatically induce correspondences among features from different domains. We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger.", "citations": ["5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "df2b0e26d0599ce3e70df8a9da02e51594e0e992"], "references": ["0b44fcbeea9415d400c5f5789d6b892b6f98daff", "783480acff435bfbc15ffcdb4f15eccddaa0c810", "2c5135a0531bc5ad7dd890f018e67a40529f5bcb"], "score": 1.47591748232316, "embeddings": [0.37737024315262857, -0.04775925987789902, -0.0664564082073908, -0.34370093077238506, 0.10233195991010967, -0.07757467564330152, -0.49520094908062207, -0.3866195471386899, -0.01387962007874087, 0.2802441212282395, -0.20009105107743433, 0.22726472854989935, 0.16618824277491387, -0.027322345913296693, -0.228615330690356, 0.013916108355230217, 0.08427952475860764, 0.04507187604444256, 0.013776589545575603, 0.053247637505897005, -0.1199776802142447, 0.14709743411349752, 0.07638182683365927, -0.05345568540892808, -0.05808341206986183, 0.035630211344512844, 0.0013951761946865704, -0.04985881389001446, -0.05621396036788562, 0.0507527709920836, -0.022449383772955835, 0.009077795429186965]}, {"paper_id": "475354f10798f110d34792b6d88f31d6d5cb099e", "paper_title": "Automatically Constructing a Corpus of Sentential Paraphrases", "paper_year": 2005, "paper_venue": "IWP@IJCNLP", "paper_authors": ["William B. Dolan", "Chris Brockett"], "paper_citations": 192, "paper_abstract": "An obstacle to research in automatic paraphrase identification and generation is the lack of large-scale, publiclyavailable labeled corpora of sentential paraphrases. This paper describes the creation of the recently-released Microsoft Research Paraphrase Corpus, which contains 5801 sentence pairs, each hand-labeled with a binary judgment as to whether the pair constitutes a paraphrase. The corpus was created using heuristic extraction techniques in conjunction with an SVM-based classifier to select likely sentence-level paraphrases from a large corpus of topicclustered news data. These pairs were then submitted to human judges, who confirmed that 67% were in fact semantically equivalent. In addition to describing the corpus itself, we explore a number of issues that arose in defining guidelines for the human raters.", "citations": ["cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "93b8da28d006415866bf48f9a6e06b5242129195", "df2b0e26d0599ce3e70df8a9da02e51594e0e992"], "references": [], "score": 1.589770648949163, "embeddings": [0.41756727793224674, -0.029422085535791478, -0.03272082817144435, -0.4633990955811536, -0.1439875899275261, 0.17564458704932956, 0.0035549502987307605, 0.05411109380744085, -0.1372310882804309, 0.23446463596680156, -0.07502381887850505, -0.289717775497516, -0.154014518512768, -0.09525837263591044, 0.06370190338644956, 0.2547418462487285, -0.2424238033293091, 0.12964570253014807, -0.20172659154023892, 0.3161282791801669, -0.009698993547411359, -0.1398514723413891, -0.001465426104971051, -0.01755846830705558, 0.02108049586566871, -0.007038603979509358, 0.1914433389284131, -0.06704400937987667, -0.09428769322883088, 0.008505899878403776, -0.027015065483300262, -0.04264725820874315]}, {"paper_id": "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb", "paper_title": "Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition", "paper_year": 2003, "paper_venue": "CoNLL", "paper_authors": ["Erik F. Tjong Kim Sang", "Fien De Meulder"], "paper_citations": 789, "paper_abstract": "We describe the CoNLL-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance.", "citations": ["3febb2bed8865945e7fddc99efd791887bb7e14f", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e"], "references": [], "score": 1.6254934579096925, "embeddings": [0.508780780716829, -0.044582684792116786, -0.10879920825788819, -0.4727497186611953, 0.1650663195999968, -0.2680377252285821, 0.2178677276533506, 0.1227370935963565, 0.12778726440550106, -0.06692526704955072, 0.0449644973696281, 0.14387450070468522, -0.08877441427617742, -0.06005422915553199, -0.019806592662303838, 0.01394607622274147, 0.049095530533205096, -0.09579168658866159, 0.2376022571434118, 0.16601744514000508, 0.16205148999867755, 0.08097412180551396, 0.3163761070513083, -0.12264214265838258, 0.00947168907167446, -0.10150232283449748, 0.07752724905164446, 0.04948577430747508, 0.10069612461779967, -0.11091333934119098, 0.019427643736374126, 0.037047176770904744]}, {"paper_id": "766ce989b8b8b984f7a4691fd8c9af4bdb2b74cd", "paper_title": "\"Cloze procedure\": a new tool for measuring readability.", "paper_year": 1953, "paper_venue": "", "paper_authors": ["Wilson L. Taylor"], "paper_citations": 698, "paper_abstract": "Here is the first comprehensive statement of a research method and its theory which were introduced briefly during a workshop at the 1953 AEJ convention. Included are findings from three pilot studies and two experiments in which \u201ccloze procedure\u201d results are compared with those of two readability formulas.", "citations": ["d1505c6123c102e53eb19dff312cb25cea840b72", "df2b0e26d0599ce3e70df8a9da02e51594e0e992"], "references": [], "score": 1.4311659835239494, "embeddings": [0.32783404331794636, 0.30062033294183155, 0.16231325619626003, -0.1557809431879493, 0.02561963617315656, -0.08923683751246489, -0.003074037361805915, -0.10818029549707774, -0.11379571525413563, 0.35801256148685817, 0.056366568807640766, -0.4194197645588165, 0.2586889787604793, -0.3863088566524742, 0.20005031802218562, -0.2144835158280302, 0.09349801512121482, -0.23616967282712567, 0.1841865318572262, -0.027330310135831034, 0.024914176829795725, -0.02190637941197593, -0.058301122827187676, 0.021413408260153775, 0.010114119071040975, 0.03908336416208292, 0.030137632749973487, -0.015051890108265176, 0.037661922654044805, -0.016685201134867413, -0.009375460406702263, 0.010927189960862801]}], [{"paper_id": "0a10d64beb0931efdc24a28edaa91d539194b2e2", "paper_title": "Efficient Estimation of Word Representations in Vector Space", "paper_year": 2013, "paper_venue": "ICLR", "paper_authors": ["Tomas Mikolov", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean"], "paper_citations": 1068, "paper_abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.", "citations": ["47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "59761abc736397539bdd01ad7f9d91c8607c0457", "9784fbf77295860b2e412137b86356d70b25e3c0", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "0e6824e137847be0599bb0032e37042ed2ef5045", "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "a4cec122a08216fe8a3bc19b22e78fbaea096256", "bc1d609520290e0460c49b685675eb5a57fa5935"], "references": ["23694a80bf1b9b38215be3e23068dd75296bc90f"], "score": 0.5461109231990735, "embeddings": [0.6257582126601504, -0.153327645625529, -0.10058794285564392, 0.021579623140934763, -0.11503197968300045, 0.004836841149837825, 0.12113522185316686, 0.09360312729183809, -0.2527432072105346, -0.11003174308580653, 0.15703613049450268, 0.19139882100868227, 0.14037434836461565, -0.1389907251495105, -0.07322979243920026, 0.14666207182982216, 0.10692107239284052, 0.09955214279106153, 0.33261363436006003, 0.07427379742544919, 0.0271033356548528, -0.2695849239822651, -0.036371730530393824, -0.0822831779387497, -0.041715211664559215, 0.040051690713433154, -0.14656445624142903, 0.027145346810934744, -0.2972414666400818, 0.027710414645330795, 0.02302476115207752, 0.01609053423028336]}, {"paper_id": "27e5bd13d581ef682b96038dce4c18f260122352", "paper_title": "The PASCAL Recognising Textual Entailment Challenge", "paper_year": 2005, "paper_venue": "MLCW", "paper_authors": ["Ido Dagan", "Oren Glickman", "Bernardo Magnini"], "paper_citations": 854, "paper_abstract": "This paper describes the PASCAL Network of Excellence first Recognising Textual Entailment (RTE-1) Challenge benchmark 1 . The RTE task is defined as recognizing, given two text fragments, whether the meaning of one text can be inferred (entailed) from the other. This application-independent task is suggested as capturing major inferences about the variability of semantic expression which are commonly needed across multiple applications. The Challenge has raised noticeable attention in the research community, attracting 17 submissions from diverse groups, suggesting the generic relevance of the task.", "citations": ["93b8da28d006415866bf48f9a6e06b5242129195", "745d86adca56ec50761591733e157f84cfb19671", "128cb6b891aee1b5df099acb48e2efecfcff689f", "9784fbf77295860b2e412137b86356d70b25e3c0", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "af5c4b80fbf847f69a202ba5a780a3dd18c1a027"], "references": [], "score": 0.41549304094648876, "embeddings": [0.4289543788154783, 0.03241809471814029, 0.040914109106510124, -0.3559701328919835, -0.33447301599785356, 0.5501179534799132, -0.1975072568628866, -0.23831455071248808, 0.1327360696021637, -0.2440039035322742, 0.20230772260625318, -0.06805782517514213, -0.05730052435656707, -0.015134410845775513, -0.007657162280666187, -0.06909187474432461, 0.007374671361997052, 0.024102409712287283, 0.05482255309723738, -0.076454556994166, 0.019873556002054284, 0.08066538988391472, 0.055237384486906536, 0.10485732255101288, 0.05888729354631571, 0.000824004380503361, -0.03966749993645777, 0.002196317712719123, -0.037839873330019834, -0.043322779537926855, -0.06746736946355539, -0.043015448892692466]}, {"paper_id": "0b3cfbf79d50dae4a16584533227bb728e3522aa", "paper_title": "Long Short-Term Memory", "paper_year": 1997, "paper_venue": "Neural Computation", "paper_authors": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "paper_citations": 1190, "paper_abstract": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.", "citations": ["5171157c2c09a85ad6558c5c03da6b75b0cf5fe6", "071b16f25117fb6133480c6259227d54fc2a5ea0", "007ab5528b3bd310a80d553cccad4b78dc496b02", "146f6f6ed688c905fb6e346ad02332efd5464616", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "b9de9599d7241459db9213b5cdd7059696f5ef8d", "8c1b00128e74f1cd92aede3959690615695d5101", "421fc2556836a6b441de806d7b393a35b6eaea58", "4e88de2930a4435f737c3996287a90ff87b95c59", "39dba6f22d72853561a4ed684be265e179a39e4f", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "27e98e09cf09bc13c913d01676e5f32624011050", "3febb2bed8865945e7fddc99efd791887bb7e14f", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "9784fbf77295860b2e412137b86356d70b25e3c0", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "652d159bf64a70194127722d19841daa99a69b64", "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "af5c4b80fbf847f69a202ba5a780a3dd18c1a027", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "0b544dfe355a5070b60986319a3f51fb45d1348e", "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "d1505c6123c102e53eb19dff312cb25cea840b72", "0e6824e137847be0599bb0032e37042ed2ef5045", "a4cec122a08216fe8a3bc19b22e78fbaea096256"], "references": [], "score": 0.6912546060591622, "embeddings": [0.8053236832950449, 0.1920854956595874, -0.23063066987228684, 0.08125033147780039, 0.07283770471052452, 0.22720241681770387, 0.03662207320128146, 0.08560279342466853, 0.002827133358765992, -0.08878442740039483, -0.11216232979834723, 0.05569808500960578, 0.21403391126125573, 0.07163470747761516, 0.15632186602036097, -0.17374258736997134, -0.05962668107024796, 0.040056446996301766, -0.046903164942216524, -0.027027309745800292, 0.02284699746211771, -0.008749134020251008, -0.039400417780758965, -0.04884779905449613, 0.038782334878438414, 0.014926324830697214, -0.04878928182280764, -0.004260105709987421, -0.12699765311888025, -0.15954830261970115, -0.019122298654145956, -0.03645161910388025]}]], [[{"paper_id": "bc1d609520290e0460c49b685675eb5a57fa5935", "paper_title": "An efficient framework for learning sentence representations", "paper_year": 2018, "paper_venue": "ICLR", "paper_authors": ["Lajanugen Logeswaran", "Honglak Lee"], "paper_citations": 57, "paper_abstract": "In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.", "citations": ["cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "df2b0e26d0599ce3e70df8a9da02e51594e0e992"], "references": ["87f40e6f3022adbc1f1905e3e506abad05a9964f", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "26e743d5bd465f49b9538deaf116c15e61b7951f", "1005645c05585c2042e3410daeed638b55e2474d", "687bac2d3320083eb4530bf18bb8f8f721477600", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "71b7178df5d2b112d07e45038cb5637208659ff7", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "4e88de2930a4435f737c3996287a90ff87b95c59", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "1510cf4b8abea80b9f352325ca4c132887de21a0", "a97dc52807d80454e78d255f9fbd7b0fab56bd03", "0a10d64beb0931efdc24a28edaa91d539194b2e2"], "score": 1.6100989989243475, "embeddings": [0.648249466479985, -0.3319305543057705, 0.13744591672770076, -0.09698544184366611, -0.12610609098629477, 0.06807144591523898, 0.3154263462667831, 0.12094559805281421, -0.35644116190794006, 0.08763463176324306, -0.014749796701574064, 0.05818324158078224, 0.12917387183429485, 0.02043704663469392, -0.0872778468474799, 0.09229166902124403, 0.10772124016828945, 0.016347684071442884, -0.021113275326712984, -0.0812345499139178, -0.13218852936490594, 0.028351605083257934, 0.0016414915635427905, 0.10132192404099676, -0.17942550780946026, -0.07480956028054032, -0.009107621693035514, -0.04538542000828847, -0.009208627113034846, 0.047491704184688545, 0.08165779793016426, 0.19663156895260667]}, {"paper_id": "a97dc52807d80454e78d255f9fbd7b0fab56bd03", "paper_title": "Discourse-Based Objectives for Fast Unsupervised Sentence Representation Learning", "paper_year": 2017, "paper_venue": "ArXiv", "paper_authors": ["Yacine Jernite", "Samuel R. Bowman", "David Sontag"], "paper_citations": 35, "paper_abstract": "This work presents a novel objective function for the unsupervised training of neural network sentence encoders. It exploits signals from paragraph-level discourse coherence to train these models to understand text. Our objective is purely discriminative, allowing us to train models many times faster than was possible under prior methods, and it yields models which perform well in extrinsic evaluations.", "citations": ["bc1d609520290e0460c49b685675eb5a57fa5935", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "df2b0e26d0599ce3e70df8a9da02e51594e0e992"], "references": ["39dba6f22d72853561a4ed684be265e179a39e4f", "26e743d5bd465f49b9538deaf116c15e61b7951f", "687bac2d3320083eb4530bf18bb8f8f721477600", "05aba481e8a221df5d8775a3bb749001e7f2525e", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "47a87c2cbdd928bb081974d308b3d9cf678d257e", "0e6824e137847be0599bb0032e37042ed2ef5045", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292"], "score": 1.582511119548399, "embeddings": [0.5851008967750367, -0.24089548930975757, -0.06607895310550303, 0.002877754528720841, 0.011911892931047873, 0.16214662909535904, 0.28749602861523094, 0.16175445804713168, -0.06765232710477588, 0.2772285492028417, -0.09835009816166969, -0.266653690525209, 0.16042528709779055, 0.16892714188486477, -0.3300471856655366, 0.07932419599423456, 0.13559793717999666, 0.01199825815340822, -0.07617334556336522, -0.1428374631619631, -0.08482960254573461, -0.08898252881736785, 0.12421268321694111, 0.1338449837735696, -0.1404103323009535, -0.033489920237938155, -0.04113450965480698, 0.07333942798531966, 0.011963439039598582, -0.0729967681643516, 0.035522306296038086, 0.02145681542496352]}, {"paper_id": "26e743d5bd465f49b9538deaf116c15e61b7951f", "paper_title": "Learning Distributed Representations of Sentences from Unlabelled Data", "paper_year": 2016, "paper_venue": "HLT-NAACL", "paper_authors": ["Felix Hill", "Kyunghyun Cho", "Anna Korhonen"], "paper_citations": 238, "paper_abstract": "Unsupervised methods for learning distributed representations of words are ubiquitous in today's NLP research, but far less is known about the best ways to learn distributed phrase or sentence representations from unlabelled data. This paper is a systematic comparison of models that learn such representations. We find that the optimal approach depends critically on the intended application. Deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-linear models work best for building representation spaces that can be decoded with simple spatial distance metrics. We also propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance.", "citations": ["a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "93b8da28d006415866bf48f9a6e06b5242129195", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "a97dc52807d80454e78d255f9fbd7b0fab56bd03", "bc1d609520290e0460c49b685675eb5a57fa5935"], "references": ["87f40e6f3022adbc1f1905e3e506abad05a9964f", "39dba6f22d72853561a4ed684be265e179a39e4f", "0b544dfe355a5070b60986319a3f51fb45d1348e", "843959ffdccf31c6694d135fad07425924f785b1", "745d86adca56ec50761591733e157f84cfb19671", "03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f", "cfa2646776405d50533055ceb1b7f050e9014dcb", "1510cf4b8abea80b9f352325ca4c132887de21a0", "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd"], "score": 1.638133261099934, "embeddings": [0.6725520371232716, -0.30522021058408394, 0.04574322093510699, -0.07138619427631239, -0.0892492422387835, 0.04949134862106356, 0.23590814994672626, 0.13621577398001555, -0.19951995636924172, 0.11259933500929625, 0.06300822986500201, -0.1322078525733599, 0.05809188351968385, 0.13690034222402594, 0.036176530669486714, 0.1326309763132472, 0.0896297126688207, 0.06955576471089155, 0.049024049642022276, -0.11205322951079218, -0.2449512214582925, 0.29284876591563286, 0.01636835317254095, -0.129526614378938, 0.06332715301480961, 0.1335906874797752, 0.05785014753071081, 0.16272552794803793, -0.0341397537702199, -0.02809939526472382, -0.04789096293275151, -0.034392597109816465]}, {"paper_id": "1510cf4b8abea80b9f352325ca4c132887de21a0", "paper_title": "Distributed Representations of Sentences and Documents", "paper_year": 2014, "paper_venue": "ICML", "paper_authors": ["Quoc V. Le", "Tomas Mikolov"], "paper_citations": 1029, "paper_abstract": "Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \"powerful,\" \"strong\" and \"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.", "citations": ["a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "93b8da28d006415866bf48f9a6e06b5242129195", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "26e743d5bd465f49b9538deaf116c15e61b7951f", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "4e88de2930a4435f737c3996287a90ff87b95c59", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "bc1d609520290e0460c49b685675eb5a57fa5935"], "references": ["1005645c05585c2042e3410daeed638b55e2474d", "cfa2646776405d50533055ceb1b7f050e9014dcb", "3a0e788268fafb23ab20da0e98bb578b06830f7d", "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "dac72f2c509aee67524d3321f77e97e8eff51de6"], "score": 1.6491650392137707, "embeddings": [0.6531211457340278, -0.3874843507619638, 0.25682892419765085, -0.18621843963695311, -0.004316009876459121, 0.06261819317408301, 0.2443926025839197, 0.04776094662956615, -0.2542117603727027, 0.11203078156612253, -0.11523294892280415, 0.06768211404399485, 0.010750290331045988, 0.028965933426917848, 0.10678370195786702, 0.09544399350423849, -0.07186033306329319, 0.02153679154392988, -0.0842041147197851, -0.14297203748319068, -0.08055726123433955, 0.06909573971165961, -0.14739078383625984, 0.0759687948923924, 0.0021219334390033327, -0.004211506557161317, -0.015004633970813462, 0.21164282480398822, 0.0995460108670705, -0.09219947818008695, -0.06461058199459349, 0.025651218219033885]}, {"paper_id": "687bac2d3320083eb4530bf18bb8f8f721477600", "paper_title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "paper_year": 2013, "paper_venue": "EMNLP", "paper_authors": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts"], "paper_citations": 1028, "paper_abstract": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.", "citations": ["3febb2bed8865945e7fddc99efd791887bb7e14f", "93b8da28d006415866bf48f9a6e06b5242129195", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "9784fbf77295860b2e412137b86356d70b25e3c0", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "4e88de2930a4435f737c3996287a90ff87b95c59", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "a97dc52807d80454e78d255f9fbd7b0fab56bd03", "a4cec122a08216fe8a3bc19b22e78fbaea096256", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "bc1d609520290e0460c49b685675eb5a57fa5935"], "references": ["745d86adca56ec50761591733e157f84cfb19671", "23694a80bf1b9b38215be3e23068dd75296bc90f", "cfa2646776405d50533055ceb1b7f050e9014dcb", "05aba481e8a221df5d8775a3bb749001e7f2525e", "3a0e788268fafb23ab20da0e98bb578b06830f7d", "27e38351e48fe4b7da2775bf94341738bc4da07e", "1a07186bc10592f0330655519ad91652125cd907", "2b669398c4cf2ebe04375c8b1beae20f4ac802fa"], "score": 1.6112181634296108, "embeddings": [0.738940604476252, -0.36823869606084075, 0.3534228010093452, 0.017249381102677208, -0.07467869046090563, 0.1836230034257913, 0.10372575524182939, 0.05406903259849254, 0.18713348625459797, 0.05600818206058347, -0.1020514893654804, -0.09340480766009018, 0.015129058407278401, 0.039745513540188224, -0.051578463650131474, 0.09674396698847035, 0.09683660792191956, -0.00653787470976124, -0.0029395808571301848, -0.06506581527839553, -0.06455836785311933, -0.1205785506659907, -0.0027557732325163677, 0.04123414316062178, -0.03700613476166426, -0.041267817863684594, -0.02467945080950091, -0.04630709948054565, -0.019921023566758043, -0.13232880759573226, 0.09556483459089575, -0.04602685520733386]}, {"paper_id": "dac72f2c509aee67524d3321f77e97e8eff51de6", "paper_title": "Word Representations: A Simple and General Method for Semi-Supervised Learning", "paper_year": 2010, "paper_venue": "ACL", "paper_authors": ["Joseph P. Turian", "Lev-Arie Ratinov", "Yoshua Bengio"], "paper_citations": 1013, "paper_abstract": "If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize.com/projects/wordreprs/", "citations": ["87f40e6f3022adbc1f1905e3e506abad05a9964f", "3febb2bed8865945e7fddc99efd791887bb7e14f", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "59761abc736397539bdd01ad7f9d91c8607c0457", "03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f", "cfa2646776405d50533055ceb1b7f050e9014dcb", "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "4e88de2930a4435f737c3996287a90ff87b95c59", "1510cf4b8abea80b9f352325ca4c132887de21a0", "2538e3eb24d26f31482c479d95d2e26c0e79b990", "a4cec122a08216fe8a3bc19b22e78fbaea096256"], "references": ["23694a80bf1b9b38215be3e23068dd75296bc90f", "1005645c05585c2042e3410daeed638b55e2474d", "783480acff435bfbc15ffcdb4f15eccddaa0c810", "3a0e788268fafb23ab20da0e98bb578b06830f7d", "1a07186bc10592f0330655519ad91652125cd907"], "score": 1.5563505815278433, "embeddings": [0.6542501875903487, -0.4074251485985725, 0.4184295315620489, -0.028863704592129117, 0.1790550131047163, -0.13846362971275886, -0.06552566119268917, -0.019367079683643727, -0.1897060190289743, -0.10247443864362074, -0.07205321954589361, 0.17094244354226806, -0.12959274354311562, -0.19207242011012038, 0.03295281933177201, -0.08456451875793489, -0.012165072066852049, 0.016058843138781642, -0.06693818631913627, -0.0793198876353277, 0.019577408515354312, -0.047876321944076795, -0.062058168768494036, -0.011160259545070455, 0.03885616730353417, -0.022845074320778175, 0.04389414184739122, 0.07522379548231191, 0.01841755485905718, -0.020599098061449345, 0.004763523157472219, -0.055545600439253155]}, {"paper_id": "1a07186bc10592f0330655519ad91652125cd907", "paper_title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "paper_year": 2008, "paper_venue": "ICML", "paper_authors": ["Ronan Collobert", "Jason Weston"], "paper_citations": 1049, "paper_abstract": "We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.", "citations": ["cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f", "3a0e788268fafb23ab20da0e98bb578b06830f7d", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "0d67362a5630ec3b7562327acc278c1c996454b5", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "cfa2646776405d50533055ceb1b7f050e9014dcb", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "9784fbf77295860b2e412137b86356d70b25e3c0", "687bac2d3320083eb4530bf18bb8f8f721477600", "27e38351e48fe4b7da2775bf94341738bc4da07e", "79dc84a3bf76f1cb983902e2591d913cee5bdb0e", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "a4cec122a08216fe8a3bc19b22e78fbaea096256", "dac72f2c509aee67524d3321f77e97e8eff51de6"], "references": ["2c5135a0531bc5ad7dd890f018e67a40529f5bcb", "23694a80bf1b9b38215be3e23068dd75296bc90f", "162d958ff885f1462aeda91cd72582323fd6a1f4"], "score": 1.4797371532629193, "embeddings": [0.6879419875189792, -0.4213145933706874, 0.40189328458907825, 0.10017764382968576, 0.10103933399210877, -0.023247911015305517, -0.16797593743979694, 0.12533770382556808, 0.1211813128597641, 0.059611203185821623, 0.03172440012552111, 0.0274256579526623, -0.07238426520082376, -0.010867783835785856, 0.04745173056773495, -0.06167421272900694, 0.01940451737356478, 0.08369319081318904, 0.05953546168653528, 0.07583679472330965, 0.12279733743980709, 0.018038479212642254, -0.08952762756457258, -0.03888131952251156, -0.03362630217046118, -0.10021390876380104, -0.04178496666056193, -0.13368218754644445, 0.01937728148766399, 0.11625650535618574, -0.0722642257538681, 0.008903801467282875]}, {"paper_id": "1005645c05585c2042e3410daeed638b55e2474d", "paper_title": "A Scalable Hierarchical Distributed Language Model", "paper_year": 2008, "paper_venue": "NIPS", "paper_authors": ["Andriy Mnih", "Geoffrey E. Hinton"], "paper_citations": 591, "paper_abstract": "Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the non-hierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models.", "citations": ["87f40e6f3022adbc1f1905e3e506abad05a9964f", "0d67362a5630ec3b7562327acc278c1c996454b5", "bc1d609520290e0460c49b685675eb5a57fa5935", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "02b3d1d162080d9aefd3fc30a0bcc9a843073b5d", "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "652d159bf64a70194127722d19841daa99a69b64", "1510cf4b8abea80b9f352325ca4c132887de21a0", "a4cec122a08216fe8a3bc19b22e78fbaea096256", "dac72f2c509aee67524d3321f77e97e8eff51de6"], "references": ["d87ceda3042f781c341ac17109d1e94a717f5f60", "783480acff435bfbc15ffcdb4f15eccddaa0c810", "23694a80bf1b9b38215be3e23068dd75296bc90f"], "score": 1.501524143266435, "embeddings": [0.560047881044051, -0.34418250932461397, 0.14377627987513572, 0.0342332938605878, 0.20554880627007335, -0.19421710216579652, -0.13977084636310158, -0.2170415821612253, -0.4907709501837754, -0.17037923052829168, -0.02434202988647762, 0.055736397505028124, -0.031587661889129384, 0.036014101743894585, 0.17343580316726814, 0.03785389285673938, 0.010485656447014059, -0.0728348803592212, -0.1426464249943225, -0.11025946969888817, 0.007049577743719637, -0.09325919430620506, 0.05636207091487051, 0.13014945086054142, -0.0023235581707344182, -0.11276391349283758, 0.016254823741524444, 0.0294492207961847, 0.05872019873230912, -0.07181761827539301, 0.027762391383872133, 0.044988962470276644]}, {"paper_id": "2c5135a0531bc5ad7dd890f018e67a40529f5bcb", "paper_title": "A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data", "paper_year": 2005, "paper_venue": "J. Mach. Learn. Res.", "paper_authors": ["Rie Kubota Ando", "Tong Zhang"], "paper_citations": 822, "paper_abstract": "One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don't have a complete understanding of their effectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Specifically we consider learning predictive structures on hypothesis spaces (that is, what kind of classifiers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi-supervised learning setting.", "citations": ["df2b0e26d0599ce3e70df8a9da02e51594e0e992", "3a0e788268fafb23ab20da0e98bb578b06830f7d", "1a07186bc10592f0330655519ad91652125cd907", "2538e3eb24d26f31482c479d95d2e26c0e79b990", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "9fa8d73e572c3ca824a04a5f551b602a17831bc5"], "references": [], "score": 1.4842891586475344, "embeddings": [0.4443110130298042, -0.20040398147848476, 0.17437582368382223, -0.2433131932868902, 0.09126301167657525, -0.10520335164793443, -0.3269924385487647, -0.08775731116690645, 0.21602725587703392, 0.5051066305701616, -0.10681392177489678, 0.126419392561246, 0.12323540865765496, 0.21555406953164644, -0.06863903826473779, -0.09575415159738385, -0.1890209752879202, 0.10525262207431538, 0.09698927062151383, -0.09239041994620413, 0.11447473471042456, 0.012150135752453418, -0.0935477008132685, -0.15157786761292552, -0.09988594037891867, -0.059349769314406185, -0.042590163402390545, 0.05926089434733053, -0.040358534215226095, 0.03207234536781684, -0.03379583142691248, -0.04146501276612263]}, {"paper_id": "783480acff435bfbc15ffcdb4f15eccddaa0c810", "paper_title": "Class-Based n-gram Models of Natural Language", "paper_year": 1992, "paper_venue": "Computational Linguistics", "paper_authors": ["Peter F. Brown", "Vincent J. Della Pietra", "Peter V. de Souza", "Jennifer C. Lai", "Robert L. Mercer"], "paper_citations": 1009, "paper_abstract": "We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics.", "citations": ["23694a80bf1b9b38215be3e23068dd75296bc90f", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "1005645c05585c2042e3410daeed638b55e2474d", "2538e3eb24d26f31482c479d95d2e26c0e79b990", "9fa8d73e572c3ca824a04a5f551b602a17831bc5", "dac72f2c509aee67524d3321f77e97e8eff51de6"], "references": [], "score": 1.4753399409051533, "embeddings": [0.43954127705283585, -0.21149799784290102, 0.13305754521185864, -0.20169836929450097, 0.2211819940348825, -0.20865140102440816, -0.3947269978585454, -0.2395379634154791, -0.36613328299600256, 0.07859330123344782, -0.028296796802829673, 0.21780538055413468, -0.051416323626321314, -0.07801502128768315, -0.11080598664609753, -0.1880716603785359, 0.029850330904564293, -0.004998478781435067, -0.13408076258936386, -0.06689136725173966, -0.20184004166408925, -0.1358099863925121, 0.1806565387367467, 0.021016991007939547, 0.13209843316305173, 0.0007667871445081083, 0.09141692396984609, -0.017377725351505342, 0.004366635876857968, -0.050557935514654886, 0.08892502995720251, -0.031123592915091473]}], [{"paper_id": "4e88de2930a4435f737c3996287a90ff87b95c59", "paper_title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks", "paper_year": 2015, "paper_venue": "ACL", "paper_authors": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "paper_citations": 1004, "paper_abstract": "A Long Short-Term Memory (LSTM) network is a type of recurrent neural network architecture which has recently obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. TreeLSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).", "citations": ["6e795c6e9916174ae12349f5dc3f516570c17ce8", "9784fbf77295860b2e412137b86356d70b25e3c0", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "bc1d609520290e0460c49b685675eb5a57fa5935"], "references": ["87f40e6f3022adbc1f1905e3e506abad05a9964f", "39dba6f22d72853561a4ed684be265e179a39e4f", "745d86adca56ec50761591733e157f84cfb19671", "79dc84a3bf76f1cb983902e2591d913cee5bdb0e", "0b3cfbf79d50dae4a16584533227bb728e3522aa", "071b16f25117fb6133480c6259227d54fc2a5ea0", "03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f", "687bac2d3320083eb4530bf18bb8f8f721477600", "05aba481e8a221df5d8775a3bb749001e7f2525e", "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "27e38351e48fe4b7da2775bf94341738bc4da07e", "1510cf4b8abea80b9f352325ca4c132887de21a0", "dac72f2c509aee67524d3321f77e97e8eff51de6"], "score": 0.5010092026674624, "embeddings": [0.6995622260535891, -0.33994613090219916, 0.34168130331596086, 0.13793429312572533, -0.02679538082567164, 0.2270357455079131, 0.20557821417967315, 0.06163563782366388, 0.09068676098905429, -0.04399785202701526, -0.05491990905805081, 0.07705503668324472, 0.10956684757284482, -0.06439997728195472, 0.07174325908556549, 0.02513586920392715, 0.1188416566269638, -0.07763236097836874, -0.1586879799200508, -0.14829737444604807, -0.13896900059012127, 0.0023115602590875845, 0.02714603739461736, -0.08468874892165958, -0.037015597034609986, -0.08175796988185048, 0.020816703680284813, -0.09167939997813625, -0.0420610470617613, -0.00442432847529637, 0.07501279892059153, 0.032380666276826384]}, {"paper_id": "03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f", "paper_title": "A Convolutional Neural Network for Modelling Sentences", "paper_year": 2014, "paper_venue": "ACL", "paper_authors": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "paper_citations": 1009, "paper_abstract": "The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline.", "citations": ["02b3d1d162080d9aefd3fc30a0bcc9a843073b5d", "26e743d5bd465f49b9538deaf116c15e61b7951f", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "4e88de2930a4435f737c3996287a90ff87b95c59", "d1505c6123c102e53eb19dff312cb25cea840b72"], "references": ["745d86adca56ec50761591733e157f84cfb19671", "79dc84a3bf76f1cb983902e2591d913cee5bdb0e", "162d958ff885f1462aeda91cd72582323fd6a1f4", "cfa2646776405d50533055ceb1b7f050e9014dcb", "687bac2d3320083eb4530bf18bb8f8f721477600", "05aba481e8a221df5d8775a3bb749001e7f2525e", "1a07186bc10592f0330655519ad91652125cd907", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "dac72f2c509aee67524d3321f77e97e8eff51de6"], "score": 0.347521145332254, "embeddings": [0.5983917673883277, -0.3621480212619326, 0.37048535260979365, 0.26130820467666416, 0.03689676417689869, 0.056554822290502516, 0.02689509786146583, 0.04165455821215377, 0.27029146657398423, -0.06896086018991565, 0.07075033479966003, -0.17395378866837988, 0.11174871005999776, -0.14630450919059326, -0.05027416521656131, 0.09088376797716576, 0.1045582098794586, -0.0927935674175894, -0.16271258627606408, -0.02394339074792163, -0.17524753751028643, 0.11614075420266434, 0.03886616221503825, -0.12822392497135843, -0.026365643387902395, -0.044723978669352286, 0.01935107071078339, -0.007599689650187195, -0.043368729886776786, 0.12696700632667393, -0.058428500251781736, -0.08467996670808012]}, {"paper_id": "79dc84a3bf76f1cb983902e2591d913cee5bdb0e", "paper_title": "Grounded Compositional Semantics for Finding and Describing Images with Sentences", "paper_year": 2014, "paper_venue": "Transactions of the Association for Computational Linguistics", "paper_authors": ["Richard Socher", "Andrej Karpathy", "Quoc V. Le", "Christopher D. Manning", "Andrew Y. Ng"], "paper_citations": 482, "paper_abstract": "Previous work on Recursive Neural Networks (RNNs) shows that these models can produce compositional feature vectors for accurately representing and classifying sentences or images. However, the sentence vectors of previous models cannot accurately represent visually grounded meaning. We introduce the DT-RNN model which uses dependency trees to embed sentences into a vector space in order to retrieve images that are described by those sentences. Unlike previous RNN-based models which use constituency trees, DT-RNNs naturally focus on the action and agents in a sentence. They are better able to abstract from the details of word order and syntactic expression. DT-RNNs outperform other recursive and recurrent neural networks, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image.", "citations": ["6e795c6e9916174ae12349f5dc3f516570c17ce8", "4e88de2930a4435f737c3996287a90ff87b95c59", "03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f", "bc8fa64625d9189f5801837e7b133e7fe3c581f7"], "references": ["38211dc39e41273c0007889202c69f841e02248a", "745d86adca56ec50761591733e157f84cfb19671", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "cfa2646776405d50533055ceb1b7f050e9014dcb", "05aba481e8a221df5d8775a3bb749001e7f2525e", "3a0e788268fafb23ab20da0e98bb578b06830f7d", "27e38351e48fe4b7da2775bf94341738bc4da07e", "1a07186bc10592f0330655519ad91652125cd907", "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "abd1c342495432171beb7ca8fd9551ef13cbd0ff"], "score": 0.30512776865515245, "embeddings": [0.5460868855247902, -0.4222554893147501, 0.42458774005217487, 0.32101390607129543, -0.172196491042954, 0.03108629947279439, 0.07329518396468811, -0.11035730668739387, 0.35928055136201287, -0.023289594293357467, -0.08860633102624971, -0.01815768786368319, -0.03455489421965031, -0.11901821967655907, -0.006366954428053314, 0.031051954186062602, 0.06045754219200372, -0.028649816310088897, -0.055591720625815126, 0.06016049320752064, -0.01239314254343276, 0.04117707017963813, -0.0450602147679005, -0.08881763992917353, -0.01728134956860552, -0.01665998243852836, 0.04396787933681096, -0.04598003419659367, -0.009583240858316458, -0.06556028572111404, 0.01608297020038991, 0.07483321167356932]}, {"paper_id": "27e38351e48fe4b7da2775bf94341738bc4da07e", "paper_title": "Semantic Compositionality through Recursive Matrix-Vector Spaces", "paper_year": 2012, "paper_venue": "EMNLP-CoNLL", "paper_authors": ["Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "paper_citations": 795, "paper_abstract": "Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them.", "citations": ["79dc84a3bf76f1cb983902e2591d913cee5bdb0e", "687bac2d3320083eb4530bf18bb8f8f721477600", "4e88de2930a4435f737c3996287a90ff87b95c59", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292"], "references": ["745d86adca56ec50761591733e157f84cfb19671", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "cfa2646776405d50533055ceb1b7f050e9014dcb", "3a0e788268fafb23ab20da0e98bb578b06830f7d", "1a07186bc10592f0330655519ad91652125cd907"], "score": 0.2632439362174273, "embeddings": [0.488908519746596, -0.44385900164518605, 0.46923221777399393, 0.26367051589501705, 0.026318591451095788, 0.14408959924181217, 0.07376489577645091, -0.05525205577421836, 0.3860282395054809, 0.09618948516579236, -0.11793360391136903, -0.03053355867320219, -0.03859381425086438, 0.03110251964580133, 0.031069179314201136, 0.0045433233508371485, -0.04889770057248729, 0.050792114698978146, 0.1439260930254256, 0.05534545262073166, 0.06451117988158395, 0.021769205400541166, 0.046982039707375176, 0.09110698877903255, 0.03655586143628201, 0.04288987406782302, 0.025190245900399654, -0.09998724369344347, -0.047746186412047024, -0.04388634759137162, 0.07214045179810438, 0.025388773720171627]}, {"paper_id": "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "paper_title": "Improving Word Representations via Global Context and Multiple Word Prototypes", "paper_year": 2012, "paper_venue": "ACL", "paper_authors": ["Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng"], "paper_citations": 784, "paper_abstract": "Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one representation per word. This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models.", "citations": ["1ee46c3b71ebe336d0b278de9093cfca7af7390b", "79dc84a3bf76f1cb983902e2591d913cee5bdb0e", "59761abc736397539bdd01ad7f9d91c8607c0457", "687bac2d3320083eb4530bf18bb8f8f721477600", "4e88de2930a4435f737c3996287a90ff87b95c59", "1510cf4b8abea80b9f352325ca4c132887de21a0"], "references": ["23694a80bf1b9b38215be3e23068dd75296bc90f", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "1005645c05585c2042e3410daeed638b55e2474d", "cfa2646776405d50533055ceb1b7f050e9014dcb", "1a07186bc10592f0330655519ad91652125cd907", "dac72f2c509aee67524d3321f77e97e8eff51de6"], "score": 0.40856146723629205, "embeddings": [0.5634352308541878, -0.3732683720213995, 0.5062250415493583, 0.14443495244339763, 0.12451908542133074, -0.06096206933574095, 0.0675918209767349, -0.07231598854503431, -0.12001781063760907, -0.12331189937657985, -0.13913049892116483, 0.1267423123042973, -0.19329728510013566, -0.24487829976739287, 0.02264210720010306, 0.005025095097955732, 0.037494424808575726, 0.07264575711673968, -0.050581575778810874, -0.044150915345356846, 0.029696570088351894, 0.003496356001582136, -0.13225376277753198, 0.16469970759427863, -0.026821695427786268, -0.04375649494145595, 0.04406733624439236, 0.05999031869008907, 0.06561174654531123, -0.04386853941846839, -0.03867177646278017, -0.0395799956900599]}, {"paper_id": "cfa2646776405d50533055ceb1b7f050e9014dcb", "paper_title": "Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions", "paper_year": 2011, "paper_venue": "EMNLP", "paper_authors": ["Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning"], "paper_citations": 793, "paper_abstract": "We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model's ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.", "citations": ["79dc84a3bf76f1cb983902e2591d913cee5bdb0e", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "26e743d5bd465f49b9538deaf116c15e61b7951f", "59761abc736397539bdd01ad7f9d91c8607c0457", "03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f", "687bac2d3320083eb4530bf18bb8f8f721477600", "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "27e38351e48fe4b7da2775bf94341738bc4da07e", "1510cf4b8abea80b9f352325ca4c132887de21a0", "a4cec122a08216fe8a3bc19b22e78fbaea096256"], "references": ["1a07186bc10592f0330655519ad91652125cd907", "23694a80bf1b9b38215be3e23068dd75296bc90f", "dac72f2c509aee67524d3321f77e97e8eff51de6"], "score": 0.3777168349010157, "embeddings": [0.5650575322018122, -0.48404458605690026, 0.48264844060231726, 0.19012327610957802, 0.08499934803082124, -0.01854968527300834, 0.04461314218320881, 0.00255894788691464, 0.04026027051712501, -0.07935852364630128, -0.10758287557534214, 0.01062438357656768, -0.14652407787021007, -0.22486661115380288, -0.018448393888102238, 0.026373136904725186, 0.0323361568849171, 0.12188348807362846, 0.06298704311762718, 0.024771298203670376, -0.00019356253815120376, 0.09416605293162984, -0.11519507013882313, 0.05979516335185798, 0.01858661075902601, 0.09269264351175356, 0.048689752342343955, 0.08739902859798912, 0.02395052253302859, 0.0011037369144023621, -0.04223256680453548, -0.08702203029180645]}, {"paper_id": "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "paper_title": "Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection", "paper_year": 2011, "paper_venue": "NIPS", "paper_authors": ["Richard Socher", "Eric H. Huang", "Jeffrey Pennington", "Andrew Y. Ng", "Christopher D. Manning"], "paper_citations": 574, "paper_abstract": "Paraphrase detection is the task of examining two sentences and determining whether they have the same meaning. In order to obtain high accuracy on this task, thorough syntactic and semantic analysis of the two statements is needed. We introduce a method for paraphrase detection based on recursive autoencoders (RAE). Our unsupervised RAEs are based on a novel unfolding objective and learn feature vectors for phrases in syntactic trees. These features are used to measure the word- and phrase-wise similarity between two sentences. Since sentences may be of arbitrary length, the resulting matrix of similarity measures is of variable size. We introduce a novel dynamic pooling layer which computes a fixed-sized representation from the variable-sized matrices. The pooled representation is then used as input to a classifier. Our method outperforms other state-of-the-art approaches on the challenging MSRP paraphrase corpus.", "citations": ["0b544dfe355a5070b60986319a3f51fb45d1348e", "79dc84a3bf76f1cb983902e2591d913cee5bdb0e", "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "27e38351e48fe4b7da2775bf94341738bc4da07e", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "a4cec122a08216fe8a3bc19b22e78fbaea096256", "bc1d609520290e0460c49b685675eb5a57fa5935"], "references": ["1a07186bc10592f0330655519ad91652125cd907", "23694a80bf1b9b38215be3e23068dd75296bc90f", "cfa2646776405d50533055ceb1b7f050e9014dcb", "dac72f2c509aee67524d3321f77e97e8eff51de6"], "score": 0.3235326930571138, "embeddings": [0.5594761105028233, -0.429071881211783, 0.3687566128143108, 0.331388277845221, 0.10413715778121721, 0.043134634639027884, 0.015373406728849456, 0.030513319396835804, -0.020889441062115145, -0.04761541195076051, -0.08338217502185437, 0.05317469887322277, -0.11426936550063801, -0.20323344431865284, -0.07662330011868454, -0.13088399780669285, -0.010955641761252789, 0.20477688426589694, 0.11222483184859151, 0.1829366008932113, 0.052871293280753254, 0.04978385583675196, -0.017516041674814072, 0.11563119945257105, 0.021360313846369577, 0.07157456081993492, 0.017119160047056, -0.0695457773860983, 0.005792007357575291, 0.00410481160804275, 0.0172243819952682, 0.18773353213633281]}, {"paper_id": "05aba481e8a221df5d8775a3bb749001e7f2525e", "paper_title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "paper_year": 2010, "paper_venue": "COLT", "paper_authors": ["John C. Duchi", "Elad Hazan", "Yoram Singer"], "paper_citations": 1028, "paper_abstract": "We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.", "citations": ["79dc84a3bf76f1cb983902e2591d913cee5bdb0e", "03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f", "687bac2d3320083eb4530bf18bb8f8f721477600", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "4e88de2930a4435f737c3996287a90ff87b95c59", "a97dc52807d80454e78d255f9fbd7b0fab56bd03", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "a4cec122a08216fe8a3bc19b22e78fbaea096256", "272216c1f097706721096669d85b2843c23fa77d"], "references": ["38211dc39e41273c0007889202c69f841e02248a"], "score": 0.39824955368337783, "embeddings": [0.566815169178247, -0.2177314341454867, 0.14038088513574987, 0.2552343854479732, -0.1631189371885154, 0.10477876244382563, 0.0022341731104450055, 0.09351477750067008, 0.24092882925432688, -0.08726556496898162, -0.14087202271961, -0.1650617308969714, 0.16210705377850412, -0.08295165653978576, -0.22488987648459038, 0.05367762319281339, 0.23658792090160213, -0.1605455667703547, -0.22785077458911965, 0.06443313945346232, -0.013040469475876912, -0.2058246286446283, 0.18074140435284225, -0.1982987338162571, -0.08956038582590914, -0.12717151356816545, -0.01819302614688135, 0.10372260786319162, 0.1045245001767452, 0.03238316170965411, -0.034037885155003514, -0.0012156804002346085]}, {"paper_id": "3a0e788268fafb23ab20da0e98bb578b06830f7d", "paper_title": "From Frequency to Meaning: Vector Space Models of Semantics", "paper_year": 2010, "paper_venue": "J. Artif. Intell. Res.", "paper_authors": ["Peter D. Turney", "Patrick Pantel"], "paper_citations": 1009, "paper_abstract": "Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.", "citations": ["79dc84a3bf76f1cb983902e2591d913cee5bdb0e", "687bac2d3320083eb4530bf18bb8f8f721477600", "27e38351e48fe4b7da2775bf94341738bc4da07e", "1510cf4b8abea80b9f352325ca4c132887de21a0", "dac72f2c509aee67524d3321f77e97e8eff51de6"], "references": ["d87ceda3042f781c341ac17109d1e94a717f5f60", "2c5135a0531bc5ad7dd890f018e67a40529f5bcb", "1a07186bc10592f0330655519ad91652125cd907"], "score": 0.3461523705375868, "embeddings": [0.4582582075475743, -0.41876387617134964, 0.4872474190962771, 0.06220381769800462, -0.029751266277878833, -0.002099489049194743, -0.05437377934911451, -0.1859219704756009, 0.19992602568610277, 0.2167307446387548, -0.10135615371061377, 0.08208705470719208, -0.016503301227961686, 0.2070649620337611, 0.12391047882868114, 0.007822433362776209, -0.26186079446860094, -0.10421946179796286, 0.08441970973581764, 0.01756353108934632, 0.17897525399681824, -0.048335683273480375, -0.13814856871999617, -0.0597087656141454, -0.07735902153397192, -0.06253964821734168, -0.010845035161644765, 0.104234067235147, 0.04382418737285396, -0.007910493211266476, 0.0682434628646415, 0.03196448474157052]}, {"paper_id": "745d86adca56ec50761591733e157f84cfb19671", "paper_title": "Composition in Distributional Models of Semantics", "paper_year": 2010, "paper_venue": "Cognitive Science", "paper_authors": ["Jeff Mitchell", "Mirella Lapata"], "paper_citations": 615, "paper_abstract": "Vector-based models of word meaning have become increasingly popular in cognitive science. The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar. Despite their widespread use, vector-based models are typically directed at representing words in isolation, and methods for constructing representations for phrases or sentences have received little attention in the literature. This is in marked contrast to experimental evidence (e.g., in sentential priming) suggesting that semantic similarity is more complex than simply a relation between isolated words. This article proposes a framework for representing the meaning of word combinations in vector space. Central to our approach is vector composition, which we operationalize in terms of additive and multiplicative functions. Under this framework, we introduce a wide range of composition models that we evaluate empirically on a phrase similarity task.", "citations": ["79dc84a3bf76f1cb983902e2591d913cee5bdb0e", "26e743d5bd465f49b9538deaf116c15e61b7951f", "03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f", "687bac2d3320083eb4530bf18bb8f8f721477600", "27e38351e48fe4b7da2775bf94341738bc4da07e", "4e88de2930a4435f737c3996287a90ff87b95c59"], "references": ["d87ceda3042f781c341ac17109d1e94a717f5f60", "27e5bd13d581ef682b96038dce4c18f260122352", "23694a80bf1b9b38215be3e23068dd75296bc90f"], "score": 0.26092057899042725, "embeddings": [0.46014991436165953, -0.37147718322733864, 0.3920619783902933, 0.16003775537616038, -0.12987699887209078, 0.23342886511275113, 0.08481208087167981, -0.25280127268174024, 0.23103701773750693, -0.11028723241879138, 0.11619930235704425, -0.12251820416584869, 0.0029268350197279224, 0.14228851142507143, 0.07481701805983353, 0.08230827651324753, 0.011025550425061151, -0.20394578362392138, 0.04555182886934544, -0.015513738754194617, -0.17368092430149082, 0.1333947459739589, 0.21065311067960993, 0.007704743010808559, 0.10805263922516697, 0.130376826000012, 0.004685071691455919, -0.10282217873640369, -0.10206733548867465, 0.025286826840972658, 0.06577185381107716, -0.10357073722298701]}, {"paper_id": "23694a80bf1b9b38215be3e23068dd75296bc90f", "paper_title": "A Neural Probabilistic Language Model", "paper_year": 2000, "paper_venue": "NIPS", "paper_authors": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "paper_citations": 1065, "paper_abstract": "A goal of statistical language modeling is to learn the joint probability function of sequences of words. This is intrinsically difficult because of the curse of dimensionality: we propose to fight it with its own weapons. In the proposed approach one learns simultaneously (1) a distributed representation for each word (i.e. a similarity between words) along with (2) the probability function for word sequences, expressed with these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar to words forming an already seen sentence. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach very significantly improves on a state-of-the-art trigram model.", "citations": ["071b16f25117fb6133480c6259227d54fc2a5ea0", "47a87c2cbdd928bb081974d308b3d9cf678d257e", "7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "b9de9599d7241459db9213b5cdd7059696f5ef8d", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "0d67362a5630ec3b7562327acc278c1c996454b5", "745d86adca56ec50761591733e157f84cfb19671", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "cfa2646776405d50533055ceb1b7f050e9014dcb", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "1a07186bc10592f0330655519ad91652125cd907", "39dba6f22d72853561a4ed684be265e179a39e4f", "687bac2d3320083eb4530bf18bb8f8f721477600", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "0b544dfe355a5070b60986319a3f51fb45d1348e", "1005645c05585c2042e3410daeed638b55e2474d", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "2b669398c4cf2ebe04375c8b1beae20f4ac802fa", "a4cec122a08216fe8a3bc19b22e78fbaea096256", "0a10d64beb0931efdc24a28edaa91d539194b2e2", "dac72f2c509aee67524d3321f77e97e8eff51de6"], "references": ["d87ceda3042f781c341ac17109d1e94a717f5f60", "783480acff435bfbc15ffcdb4f15eccddaa0c810"], "score": 0.5046839197131779, "embeddings": [0.6940181181453702, -0.3586584552388144, 0.13886434421194288, 0.20328747520383528, 0.27633848230884855, -0.0315453952346725, -0.08621404149315418, -0.12439360485811124, -0.2566583081222814, -0.13007957973488868, 0.08541801014410187, -0.005059874082575833, -0.1695648299228215, 0.06018882806073554, -0.019823499834948497, -0.1487752209870484, 0.049726876216582123, 0.00842754744616537, 0.09738044748594246, 0.10697574006301187, -0.020539938717340896, -0.13166277997340134, 0.06468507156203801, 0.06268047984228445, 0.10639424269938805, 0.07029879087096573, -0.0706725498326636, -0.09627137459723069, -0.0011170789535459896, 0.0489620075752164, -0.023011242307523667, -0.03190870855770852]}, {"paper_id": "d87ceda3042f781c341ac17109d1e94a717f5f60", "paper_title": "WordNet : an electronic lexical database", "paper_year": 2000, "paper_venue": "", "paper_authors": ["Christiane Fellbaum"], "paper_citations": 1036, "paper_abstract": "Part 1 The lexical database: nouns in WordNet, George A. Miller modifiers in WordNet, Katherine J. Miller a semantic network of English verbs, Christiane Fellbaum design and implementation of the WordNet lexical database and searching software, Randee I. Tengi. Part 2: automated discovery of WordNet relations, Marti A. Hearst representing verb alterations in WordNet, Karen T. Kohl et al the formalization of WordNet by methods of relational concept analysis, Uta E. Priss. Part 3 Applications of WordNet: building semantic concordances, Shari Landes et al performance and confidence in a semantic annotation task, Christiane Fellbaum et al WordNet and class-based probabilities, Philip Resnik combining local context and WordNet similarity for word sense identification, Claudia Leacock and Martin Chodorow using WordNet for text retrieval, Ellen M. Voorhees lexical chains as representations of context for the detection and correction of malapropisms, Graeme Hirst and David St-Onge temporal indexing through lexical chaining, Reem Al-Halimi and Rick Kazman COLOR-X - using knowledge from WordNet for conceptual modelling, J.F.M. Burg and R.P. van de Riet knowledge processing on an extended WordNet, Sanda M. Harabagiu and Dan I Moldovan appendix - obtaining and using WordNet.", "citations": ["a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "38211dc39e41273c0007889202c69f841e02248a", "23694a80bf1b9b38215be3e23068dd75296bc90f", "745d86adca56ec50761591733e157f84cfb19671", "1005645c05585c2042e3410daeed638b55e2474d", "3a0e788268fafb23ab20da0e98bb578b06830f7d", "71b7178df5d2b112d07e45038cb5637208659ff7"], "references": [], "score": 0.28795110852268496, "embeddings": [0.4039495365512248, -0.30370777655233727, 0.1767864476041353, 0.151939397823595, -0.24408807162506704, -0.0861704750959423, 0.04698931120777458, -0.41931588294897526, -0.2479860279997434, -0.0657114184642796, 0.16658709244006428, 0.04348382441959926, 0.03189637499930164, 0.33453835520751574, 0.15452877033391388, 0.04804734150536032, -0.1976864069556901, -0.35463698915269964, 0.024615966242646364, 0.13367139825643204, 0.05853166081738261, -0.08384374081881482, 0.08698123663869535, 0.005518863609624499, 0.04011460075515449, 0.057033390493123975, -0.04035270511024628, 0.03401985758890912, 0.017011260457890812, 0.019535218728799122, -0.019745346559748023, 0.02343629709856659]}]], [[{"paper_id": "7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d", "paper_title": "MaskGAN: Better Text Generation via Filling in the _______", "paper_year": 2018, "paper_venue": "ICLR", "paper_authors": ["William Fedus", "Ian J. Goodfellow", "Andrew M. Dai"], "paper_citations": 95, "paper_abstract": "Neural text generation models are often autoregressive language models or seq2seq models. Neural autoregressive and seq2seq models that generate text by sampling words sequentially, with each word conditioned on the previous model, are state-of-the-art for several machine translation and summarization benchmarks. These benchmarks are often defined by validation perplexity even though this is not a direct measure of sample quality. Language models are typically trained via maximum likelihood and most often with teacher forcing. Teacher forcing is well-suited to optimizing perplexity but can result in poor sample quality because generating text requires conditioning on sequences of words that were never observed at training time. We propose to improve sample quality using Generative Adversarial Network (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation. GANs were originally to designed to output differentiable values, so discrete language generation is challenging for them. We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context. We show qualitatively and quantitatively, evidence that this produces more realistic text samples compared to a maximum likelihood trained model.", "citations": ["df2b0e26d0599ce3e70df8a9da02e51594e0e992"], "references": ["39dba6f22d72853561a4ed684be265e179a39e4f", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "23694a80bf1b9b38215be3e23068dd75296bc90f", "93499a7c7f699b6630a86fad964536f9423bb6d0", "47a87c2cbdd928bb081974d308b3d9cf678d257e", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "d7da009f457917aa381619facfa5ffae9329a6e9", "272216c1f097706721096669d85b2843c23fa77d"], "score": 1.3787367521547995, "embeddings": [0.5091845702680426, 0.0343365230167002, -0.36955132640277305, 0.20337896017469576, 0.3766437722364827, 0.1981136770315532, -0.08589556853874236, -0.1696918559776633, -0.03875162985303764, 0.18145989848380745, 0.06816506030003942, 0.006593518044878293, -0.32702405812685503, -0.0001306093063346233, -0.1271511796752744, 0.19523072329138247, 0.150808509956885, -0.22070220825583242, 0.008822665474643571, 0.052724164486231635, 0.023171886232623333, -0.01939927189662479, -0.10724408104227708, 0.015805860125496885, 0.00017967568487520965, 0.14023313960624106, -0.06716836117859248, -0.007050566179129221, 0.16620164329742082, 0.018862695240389388, -0.03746136967390373, -0.05837699240822688]}, {"paper_id": "b9de9599d7241459db9213b5cdd7059696f5ef8d", "paper_title": "Character-Level Language Modeling with Deeper Self-Attention", "paper_year": 2018, "paper_venue": "AAAI", "paper_authors": ["Rami Al-Rfou", "Dokook Choe", "Noah Constant", "Mandy Guo", "Llion Jones"], "paper_citations": 26, "paper_abstract": "LSTMs and other RNN variants have shown strong performance on character-level language modeling. These models are typically trained using truncated backpropagation through time, and it is common to assume that their success stems from their ability to remember long-term contexts. In this paper, we show that a deep (64-layer) transformer model with fixed context outperforms RNN variants by a large margin, achieving state of the art on two popular benchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. To get good results at this depth, we show that it is important to add auxiliary losses, both at intermediate network layers and intermediate sequence positions.", "citations": ["df2b0e26d0599ce3e70df8a9da02e51594e0e992"], "references": ["0b544dfe355a5070b60986319a3f51fb45d1348e", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "0b3cfbf79d50dae4a16584533227bb728e3522aa", "23694a80bf1b9b38215be3e23068dd75296bc90f", "02b3d1d162080d9aefd3fc30a0bcc9a843073b5d", "47a87c2cbdd928bb081974d308b3d9cf678d257e", "5d833331b0e22ff359db05c62a8bca18c4f04b68"], "score": 1.4343677098997984, "embeddings": [0.4919171632707696, -0.044606217183502786, -0.31618704971729855, 0.07287011143899658, 0.4098458566351575, -0.04414377000228807, 0.11451387822596704, -0.3410208017062075, -0.03280680539372378, -0.1887482962511029, -0.018797017739585527, -0.32377657941369764, -0.10049932237491756, 0.12610642257276214, -0.05962166200102309, -0.25185814042899707, -0.07442071592990394, 0.11078640177760068, 0.03495109186780701, 0.11475277145135811, -0.16152879083924468, -0.012247319838960201, -0.17310244888536175, -0.04551063924075483, -0.049285811219249284, -0.02812063668277624, -0.07408659783362184, -0.007225399488975697, -0.030765660039783784, 0.06398971536346877, 0.10518184110290763, 0.03451325366331986]}, {"paper_id": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "paper_title": "Attention Is All You Need", "paper_year": 2017, "paper_venue": "NIPS", "paper_authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin"], "paper_citations": 1008, "paper_abstract": "The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms. We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.", "citations": ["8c1b00128e74f1cd92aede3959690615695d5101", "93b8da28d006415866bf48f9a6e06b5242129195", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "9784fbf77295860b2e412137b86356d70b25e3c0", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d", "b9de9599d7241459db9213b5cdd7059696f5ef8d"], "references": ["39dba6f22d72853561a4ed684be265e179a39e4f", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "0b544dfe355a5070b60986319a3f51fb45d1348e", "0b3cfbf79d50dae4a16584533227bb728e3522aa", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "02b3d1d162080d9aefd3fc30a0bcc9a843073b5d", "071b16f25117fb6133480c6259227d54fc2a5ea0", "93499a7c7f699b6630a86fad964536f9423bb6d0", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "652d159bf64a70194127722d19841daa99a69b64", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "6c8b30f63f265c32e26d999aa1fef5286b8308ad", "272216c1f097706721096669d85b2843c23fa77d", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd"], "score": 1.5460270822915572, "embeddings": [0.6791715768564098, 0.1599356000841067, -0.4341097047774485, 0.16371262084375668, 0.22215657641069267, 0.19500420510965744, -0.09213495590144083, -0.10217364141863641, 0.08229921899821944, -0.04995777497605693, -0.13809683023688746, 0.0266116517708507, -0.02400592847785969, -0.04593394004101108, 0.2473962956547679, 0.08219409181940168, 0.006405857161344772, -0.0343460902768955, -0.008793492220308969, 0.1159064862108404, -0.16939665874447302, -0.00822444007784812, -0.10386760949976158, -0.0206006939653303, -0.09374271406369154, -0.034880352315281644, -0.07154603311395571, 0.12675325287770092, 0.009547372254249203, 0.012616585320764805, 0.07407502810736694, 0.015971878991865335]}, {"paper_id": "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "paper_title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", "paper_year": 2016, "paper_venue": "ArXiv", "paper_authors": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V. Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey", "Jeff Klingner", "Apurva Shah", "Melvin Johnson", "Xiaobing Liu", "Lukasz Kaiser", "Stephan Gouws", "Yoshikiyo Kato", "Taku Kudo", "Hideto Kazawa", "Keith Stevens", "George Kurian", "Nishant Patil", "Wei Wang", "Cliff Young", "Jason Smith", "Jason Riesa", "Alex Rudnick", "Oriol Vinyals", "Gregory S. Corrado", "Macduff Hughes", "Jeffrey Dean"], "paper_citations": 1000, "paper_abstract": "Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (\"wordpieces\") for both input and output. This method provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.", "citations": ["204e3073870fae3d05bcbc2f6a8e263d9b72e776", "df2b0e26d0599ce3e70df8a9da02e51594e0e992"], "references": ["39dba6f22d72853561a4ed684be265e179a39e4f", "0b544dfe355a5070b60986319a3f51fb45d1348e", "0b3cfbf79d50dae4a16584533227bb728e3522aa", "2c03df8b48bf3fa39054345bafabfeff15bfd11d", "071b16f25117fb6133480c6259227d54fc2a5ea0", "93499a7c7f699b6630a86fad964536f9423bb6d0", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "272216c1f097706721096669d85b2843c23fa77d"], "score": 1.389785052861367, "embeddings": [0.5506052325159075, 0.0919706852999685, -0.37221782715043383, 0.3598420526605347, 0.17106599750395887, 0.24723850975882486, -0.02883019427363745, 0.12194304831845423, 0.03151695129031497, 0.11389028379850821, -0.07747914250060844, 0.09249536825575191, -0.08676888217166383, -0.06725563689845804, 0.2516033456390845, -0.20454772677625097, -0.1202029145643776, -0.0013582397279538912, -0.034778421789290935, 0.1650902552992287, -0.17695619392386217, 0.06780290304896372, 0.14309963055716324, -0.07320674492011933, -0.1157685462336195, -0.014278585837811077, -0.04523649862985377, 0.1628567737810754, 0.06435918261961047, 0.08142455063209848, 0.029189499469524795, -0.10035359241518307]}, {"paper_id": "0e6824e137847be0599bb0032e37042ed2ef5045", "paper_title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books", "paper_year": 2015, "paper_venue": "2015 IEEE International Conference on Computer Vision (ICCV)", "paper_authors": ["Yukun Zhu", "Ryan Kiros", "Richard S. Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler"], "paper_citations": 287, "paper_abstract": "Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in the current datasets. To align movies and books we propose a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.", "citations": ["df2b0e26d0599ce3e70df8a9da02e51594e0e992", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "a97dc52807d80454e78d255f9fbd7b0fab56bd03", "af5c4b80fbf847f69a202ba5a780a3dd18c1a027"], "references": ["39dba6f22d72853561a4ed684be265e179a39e4f", "0a10d64beb0931efdc24a28edaa91d539194b2e2", "0b544dfe355a5070b60986319a3f51fb45d1348e", "0b3cfbf79d50dae4a16584533227bb728e3522aa", "071b16f25117fb6133480c6259227d54fc2a5ea0", "146f6f6ed688c905fb6e346ad02332efd5464616", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "71b7178df5d2b112d07e45038cb5637208659ff7", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "d7da009f457917aa381619facfa5ffae9329a6e9", "272216c1f097706721096669d85b2843c23fa77d"], "score": 1.4793679452230395, "embeddings": [0.6388537641785624, -0.05056830956112717, -0.28411207233961877, 0.20346999302388277, -0.08056793305874752, 0.29341746514392447, 0.11948499521501375, 0.17817447124439603, -0.2042618069974652, 0.14996169641692414, 0.06478197828848554, 0.055704318995121745, 0.16206773863806395, -0.02552315870816835, -0.2665923815023723, -0.12953619913835634, -0.07763678901377205, -0.04692767186903008, -0.023440503650962598, 0.07131174289617888, 0.19248995952379258, 0.061865483347413555, 0.06631954332405557, 0.18533917561588867, -0.09211779757568624, -0.016559671561543594, 0.0532971206464352, -0.041833981563393914, -0.020526735572200375, 0.15411850706920374, 0.10601880727208277, -0.006639301523057839]}], [{"paper_id": "02b3d1d162080d9aefd3fc30a0bcc9a843073b5d", "paper_title": "Exploring the Limits of Language Modeling", "paper_year": 2016, "paper_venue": "ArXiv", "paper_authors": ["Rafal J\u00f3zefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "paper_citations": 479, "paper_abstract": "In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.", "citations": ["3febb2bed8865945e7fddc99efd791887bb7e14f", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "421fc2556836a6b441de806d7b393a35b6eaea58", "59761abc736397539bdd01ad7f9d91c8607c0457", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "b9de9599d7241459db9213b5cdd7059696f5ef8d"], "references": ["39dba6f22d72853561a4ed684be265e179a39e4f", "0b544dfe355a5070b60986319a3f51fb45d1348e", "38211dc39e41273c0007889202c69f841e02248a", "03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f", "1005645c05585c2042e3410daeed638b55e2474d", "652d159bf64a70194127722d19841daa99a69b64", "47a87c2cbdd928bb081974d308b3d9cf678d257e", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "5d833331b0e22ff359db05c62a8bca18c4f04b68"], "score": 0.5187039555235433, "embeddings": [0.6026395927997226, -0.1238272600338589, -0.36035677802077193, 0.03755705458298082, 0.332129768473766, -0.22992626745383013, 0.12760333106542318, -0.3671529250952752, 0.08910145102833172, -0.27019947374847075, -0.021537586464404058, -0.18106965794760818, 0.03173793339306186, -0.07154702211082237, -0.005333906011679103, 0.07913507342038445, -0.024502418686563058, -0.004303300919913558, -0.08881882566732056, -0.061259987666938256, -0.005827604008679424, 0.07994502755979223, -0.11436834336446751, -0.05991029540273965, -0.06323256882787097, -0.062214652941851085, 0.031364263804652355, 0.011657551589646056, -0.06148199267391225, -0.016307166859054832, -0.012396892138036308, 0.028414395474474274]}, {"paper_id": "146f6f6ed688c905fb6e346ad02332efd5464616", "paper_title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "paper_year": 2015, "paper_venue": "ICML", "paper_authors": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio"], "paper_citations": 1022, "paper_abstract": "Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.", "citations": ["1ee46c3b71ebe336d0b278de9093cfca7af7390b", "93499a7c7f699b6630a86fad964536f9423bb6d0", "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "0e6824e137847be0599bb0032e37042ed2ef5045", "a4cec122a08216fe8a3bc19b22e78fbaea096256"], "references": ["39dba6f22d72853561a4ed684be265e179a39e4f", "0b544dfe355a5070b60986319a3f51fb45d1348e", "0b3cfbf79d50dae4a16584533227bb728e3522aa", "071b16f25117fb6133480c6259227d54fc2a5ea0", "061356704ec86334dbbc073985375fe13cd39088", "652d159bf64a70194127722d19841daa99a69b64", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "71b7178df5d2b112d07e45038cb5637208659ff7", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "6c8b30f63f265c32e26d999aa1fef5286b8308ad", "272216c1f097706721096669d85b2843c23fa77d"], "score": 0.29064625618528, "embeddings": [0.5862034930005314, 0.17657506149798574, -0.292887207582469, 0.5288663841994464, -0.08475117216417548, 0.035050432579622315, -0.09039771932045229, 0.04862148325487514, -0.0549640796013535, 0.02373112436279082, -0.010434527207406894, 0.028538077397767876, 0.09659085046445555, -0.05885046254944188, -0.06752425160515642, -0.07801295766141325, -0.22954842012706578, 0.04683715375778481, -0.03335290859821187, -0.03765971091893054, 0.13165733376310992, 0.17867337763266558, 0.119744443666883, 0.18607288637257244, 0.0862102292949114, -0.10294417824550885, 0.14321003613586974, 0.007564428095023213, 0.06027849876626134, 0.1217159076402443, -0.0102434917037162, -0.005004251410908508]}, {"paper_id": "93499a7c7f699b6630a86fad964536f9423bb6d0", "paper_title": "Effective Approaches to Attention-based Neural Machine Translation", "paper_year": 2015, "paper_venue": "EMNLP", "paper_authors": ["Thang Luong", "Hieu Pham", "Christopher D. Manning"], "paper_citations": 1016, "paper_abstract": "An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.", "citations": ["8c1b00128e74f1cd92aede3959690615695d5101", "5171157c2c09a85ad6558c5c03da6b75b0cf5fe6", "1ee46c3b71ebe336d0b278de9093cfca7af7390b", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "9784fbf77295860b2e412137b86356d70b25e3c0", "b1e20420982a4f923c08652941666b189b11b7fe", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd"], "references": ["39dba6f22d72853561a4ed684be265e179a39e4f", "0b544dfe355a5070b60986319a3f51fb45d1348e", "071b16f25117fb6133480c6259227d54fc2a5ea0", "146f6f6ed688c905fb6e346ad02332efd5464616", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "d7da009f457917aa381619facfa5ffae9329a6e9"], "score": 0.3958611717082478, "embeddings": [0.5978761616313863, 0.3194176553211561, -0.18832814328452202, 0.3867820219081542, 0.22772854016420604, 0.24035272255536128, -0.022662295627737444, 0.07986357592331825, 0.007684159706126567, 0.23345726794460622, 0.1379458951039354, 0.14006687022604153, -0.23292560332177406, -0.0034846698875700063, 0.10444175685258104, -0.016857477235381915, 0.019198239701181816, -0.04037598463247392, 0.0130641847845261, 0.04959935953302166, -0.0746899776521859, 0.08114278372185264, 0.02837449423569265, -0.1155196654159113, 0.11083540741300134, -0.09658594891561896, 0.05149276272590135, 0.09818417195377332, -0.004963716951936132, -0.021204416606196087, -0.0982699044299093, 0.07266945576328142]}, {"paper_id": "071b16f25117fb6133480c6259227d54fc2a5ea0", "paper_title": "Neural Machine Translation by Jointly Learning to Align and Translate", "paper_year": 2014, "paper_venue": "ICLR", "paper_authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "paper_citations": 1133, "paper_abstract": "Abstract: Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.", "citations": ["93b8da28d006415866bf48f9a6e06b5242129195", "007ab5528b3bd310a80d553cccad4b78dc496b02", "146f6f6ed688c905fb6e346ad02332efd5464616", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "8c1b00128e74f1cd92aede3959690615695d5101", "e0222a1ae6874f7fff128c3da8769ab95963da04", "93499a7c7f699b6630a86fad964536f9423bb6d0", "4e88de2930a4435f737c3996287a90ff87b95c59", "39dba6f22d72853561a4ed684be265e179a39e4f", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "27e98e09cf09bc13c913d01676e5f32624011050", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "9784fbf77295860b2e412137b86356d70b25e3c0", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "d1505c6123c102e53eb19dff312cb25cea840b72", "0e6824e137847be0599bb0032e37042ed2ef5045", "a4cec122a08216fe8a3bc19b22e78fbaea096256"], "references": ["39dba6f22d72853561a4ed684be265e179a39e4f", "0b544dfe355a5070b60986319a3f51fb45d1348e", "23694a80bf1b9b38215be3e23068dd75296bc90f", "0b3cfbf79d50dae4a16584533227bb728e3522aa", "652d159bf64a70194127722d19841daa99a69b64", "944a1cfd79dbfb6fef460360a0765ba790f4027a"], "score": 0.5296594042593563, "embeddings": [0.7509422817825467, 0.25519909664980606, -0.15792603283227633, 0.3305293234668746, 0.1448776022848937, 0.25901845768067716, -0.046367563092132386, 0.13224039886039426, 0.01651722311078064, 0.006570908978275915, -0.04333740930630037, 0.09644615654263007, 0.11367767671860894, -1.4462628796233423e-06, 0.18160120435406896, -0.10093205870208677, -0.05430251354392717, 0.06225284938620907, 0.0016764006223032992, -0.00010319405552026489, -0.052338147552158286, -0.12247383903720528, 0.114829482933728, -0.06309980382808265, 0.04763999659782201, 0.00022581734825490016, 0.060849716550784236, 0.050249427652497486, -0.0879864296565954, -0.03298934396734205, 0.0028773206035333475, 0.048963147344904756]}, {"paper_id": "272216c1f097706721096669d85b2843c23fa77d", "paper_title": "Adam: A Method for Stochastic Optimization", "paper_year": 2014, "paper_venue": "ICLR", "paper_authors": ["Diederik P. Kingma", "Jimmy Ba"], "paper_citations": 1127, "paper_abstract": "Abstract: We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.", "citations": ["5171157c2c09a85ad6558c5c03da6b75b0cf5fe6", "93b8da28d006415866bf48f9a6e06b5242129195", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "146f6f6ed688c905fb6e346ad02332efd5464616", "7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d", "4361e64f2d12d63476fdc88faf72a0f70d9a2ffb", "8c1b00128e74f1cd92aede3959690615695d5101", "e0222a1ae6874f7fff128c3da8769ab95963da04", "27e98e09cf09bc13c913d01676e5f32624011050", "3febb2bed8865945e7fddc99efd791887bb7e14f", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "59761abc736397539bdd01ad7f9d91c8607c0457", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "0e6824e137847be0599bb0032e37042ed2ef5045", "a4cec122a08216fe8a3bc19b22e78fbaea096256"], "references": ["abd1c342495432171beb7ca8fd9551ef13cbd0ff", "652d159bf64a70194127722d19841daa99a69b64", "05aba481e8a221df5d8775a3bb749001e7f2525e"], "score": 0.6809918415376386, "embeddings": [0.6956781486257053, 0.11278441852130115, -0.2402979982224879, 0.08017547449745602, -0.02911637571407025, 0.09285935219955298, -0.05690351019798265, 0.14019229087476037, -0.053045158142987596, -0.0692550785616437, -0.27985091711184934, -0.014355127165009997, -0.05537757971977888, -0.17150802146600422, -0.06199939067534963, 0.044590467484756355, -0.16348203631228672, -0.19644588694959053, -0.13365500726557186, -0.031272355508406874, 0.11261353365123204, 0.01941047987224567, 0.08217588209809465, -0.11009380052697293, -0.10901044618235164, 0.1543734454670164, -0.20083146640205962, 0.028326408317245595, -0.07714757974303572, -0.1741956302186553, -0.22913607436589906, -0.024308094049570128]}, {"paper_id": "39dba6f22d72853561a4ed684be265e179a39e4f", "paper_title": "Sequence to Sequence Learning with Neural Networks", "paper_year": 2014, "paper_venue": "NIPS", "paper_authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "paper_citations": 1109, "paper_abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.", "citations": ["02b3d1d162080d9aefd3fc30a0bcc9a843073b5d", "071b16f25117fb6133480c6259227d54fc2a5ea0", "26e743d5bd465f49b9538deaf116c15e61b7951f", "146f6f6ed688c905fb6e346ad02332efd5464616", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "421fc2556836a6b441de806d7b393a35b6eaea58", "93499a7c7f699b6630a86fad964536f9423bb6d0", "4e88de2930a4435f737c3996287a90ff87b95c59", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "9784fbf77295860b2e412137b86356d70b25e3c0", "a97dc52807d80454e78d255f9fbd7b0fab56bd03", "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "0e6824e137847be0599bb0032e37042ed2ef5045", "a4cec122a08216fe8a3bc19b22e78fbaea096256"], "references": ["0b544dfe355a5070b60986319a3f51fb45d1348e", "0b3cfbf79d50dae4a16584533227bb728e3522aa", "23694a80bf1b9b38215be3e23068dd75296bc90f", "071b16f25117fb6133480c6259227d54fc2a5ea0", "162d958ff885f1462aeda91cd72582323fd6a1f4", "652d159bf64a70194127722d19841daa99a69b64", "47a87c2cbdd928bb081974d308b3d9cf678d257e", "abd1c342495432171beb7ca8fd9551ef13cbd0ff", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "d7da009f457917aa381619facfa5ffae9329a6e9"], "score": 0.5340424489822737, "embeddings": [0.7348266952882309, -0.08419112528431631, -0.3753177115356881, 0.33212947471922777, 0.24266439464947515, 0.2190041704199295, 0.018988920408090082, 0.09596153127495455, 0.003174138606055701, 0.1293408420147739, 0.08111797106723256, -0.03279796075697742, 0.0833724265428805, 0.05742989842791659, 0.030623190195883228, -0.029821111807663957, -0.002596646634892367, 0.04642484085031365, -0.07990722837970943, -0.11447039095847038, 0.02436940280492787, 0.031874845013333486, 0.01635635375608566, -0.04792183848277204, 0.057419261422153194, 0.036661674487822954, 0.005958101707120036, -0.014559841621503993, -0.04747434609623912, -0.1252657002531535, -0.006424759292153787, -0.013390733256698213]}, {"paper_id": "0b544dfe355a5070b60986319a3f51fb45d1348e", "paper_title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "paper_year": 2014, "paper_venue": "EMNLP", "paper_authors": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "paper_citations": 1080, "paper_abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.", "citations": ["3c78c6df5eb1695b6a399e346dde880af27d1016", "39dba6f22d72853561a4ed684be265e179a39e4f", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "02b3d1d162080d9aefd3fc30a0bcc9a843073b5d", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "26e743d5bd465f49b9538deaf116c15e61b7951f", "071b16f25117fb6133480c6259227d54fc2a5ea0", "b1e20420982a4f923c08652941666b189b11b7fe", "146f6f6ed688c905fb6e346ad02332efd5464616", "93499a7c7f699b6630a86fad964536f9423bb6d0", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "0e6824e137847be0599bb0032e37042ed2ef5045", "a4cec122a08216fe8a3bc19b22e78fbaea096256", "b9de9599d7241459db9213b5cdd7059696f5ef8d", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd"], "references": ["87f40e6f3022adbc1f1905e3e506abad05a9964f", "0b3cfbf79d50dae4a16584533227bb728e3522aa", "23694a80bf1b9b38215be3e23068dd75296bc90f", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "abd1c342495432171beb7ca8fd9551ef13cbd0ff"], "score": 0.537251304538205, "embeddings": [0.6963620101069331, 0.08041625018312555, -0.21163288963023197, 0.37540050276747855, 0.1813849445261383, 0.10742808816173785, 0.05871602508318611, 0.013971158936144883, -0.14937879346794963, -0.022717768024844397, 0.005554020152792559, -0.04391905918848126, -0.007514451643818876, -0.0006703721509186951, 0.03690213624087859, -0.2417761001785078, -0.11365447359338653, 0.16964606714943836, -0.017428824025852885, 0.1243396437042719, -0.07161077493214472, 0.22785172833145415, 0.004999802766363098, -0.11559238509705337, 0.04704861256322049, 0.10554874370873374, -0.0516604620213223, 0.04101285362936129, -0.10844137821947143, -0.09207012758955728, 0.09973330057914721, 0.11987658892591897]}, {"paper_id": "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "paper_title": "Grammar as a Foreign Language", "paper_year": 2014, "paper_venue": "NIPS", "paper_authors": ["Oriol Vinyals", "Lukasz Kaiser", "Terry K Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey E. Hinton"], "paper_citations": 523, "paper_abstract": "Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and inefficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers. It also matches the performance of standard parsers when trained only on a small human-annotated dataset, which shows that this model is highly data-efficient, in contrast to sequence-to-sequence models without the attention mechanism. Our parser is also fast, processing over a hundred sentences per second with an unoptimized CPU implementation.", "citations": ["204e3073870fae3d05bcbc2f6a8e263d9b72e776", "a4cec122a08216fe8a3bc19b22e78fbaea096256", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292"], "references": ["39dba6f22d72853561a4ed684be265e179a39e4f", "0b3cfbf79d50dae4a16584533227bb728e3522aa", "071b16f25117fb6133480c6259227d54fc2a5ea0", "652d159bf64a70194127722d19841daa99a69b64", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "0a10d64beb0931efdc24a28edaa91d539194b2e2"], "score": 0.27824099766090754, "embeddings": [0.5290912145431678, -0.0221202295437298, -0.3518138188086234, 0.3033015473196228, 0.25621548492554624, 0.1686917753744473, -0.11273775834803751, -0.01061082813302539, 0.0891062092611847, -0.003202145723343483, -0.0368837111765606, 0.1001004011965874, 0.3423337282331647, -0.026410105473523675, 0.1974155698165533, 0.24441270556772396, 0.019202896951892076, 0.19810777137961907, 0.14026126491046254, -0.016370578757364158, 0.07007504090363152, -0.23258382860564658, 0.05823211964017733, 0.11934404224187062, 0.09955319706007047, 0.07714172950663173, -0.03271654801302973, 0.03024270858039529, 0.019337539517469995, 0.07069689143612748, 0.03386530683239805, -0.0197426003450545]}, {"paper_id": "652d159bf64a70194127722d19841daa99a69b64", "paper_title": "Generating Sequences With Recurrent Neural Networks", "paper_year": 2013, "paper_venue": "ArXiv", "paper_authors": ["Alex Graves"], "paper_citations": 1016, "paper_abstract": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.", "citations": ["39dba6f22d72853561a4ed684be265e179a39e4f", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "02b3d1d162080d9aefd3fc30a0bcc9a843073b5d", "071b16f25117fb6133480c6259227d54fc2a5ea0", "421fc2556836a6b441de806d7b393a35b6eaea58", "146f6f6ed688c905fb6e346ad02332efd5464616", "272216c1f097706721096669d85b2843c23fa77d"], "references": ["0b44fcbeea9415d400c5f5789d6b892b6f98daff", "1005645c05585c2042e3410daeed638b55e2474d", "0b3cfbf79d50dae4a16584533227bb728e3522aa"], "score": 0.33384768483592403, "embeddings": [0.5233044585113289, 0.023157778068573037, -0.41261516114170155, 0.24862263871755969, 0.31686016844919457, 0.07219642315186918, -0.08775442084950845, -0.18422654422704002, -0.0031134745180269674, -0.17054333224317506, -0.1306545476061329, 0.03803749289857987, 0.29845001797148096, -0.0764432402008973, 0.24554055721550944, 0.23354807581557027, -0.10122437345628037, 0.006591487953722603, -0.14325565837387746, -0.1668137867188939, 0.1248493830031706, 0.0239424190813202, 0.023711851526841327, 0.07180256921891845, 0.021142223709228494, -0.054232824701391825, 0.06222110708778488, -0.01679576316106207, 0.01707765262382724, 0.00011702990226109062, 0.011922211709273923, 0.0210433286492359]}, {"paper_id": "944a1cfd79dbfb6fef460360a0765ba790f4027a", "paper_title": "Recurrent Continuous Translation Models", "paper_year": 2013, "paper_venue": "EMNLP", "paper_authors": ["Nal Kalchbrenner", "Phil Blunsom"], "paper_citations": 643, "paper_abstract": "We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.", "citations": ["39dba6f22d72853561a4ed684be265e179a39e4f", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "071b16f25117fb6133480c6259227d54fc2a5ea0", "03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "146f6f6ed688c905fb6e346ad02332efd5464616", "93499a7c7f699b6630a86fad964536f9423bb6d0", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "0e6824e137847be0599bb0032e37042ed2ef5045", "a4cec122a08216fe8a3bc19b22e78fbaea096256", "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd"], "references": ["23694a80bf1b9b38215be3e23068dd75296bc90f", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "05aba481e8a221df5d8775a3bb749001e7f2525e", "27e38351e48fe4b7da2775bf94341738bc4da07e", "1a07186bc10592f0330655519ad91652125cd907", "47a87c2cbdd928bb081974d308b3d9cf678d257e"], "score": 0.3593933450834664, "embeddings": [0.6205787701927615, -0.20726725543212485, -0.07757248578386752, 0.4733775328953948, 0.2333379210727971, 0.2022879159220717, -0.02453331012700199, 0.12560848180098957, 0.10166039818933975, 0.08118944232594154, -0.00056293946204476, -0.06100811443589709, 0.04640051058735771, -0.07302973335870239, -0.08940497911008725, -0.12606998669957173, -0.051248305021617924, 0.06949312920010614, 0.07448196681017756, 0.16065138214224448, 0.07143979128679584, -0.08764119003142327, 0.2643421487449211, 0.03949533025058359, 0.0854320411903482, -0.03381662862878207, -0.025085216316000757, 0.04858471144717659, -0.04986936234542906, 0.06879854300255508, -0.18897816500922807, 0.02552952545484594]}, {"paper_id": "47a87c2cbdd928bb081974d308b3d9cf678d257e", "paper_title": "Recurrent neural network based language model", "paper_year": 2010, "paper_venue": "INTERSPEECH", "paper_authors": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1s Burget", "Jan \u010cernock\u00fd", "Sanjeev Khudanpur"], "paper_citations": 1042, "paper_abstract": "A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5% on the much harder NIST RT05 task, even when the backoff model is trained on much more data than the RNN LM. We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity. Index Terms: language modeling, recurrent neural networks, speech recognition", "citations": ["39dba6f22d72853561a4ed684be265e179a39e4f", "02b3d1d162080d9aefd3fc30a0bcc9a843073b5d", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d", "b9de9599d7241459db9213b5cdd7059696f5ef8d", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "a97dc52807d80454e78d255f9fbd7b0fab56bd03", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292"], "references": ["23694a80bf1b9b38215be3e23068dd75296bc90f"], "score": 0.43829941981937426, "embeddings": [0.5235003971332602, -0.10229805561507992, -0.26350479542432576, 0.0933055052958665, 0.44488131333067904, -0.04413768387820493, 0.18432682043940987, -0.1963054115069427, 0.047972969710243535, 0.025521866063635754, 0.013884567333715063, -0.33958430479124363, -0.11737152572552535, 0.22659800717566483, -0.28813612134892286, -0.11906133056345095, 0.02035503323381395, 0.04253457736746554, 0.06543544345706018, -0.05070386729179178, -0.05471591672605114, -0.17692189278502796, -0.01710620848565014, 0.10386577719257298, 0.03499048662608611, 0.03952388162516402, -0.04359158600500552, -0.0012100738277460524, 0.04268700482230813, 0.0054871588906907276, -0.15058956159427744, -0.022548933329848128]}, {"paper_id": "d7da009f457917aa381619facfa5ffae9329a6e9", "paper_title": "Bleu: a Method for Automatic Evaluation of Machine Translation", "paper_year": 2001, "paper_venue": "ACL", "paper_authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "paper_citations": 1050, "paper_abstract": "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.", "citations": ["39dba6f22d72853561a4ed684be265e179a39e4f", "9784fbf77295860b2e412137b86356d70b25e3c0", "93499a7c7f699b6630a86fad964536f9423bb6d0", "0e6824e137847be0599bb0032e37042ed2ef5045", "7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d"], "references": [], "score": 0.2761558794374688, "embeddings": [0.4067731743511965, 0.09674310332717193, -0.29361942463760143, 0.2765312589634412, 0.2076310853768386, 0.37759411176781343, -0.04219415353990158, 0.04305498626732772, -0.0816376835598112, 0.31856648077038113, 0.2675107647969469, 0.13381466539253842, -0.29861456219293914, -0.019594068671164904, -0.13709197871102072, 0.07832928245679976, 0.14656097036296695, -0.23080899778381392, 0.0025906329811042207, -0.08220882662321015, 0.1635206004406471, 0.018976424467659125, -0.12363571169465193, -0.034616494286069784, 5.754529854396964e-05, -0.023814020400683204, 0.10281070243840265, -0.029303275814604096, -0.08768457847146684, -0.04874464855613015, 0.11529982585063822, -0.02479573568162205]}, {"paper_id": "0b44fcbeea9415d400c5f5789d6b892b6f98daff", "paper_title": "Building a Large Annotated Corpus of English: The Penn Treebank", "paper_year": 1993, "paper_venue": "Computational Linguistics", "paper_authors": ["Mitchell P. Marcus", "Beatrice Santorini", "Mary Ann Marcinkiewicz"], "paper_citations": 1072, "paper_abstract": "Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant.", "citations": ["47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "3febb2bed8865945e7fddc99efd791887bb7e14f", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "02b3d1d162080d9aefd3fc30a0bcc9a843073b5d", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "05dd7254b632376973f3a1b4d39485da17814df5", "652d159bf64a70194127722d19841daa99a69b64", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d", "9fa8d73e572c3ca824a04a5f551b602a17831bc5"], "references": [], "score": 0.4638494464445886, "embeddings": [0.5225197343670082, 0.08023864168092351, -0.36181120357153485, -0.11442549315494963, 0.2612450813390195, -0.021668440703165447, -0.16295596063060616, -0.3437236245653934, 0.16437570896361636, -0.027117217374895797, -0.11034147568946522, 0.13647618026179983, 0.15061707255811854, -0.11277305589932124, 0.05563840776338929, 0.42023008859216837, 0.15414662439421073, -0.009394296725270306, 0.04502584911236792, 0.10734486542996047, -0.03436107176054819, 0.09126253256663912, -0.06667344273645194, 0.0558103544296475, -0.018467961558721383, 0.07882657381496574, -0.06434815639679842, -0.06877235934163865, 0.04976269873950096, -0.08281761337233301, -0.06593317377324182, 0.07675079883827121]}]], [[{"paper_id": "843959ffdccf31c6694d135fad07425924f785b1", "paper_title": "Extracting and composing robust features with denoising autoencoders", "paper_year": 2008, "paper_venue": "ICML", "paper_authors": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "paper_citations": 1011, "paper_abstract": "Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.", "citations": ["0d67362a5630ec3b7562327acc278c1c996454b5", "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "26e743d5bd465f49b9538deaf116c15e61b7951f", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "a4cec122a08216fe8a3bc19b22e78fbaea096256", "6c8b30f63f265c32e26d999aa1fef5286b8308ad"], "references": ["0d67362a5630ec3b7562327acc278c1c996454b5"], "score": 1.4771959411409044, "embeddings": [0.47805152002469053, -0.10814422523782032, -0.029536045600188892, -0.034440598108103335, -0.08126973570509755, -0.13096264296980317, -0.36585617999020764, 0.3266543737399654, -0.23885155981345277, -0.04049620338329139, -0.13475073752238176, -0.3662095628019113, -0.08236099191586345, 0.27009919412912325, 0.11308316877089507, 0.15445924184507537, 0.1813167993645766, 0.06976099544000339, 0.17487919255042633, -0.011212905185037194, -7.212116601573761e-05, 0.22437403262610292, 0.04097789683163027, 0.02061091655736651, -0.07182447108438966, 0.05008711425258458, 0.1321288474654818, 0.0980808110201786, 0.04489568665499512, 0.04327597982540267, 0.08001463755164394, -0.011356521560823755]}], [{"paper_id": "a4cec122a08216fe8a3bc19b22e78fbaea096256", "paper_title": "Deep Learning", "paper_year": 2015, "paper_venue": "Nature", "paper_authors": ["Ian G Goodfellow", "Yoshua Bengio", "Aaron C. Courville"], "paper_citations": 1002, "paper_abstract": "Machine-learning technology powers many aspects of modern society: from web searches to content filtering on social networks to recommendations on e-commerce websites, and it is increasingly present in consumer products such as cameras and smartphones. Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users\u2019 interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition or machine-learning system required careful engineering and considerable domain expertise to design a feature extractor that transformed the raw data (such as the pixel values of an image) into a suitable internal representation or feature vector from which the learning subsystem, often a classifier, could detect or classify patterns in the input. Representation learning is a set of methods that allows a machine to be fed with raw data and to automatically discover the representations needed for detection or classification. Deep-learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. With the composition of enough such transformations, very complex functions can be learned. For classification tasks, higher layers of representation amplify aspects of the input that are important for discrimination and suppress irrelevant variations. An image, for example, comes in the form of an array of pixel values, and the learned features in the first layer of representation typically represent the presence or absence of edges at particular orientations and locations in the image. The second layer typically detects motifs by spotting particular arrangements of edges, regardless of small variations in the edge positions. The third layer may assemble motifs into larger combinations that correspond to parts of familiar objects, and subsequent layers would detect objects as combinations of these parts. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. In addition to beating records in image recognition and speech recognition, it has beaten other machine-learning techniques at predicting the activity of potential drug molecules, analysing particle accelerator data, reconstructing brain circuits, and predicting the effects of mutations in non-coding DNA on gene expression and disease. Perhaps more surprisingly, deep learning has produced extremely promising results for various tasks in natural language understanding, particularly topic classification, sentiment analysis, question answering and language translation. We think that deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data. New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress.", "citations": ["cb0f3ee1e98faf92429d601cdcd76c69c1e484eb"], "references": ["071b16f25117fb6133480c6259227d54fc2a5ea0", "146f6f6ed688c905fb6e346ad02332efd5464616", "944a1cfd79dbfb6fef460360a0765ba790f4027a", "0d67362a5630ec3b7562327acc278c1c996454b5", "ae5e6c6f5513613a161b2c85563f9708bf2e9178", "061356704ec86334dbbc073985375fe13cd39088", "162d958ff885f1462aeda91cd72582323fd6a1f4", "cfa2646776405d50533055ceb1b7f050e9014dcb", "1a07186bc10592f0330655519ad91652125cd907", "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd", "272216c1f097706721096669d85b2843c23fa77d", "39dba6f22d72853561a4ed684be265e179a39e4f", "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "38211dc39e41273c0007889202c69f841e02248a", "843959ffdccf31c6694d135fad07425924f785b1", "687bac2d3320083eb4530bf18bb8f8f721477600", "0b544dfe355a5070b60986319a3f51fb45d1348e", "0b3cfbf79d50dae4a16584533227bb728e3522aa", "23694a80bf1b9b38215be3e23068dd75296bc90f", "1005645c05585c2042e3410daeed638b55e2474d", "05aba481e8a221df5d8775a3bb749001e7f2525e", "2538e3eb24d26f31482c479d95d2e26c0e79b990", "6c8b30f63f265c32e26d999aa1fef5286b8308ad", "0a10d64beb0931efdc24a28edaa91d539194b2e2", "dac72f2c509aee67524d3321f77e97e8eff51de6"], "score": 0.4365274845045941, "embeddings": [0.7088723037194924, -0.23271112155786133, -0.0461741706694334, 0.39963855377704255, -0.03948413392845396, -0.09808056885747489, -0.3347357705746625, 0.19390756075656954, -0.09508247507170735, -0.1386941232966462, 0.04171200550164349, -0.07949008375428962, 0.06764496084267622, -0.06041875234773874, -0.021663846443183622, -0.008106712371901451, -0.002265538194329721, 0.1572131849112935, 0.06703687566047907, 0.042814802883390374, 0.09477343547429418, -0.1044143936731818, 0.043851616987420045, -0.04838925660281771, 0.059617499819831644, 0.0776078588167675, 0.010629449238322588, 0.0015378855031553183, 0.09105406835788248, -0.010255397064361635, 0.044116903872346604, -0.018629671165212587]}, {"paper_id": "6c8b30f63f265c32e26d999aa1fef5286b8308ad", "paper_title": "Dropout: a simple way to prevent neural networks from overfitting", "paper_year": 2014, "paper_venue": "J. Mach. Learn. Res.", "paper_authors": ["Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "paper_citations": 1065, "paper_abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.", "citations": ["5171157c2c09a85ad6558c5c03da6b75b0cf5fe6", "3febb2bed8865945e7fddc99efd791887bb7e14f", "27e98e09cf09bc13c913d01676e5f32624011050", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "e0222a1ae6874f7fff128c3da8769ab95963da04", "007ab5528b3bd310a80d553cccad4b78dc496b02", "146f6f6ed688c905fb6e346ad02332efd5464616", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "a4cec122a08216fe8a3bc19b22e78fbaea096256", "4361e64f2d12d63476fdc88faf72a0f70d9a2ffb"], "references": ["abd1c342495432171beb7ca8fd9551ef13cbd0ff", "843959ffdccf31c6694d135fad07425924f785b1"], "score": 0.5435399071969504, "embeddings": [0.5871349398250891, 0.34040040926624004, -0.10439204245148136, 0.041138530511109495, -0.19502345131736246, -0.01935262850656709, -0.2923503893808142, 0.13798323923442976, 0.055382628148643545, -0.21992495487746702, -0.3991815394868064, -0.10941313018925689, -0.11387679725455739, 0.1092265883279243, 0.01339929016832044, -0.047844471890343986, 0.008350486172672792, -0.12079743675342675, 0.10997600002404992, -0.10223987302856004, 0.010750207220374654, 0.010240727552851846, -0.034326963720379654, 0.028311985294022148, 0.0284778068511716, 0.06958643236779644, 0.12275337323588605, 0.08551626324443765, -0.19807422346825515, -0.12038993096129919, -0.05572206227322919, 0.0974593035397095]}, {"paper_id": "2538e3eb24d26f31482c479d95d2e26c0e79b990", "paper_title": "Natural Language Processing (almost) from Scratch", "paper_year": 2011, "paper_venue": "J. Mach. Learn. Res.", "paper_authors": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel P. Kuksa"], "paper_citations": 1040, "paper_abstract": "We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.", "citations": ["3febb2bed8865945e7fddc99efd791887bb7e14f", "93b8da28d006415866bf48f9a6e06b5242129195", "9784fbf77295860b2e412137b86356d70b25e3c0", "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "d1505c6123c102e53eb19dff312cb25cea840b72", "a4cec122a08216fe8a3bc19b22e78fbaea096256"], "references": ["2c5135a0531bc5ad7dd890f018e67a40529f5bcb", "783480acff435bfbc15ffcdb4f15eccddaa0c810", "162d958ff885f1462aeda91cd72582323fd6a1f4", "0c7f52c753a65ceaf3755e20b906ffd0c05c994a", "dac72f2c509aee67524d3321f77e97e8eff51de6"], "score": 0.5928354278751087, "embeddings": [0.6928814376257083, -0.0941333647908093, 0.08052570982475071, -0.31277031055386695, 0.09188517189561692, -0.1820011135803741, -0.22899204536734602, 0.2603874677031953, 0.08396129194999305, 0.11545733271356391, 0.2686709243648319, 0.15458308203716709, 0.0626378347447292, 0.00564974271316684, -0.033987479948993746, -0.09882941105375034, -0.13613628705271963, 0.0002207776348459735, -0.11221383393781424, -0.10472947704824594, -0.10850970213552077, -0.12335466676264133, -0.08483286583105565, -0.060734654357271815, 0.1592539821106756, 0.02833045549960857, 0.046589360226652915, -0.02151517897681496, 0.05529123473735965, -0.0001765177135487553, -0.028908422239719397, 0.03066632939762802]}, {"paper_id": "0d67362a5630ec3b7562327acc278c1c996454b5", "paper_title": "Learning Deep Architectures for AI", "paper_year": 2007, "paper_venue": "Foundations and Trends in Machine Learning", "paper_authors": ["Yoshua Bengio"], "paper_citations": 1011, "paper_abstract": "Theoretical results strongly suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one needs deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult optimization task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This paper discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.", "citations": ["f37e1b62a767a307c046404ca96bc140b3e68cb5", "a4cec122a08216fe8a3bc19b22e78fbaea096256", "843959ffdccf31c6694d135fad07425924f785b1"], "references": ["843959ffdccf31c6694d135fad07425924f785b1", "23694a80bf1b9b38215be3e23068dd75296bc90f", "1005645c05585c2042e3410daeed638b55e2474d", "162d958ff885f1462aeda91cd72582323fd6a1f4", "1a07186bc10592f0330655519ad91652125cd907"], "score": 0.35997332080236966, "embeddings": [0.4493381575938659, -0.2448448347428844, 0.1243085403830864, 0.10921450335843272, 0.12256150629877617, -0.2075877775615012, -0.5113484940544949, 0.24446904057291047, -0.24546289554573175, -0.17407322638591693, 0.1373988603810833, -0.21452225489517424, -0.06169162941703312, 0.18754957002652228, 0.14547576267686577, 0.021139319887352983, 0.1796577705602609, 0.04047947657773044, 0.03706544238131481, 0.07810676952710696, 0.13280361523141962, -0.009839398151801041, 0.017022994917569093, -0.0017151883840145791, -0.11076406001631608, -0.10691183562033557, -0.09044700705339873, -0.12950486864757185, 0.006686036245412747, -0.06168404245131638, -0.053882267896756014, -0.023379546312345373]}, {"paper_id": "0c7f52c753a65ceaf3755e20b906ffd0c05c994a", "paper_title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data", "paper_year": 2001, "paper_venue": "ICML", "paper_authors": ["John D. Lafferty", "Andrew McCallum", "Fernando C Pereira"], "paper_citations": 1043, "paper_abstract": "We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.", "citations": ["2538e3eb24d26f31482c479d95d2e26c0e79b990", "3febb2bed8865945e7fddc99efd791887bb7e14f", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "ac11062f1f368d97f4c826c317bf50dcc13fdb59"], "references": ["162d958ff885f1462aeda91cd72582323fd6a1f4"], "score": 0.3593817727563252, "embeddings": [0.4208938281766446, -0.11575785967248282, -0.08732468551144462, -0.31401280193282444, 0.1903353973057817, -0.3631903136989129, -0.139891720222987, 0.3028204293496712, 0.3105001621459454, -0.10317304825104186, 0.36043004830447906, 0.05320353961166695, 0.055470785381318104, -0.07660999719032159, -0.09377281020280202, 0.022148580452955872, -0.19459572901883607, -0.1736794822126305, -0.07869018275397166, 0.03324772387369317, -0.10008568548125792, 0.0028486129223181064, -0.03541778360025407, 0.17671258376355936, 0.006175055787442687, 0.0978734002376861, -0.003980453129054273, 0.15081016056839025, -0.07772329383107136, -0.041805853386469825, 0.05714418265388034, 0.06723922751331202]}, {"paper_id": "162d958ff885f1462aeda91cd72582323fd6a1f4", "paper_title": "Gradient-based learning applied to document recognition", "paper_year": 1998, "paper_venue": "", "paper_authors": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "paper_citations": 1037, "paper_abstract": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.", "citations": ["0d67362a5630ec3b7562327acc278c1c996454b5", "39dba6f22d72853561a4ed684be265e179a39e4f", "0c7f52c753a65ceaf3755e20b906ffd0c05c994a", "03ff3f8f4d5a700fbe8f3a3e63a39523c29bb60f", "1a07186bc10592f0330655519ad91652125cd907", "2538e3eb24d26f31482c479d95d2e26c0e79b990", "a4cec122a08216fe8a3bc19b22e78fbaea096256"], "references": [], "score": 0.22873483545292608, "embeddings": [0.4575314300864132, -0.252109509938636, 0.07817242725804638, 0.11469690118766461, 0.15624785595767143, -0.20448656550237176, -0.4549575124532408, 0.3796663633635906, 0.19332172567544584, -0.06741038859183264, 0.40251765841207654, -0.1330135940624831, 0.11609941593843971, 0.0114664286036054, -0.0005353366910297892, 0.004641025910051741, -0.03346095622189398, -0.04282882623151444, -0.17263177731429066, 0.007322037720565052, -0.029505734272535847, 0.031211177021738402, -0.09442379588642315, 0.011401376925742671, -0.04032607796417303, -0.0136441083027814, -0.05798030294670046, -0.051125528788724554, -0.010956692189882348, 0.05101034118890853, -0.028374444360635145, -0.014162408277315172]}]]], "importance": [5.8424199187021815, 14.901759132571357, 45.87656968929864, 19.91303813933992, 12.865227698446835, 3.9981886897291785], "clusterNames": ["neural network", "reading comprehension", "natural language inference", "language model", "machine translation", "learning deep architecture"], "tagGroups": [["neural network", "deep convolutional neural", "object recognition", "computer vision", "current state"], ["reading comprehension", "question answering", "model achieves", "stanford question answering", "natural language"], ["natural language inference", "language model", "wide range", "nlp task", "large corpus"], ["language model", "natural language", "neural network", "vector space", "learning algorithm"], ["machine translation", "source sentence", "recurrent neural network", "language model", "neural machine translation"], ["learning deep architecture", "unsupervised learning", "learning algorithm", "task including", "internal representation"]], "related_paper_titles": ["Improving Language Understanding by Generative Pre-Training", "Glove: Global Vectors for Word Representation", "Deep contextualized word representations", "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "Distributed Representations of Words and Phrases and their Compositionality"]}
