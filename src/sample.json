{"root": {"paper_id": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "paper_year": 2018, "paper_venue": "NAACL-HLT", "paper_citations": 997, "paper_abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. \nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "citations": [], "references": ["3c78c6df5eb1695b6a399e346dde880af27d1016", "1510cf4b8abea80b9f352325ca4c132887de21a0", "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "27e98e09cf09bc13c913d01676e5f32624011050", "dac72f2c509aee67524d3321f77e97e8eff51de6", "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "05dd7254b632376973f3a1b4d39485da17814df5", "081651b38ff7533550a3adfc1c00da333a8fe86c", "93b8da28d006415866bf48f9a6e06b5242129195", "421fc2556836a6b441de806d7b393a35b6eaea58", "26e743d5bd465f49b9538deaf116c15e61b7951f", "9fa8d73e572c3ca824a04a5f551b602a17831bc5", "128cb6b891aee1b5df099acb48e2efecfcff689f", "687bac2d3320083eb4530bf18bb8f8f721477600", "766ce989b8b8b984f7a4691fd8c9af4bdb2b74cd", "38211dc39e41273c0007889202c69f841e02248a", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "783480acff435bfbc15ffcdb4f15eccddaa0c810", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "b9de9599d7241459db9213b5cdd7059696f5ef8d", "843959ffdccf31c6694d135fad07425924f785b1", "8c1b00128e74f1cd92aede3959690615695d5101", "3febb2bed8865945e7fddc99efd791887bb7e14f", "59761abc736397539bdd01ad7f9d91c8607c0457", "4361e64f2d12d63476fdc88faf72a0f70d9a2ffb", "475354f10798f110d34792b6d88f31d6d5cb099e", "1a07186bc10592f0330655519ad91652125cd907", "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d", "26b47e35fe6e4260fdf7b7cc98f279a73c277494", "e0222a1ae6874f7fff128c3da8769ab95963da04", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "a97dc52807d80454e78d255f9fbd7b0fab56bd03", "007ab5528b3bd310a80d553cccad4b78dc496b02", "af5c4b80fbf847f69a202ba5a780a3dd18c1a027", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "0e6824e137847be0599bb0032e37042ed2ef5045", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "2c5135a0531bc5ad7dd890f018e67a40529f5bcb", "1e077413b25c4d34945cc2707e17e46ed4fe784a", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "1005645c05585c2042e3410daeed638b55e2474d", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "bc1d609520290e0460c49b685675eb5a57fa5935", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e"]}, "branches": [[[{"paper_id": "f04df4e20a18358ea2f689b4c129781628ef7fc1", "paper_title": "A large annotated corpus for learning natural language inference", "paper_year": 2015, "paper_venue": "EMNLP", "paper_citations": 808, "paper_abstract": "Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.", "citations": ["a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "3febb2bed8865945e7fddc99efd791887bb7e14f", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "af5c4b80fbf847f69a202ba5a780a3dd18c1a027", "93b8da28d006415866bf48f9a6e06b5242129195"], "references": ["f37e1b62a767a307c046404ca96bc140b3e68cb5", "687bac2d3320083eb4530bf18bb8f8f721477600"]}, {"paper_id": "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "paper_title": "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data", "paper_year": 2017, "paper_venue": "EMNLP", "paper_citations": 474, "paper_abstract": "Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.", "citations": ["1e077413b25c4d34945cc2707e17e46ed4fe784a", "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "bc1d609520290e0460c49b685675eb5a57fa5935", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "af5c4b80fbf847f69a202ba5a780a3dd18c1a027", "93b8da28d006415866bf48f9a6e06b5242129195", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e"], "references": ["26e743d5bd465f49b9538deaf116c15e61b7951f", "0e6824e137847be0599bb0032e37042ed2ef5045", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "38211dc39e41273c0007889202c69f841e02248a", "1510cf4b8abea80b9f352325ca4c132887de21a0", "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "14ce7635ff18318e7094417d0f92acbec6669f1c", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "1a07186bc10592f0330655519ad91652125cd907", "23694a80bf1b9b38215be3e23068dd75296bc90f", "39dba6f22d72853561a4ed684be265e179a39e4f", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "0a6383b13794452fb7339a7f8a5384885186ccf6"]}, {"paper_id": "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "paper_title": "A Decomposable Attention Model for Natural Language Inference", "paper_year": 2016, "paper_venue": "EMNLP", "paper_citations": 410, "paper_abstract": "We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.", "citations": ["26b47e35fe6e4260fdf7b7cc98f279a73c277494", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "af5c4b80fbf847f69a202ba5a780a3dd18c1a027"], "references": ["f04df4e20a18358ea2f689b4c129781628ef7fc1", "f37e1b62a767a307c046404ca96bc140b3e68cb5"]}, {"paper_id": "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "paper_title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference", "paper_year": 2017, "paper_venue": "NAACL-HLT", "paper_citations": 314, "paper_abstract": "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. In addition to being one of the largest corpora available for the task of NLI, at 433k examples, this corpus improves upon available resources in its coverage: it offers data from ten distinct genres of written and spoken English--making it possible to evaluate systems on nearly the full complexity of the language--and it offers an explicit setting for the evaluation of cross-genre domain adaptation.", "citations": ["ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "af5c4b80fbf847f69a202ba5a780a3dd18c1a027", "93b8da28d006415866bf48f9a6e06b5242129195"], "references": ["f37e1b62a767a307c046404ca96bc140b3e68cb5", "9fa8d73e572c3ca824a04a5f551b602a17831bc5", "1a2a770d23b4a171fa81de62a78a3deb0588f238", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "2cd8e8f510c89c7c18268e8ad51c061e459ad321"]}, {"paper_id": "475354f10798f110d34792b6d88f31d6d5cb099e", "paper_title": "Automatically Constructing a Corpus of Sentential Paraphrases", "paper_year": 2005, "paper_venue": "IWP@IJCNLP", "paper_citations": 192, "paper_abstract": "An obstacle to research in automatic paraphrase identification and generation is the lack of large-scale, publiclyavailable labeled corpora of sentential paraphrases. This paper describes the creation of the recently-released Microsoft Research Paraphrase Corpus, which contains 5801 sentence pairs, each hand-labeled with a binary judgment as to whether the pair constitutes a paraphrase. The corpus was created using heuristic extraction techniques in conjunction with an SVM-based classifier to select likely sentence-level paraphrases from a large corpus of topicclustered news data. These pairs were then submitted to human judges, who confirmed that 67% were in fact semantically equivalent. In addition to describing the corpus itself, we explore a number of issues that arose in defining guidelines for the human raters.", "citations": ["93b8da28d006415866bf48f9a6e06b5242129195"], "references": ["4de39c94e340a108fff01a90a67b0c17c86fb981", "0e2795b1329b25ba3709584b96fd5cb4c96f6f22"]}, {"paper_id": "93b8da28d006415866bf48f9a6e06b5242129195", "paper_title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "paper_year": 2018, "paper_venue": "ICLR", "paper_citations": 159, "paper_abstract": "For natural language understanding (NLU) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark (GLUE), a tool for evaluating and analyzing the performance of models across a diverse range of existing NLU tasks. GLUE is model-agnostic, but it incentivizes sharing knowledge across tasks because certain tasks have very limited training data. We further provide a hand-crafted diagnostic test suite that enables detailed linguistic analysis of NLU models. We evaluate baselines based on current methods for multi-task and transfer learning and find that they do not immediately give substantial improvements over the aggregate performance of training a separate model per task, indicating room for improvement in developing general and robust NLU systems.", "citations": ["af5c4b80fbf847f69a202ba5a780a3dd18c1a027", "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb"], "references": ["26e743d5bd465f49b9538deaf116c15e61b7951f", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "475354f10798f110d34792b6d88f31d6d5cb099e", "687bac2d3320083eb4530bf18bb8f8f721477600", "128cb6b891aee1b5df099acb48e2efecfcff689f", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "1510cf4b8abea80b9f352325ca4c132887de21a0", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "05dd7254b632376973f3a1b4d39485da17814df5", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "3febb2bed8865945e7fddc99efd791887bb7e14f", "007ab5528b3bd310a80d553cccad4b78dc496b02", "5d833331b0e22ff359db05c62a8bca18c4f04b68"]}, {"paper_id": "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "paper_title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation", "paper_year": 2017, "paper_venue": "SemEval@ACL", "paper_citations": 156, "paper_abstract": "Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in all language tracks. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the STS Benchmark is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017).", "citations": ["93b8da28d006415866bf48f9a6e06b5242129195"], "references": ["26e743d5bd465f49b9538deaf116c15e61b7951f", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "1510cf4b8abea80b9f352325ca4c132887de21a0", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c"]}], [{"paper_id": "0e2795b1329b25ba3709584b96fd5cb4c96f6f22", "paper_title": "A Systematic Comparison of Various Statistical Alignment Models", "paper_year": 2003, "paper_venue": "Computational Linguistics", "paper_citations": 999, "paper_abstract": "We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.", "citations": [], "references": []}]], [[{"paper_id": "843959ffdccf31c6694d135fad07425924f785b1", "paper_title": "Extracting and composing robust features with denoising autoencoders", "paper_year": 2008, "paper_venue": "ICML", "paper_citations": 999, "paper_abstract": "Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.", "citations": [], "references": ["cbd1ade5b869b13d1853aa0753b82fb35c26bcba", "213d7af7107fa4921eb0adea82c9f711fd105232"]}, {"paper_id": "3febb2bed8865945e7fddc99efd791887bb7e14f", "paper_title": "Deep contextualized word representations", "paper_year": 2018, "paper_venue": "NAACL-HLT", "paper_citations": 999, "paper_abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pretrained network is crucial, allowing downstream models to mix different types of semi-supervision signals.", "citations": ["1e077413b25c4d34945cc2707e17e46ed4fe784a", "27e98e09cf09bc13c913d01676e5f32624011050", "26b47e35fe6e4260fdf7b7cc98f279a73c277494", "e0222a1ae6874f7fff128c3da8769ab95963da04", "af5c4b80fbf847f69a202ba5a780a3dd18c1a027", "93b8da28d006415866bf48f9a6e06b5242129195", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "421fc2556836a6b441de806d7b393a35b6eaea58"], "references": ["687bac2d3320083eb4530bf18bb8f8f721477600", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "3c78c6df5eb1695b6a399e346dde880af27d1016", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "dac72f2c509aee67524d3321f77e97e8eff51de6", "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "59761abc736397539bdd01ad7f9d91c8607c0457", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "05dd7254b632376973f3a1b4d39485da17814df5", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb", "007ab5528b3bd310a80d553cccad4b78dc496b02", "0a6383b13794452fb7339a7f8a5384885186ccf6", "5d833331b0e22ff359db05c62a8bca18c4f04b68"]}, {"paper_id": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "paper_title": "Attention Is All You Need", "paper_year": 2017, "paper_venue": "NIPS", "paper_citations": 997, "paper_abstract": "The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms. We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.", "citations": ["b9de9599d7241459db9213b5cdd7059696f5ef8d", "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d"], "references": ["39dba6f22d72853561a4ed684be265e179a39e4f", "2d9e3f53fcdb548b0b3c4d4efb197f164fe0c381", "0626908dd710b91aece1a81f4ca0635f23fc47f3", "2cd8e8f510c89c7c18268e8ad51c061e459ad321"]}, {"paper_id": "6e795c6e9916174ae12349f5dc3f516570c17ce8", "paper_title": "Skip-Thought Vectors", "paper_year": 2015, "paper_venue": "NIPS", "paper_citations": 846, "paper_abstract": "We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.", "citations": ["0e6824e137847be0599bb0032e37042ed2ef5045", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "a97dc52807d80454e78d255f9fbd7b0fab56bd03", "bc1d609520290e0460c49b685675eb5a57fa5935", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "af5c4b80fbf847f69a202ba5a780a3dd18c1a027", "59761abc736397539bdd01ad7f9d91c8607c0457", "93b8da28d006415866bf48f9a6e06b5242129195", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e"], "references": ["0e6824e137847be0599bb0032e37042ed2ef5045", "687bac2d3320083eb4530bf18bb8f8f721477600", "1510cf4b8abea80b9f352325ca4c132887de21a0", "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "061356704ec86334dbbc073985375fe13cd39088", "39dba6f22d72853561a4ed684be265e179a39e4f", "2d9e3f53fcdb548b0b3c4d4efb197f164fe0c381"]}, {"paper_id": "5d833331b0e22ff359db05c62a8bca18c4f04b68", "paper_title": "One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling", "paper_year": 2013, "paper_venue": "INTERSPEECH", "paper_citations": 423, "paper_abstract": "We propose a new benchmark corpus to be used for measuring progress in statistical language modeling. With almost one billion words of training data, we hope this benchmark will be useful to quickly evaluate novel language modeling techniques, and to compare their contribution when combined with other advanced techniques. We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned Kneser-Ney 5-gram model achieves perplexity 67.6; a combination of techniques leads to 35% reduction in perplexity, or 10% reduction in cross-entropy (bits), over that baseline. \nThe benchmark is available as a code.google.com project; besides the scripts needed to rebuild the training/held-out data, it also makes available log-probability values for each word in each of ten held-out data sets, for each of the baseline n-gram models.", "citations": ["0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "d0e51833b3db3af1c762ab723efd08f117a497c8", "b9de9599d7241459db9213b5cdd7059696f5ef8d", "3febb2bed8865945e7fddc99efd791887bb7e14f", "93b8da28d006415866bf48f9a6e06b5242129195", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "421fc2556836a6b441de806d7b393a35b6eaea58"], "references": ["47a87c2cbdd928bb081974d308b3d9cf678d257e"]}, {"paper_id": "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "paper_title": "Semi-supervised Sequence Learning", "paper_year": 2015, "paper_venue": "NIPS", "paper_citations": 364, "paper_abstract": "We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a \"pretraining\" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.", "citations": ["0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "1e077413b25c4d34945cc2707e17e46ed4fe784a", "a97dc52807d80454e78d255f9fbd7b0fab56bd03", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "3febb2bed8865945e7fddc99efd791887bb7e14f", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e"], "references": ["56623a496727d5c71491850e04512ddf4152b487", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "687bac2d3320083eb4530bf18bb8f8f721477600", "51a55df1f023571a7e07e338ee45a3e3d66ef73e", "2c5135a0531bc5ad7dd890f018e67a40529f5bcb", "1510cf4b8abea80b9f352325ca4c132887de21a0", "052b1d8ce63b07fec3de9dbb583772d860b7c769", "649d03490ef72c5274e3bccd03d7a299d2f8da91", "39dba6f22d72853561a4ed684be265e179a39e4f", "47a87c2cbdd928bb081974d308b3d9cf678d257e"]}, {"paper_id": "1e077413b25c4d34945cc2707e17e46ed4fe784a", "paper_title": "Universal Language Model Fine-tuning for Text Classification", "paper_year": 2018, "paper_venue": "ACL", "paper_citations": 332, "paper_abstract": "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.", "citations": ["ac11062f1f368d97f4c826c317bf50dcc13fdb59", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e"], "references": ["1827de6fa9c9c1b3d647a9d707042e89cf94abf0", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "51a55df1f023571a7e07e338ee45a3e3d66ef73e", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "649d03490ef72c5274e3bccd03d7a299d2f8da91", "3febb2bed8865945e7fddc99efd791887bb7e14f", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "4a18360a14facea50dc819145b1daf4c53d5d59e", "081651b38ff7533550a3adfc1c00da333a8fe86c"]}, {"paper_id": "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "paper_title": "Learned in Translation: Contextualized Word Vectors", "paper_year": 2017, "paper_venue": "NIPS", "paper_citations": 244, "paper_abstract": "Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.", "citations": ["1e077413b25c4d34945cc2707e17e46ed4fe784a", "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "3febb2bed8865945e7fddc99efd791887bb7e14f", "93b8da28d006415866bf48f9a6e06b5242129195", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e"], "references": ["26e743d5bd465f49b9538deaf116c15e61b7951f", "1827de6fa9c9c1b3d647a9d707042e89cf94abf0", "687bac2d3320083eb4530bf18bb8f8f721477600", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "38211dc39e41273c0007889202c69f841e02248a", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "061356704ec86334dbbc073985375fe13cd39088", "649d03490ef72c5274e3bccd03d7a299d2f8da91", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "39dba6f22d72853561a4ed684be265e179a39e4f", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "05dd7254b632376973f3a1b4d39485da17814df5", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "2f4df08d9072fc2ac181b7fced6a245315ce05c8", "007ab5528b3bd310a80d553cccad4b78dc496b02"]}, {"paper_id": "7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d", "paper_title": "MaskGAN: Better Text Generation via Filling in the _______", "paper_year": 2018, "paper_venue": "ICLR", "paper_citations": 95, "paper_abstract": "Neural text generation models are often autoregressive language models or seq2seq models. Neural autoregressive and seq2seq models that generate text by sampling words sequentially, with each word conditioned on the previous model, are state-of-the-art for several machine translation and summarization benchmarks. These benchmarks are often defined by validation perplexity even though this is not a direct measure of sample quality. Language models are typically trained via maximum likelihood and most often with teacher forcing. Teacher forcing is well-suited to optimizing perplexity but can result in poor sample quality because generating text requires conditioning on sequences of words that were never observed at training time. We propose to improve sample quality using Generative Adversarial Network (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation. GANs were originally to designed to output differentiable values, so discrete language generation is challenging for them. We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context. We show qualitatively and quantitatively, evidence that this produces more realistic text samples compared to a maximum likelihood trained model.", "citations": [], "references": ["649d03490ef72c5274e3bccd03d7a299d2f8da91", "d0e51833b3db3af1c762ab723efd08f117a497c8", "39dba6f22d72853561a4ed684be265e179a39e4f", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "23694a80bf1b9b38215be3e23068dd75296bc90f", "47a87c2cbdd928bb081974d308b3d9cf678d257e"]}, {"paper_id": "bc1d609520290e0460c49b685675eb5a57fa5935", "paper_title": "An efficient framework for learning sentence representations", "paper_year": 2018, "paper_venue": "ICLR", "paper_citations": 57, "paper_abstract": "In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.", "citations": [], "references": ["26e743d5bd465f49b9538deaf116c15e61b7951f", "687bac2d3320083eb4530bf18bb8f8f721477600", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "1510cf4b8abea80b9f352325ca4c132887de21a0", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "c4fd9c86b2b41df51a6fe212406dda81b1997fd4", "a97dc52807d80454e78d255f9fbd7b0fab56bd03", "1005645c05585c2042e3410daeed638b55e2474d", "446fbff6a2a7c9989b0a0465f960e236d9a5e886"]}, {"paper_id": "4361e64f2d12d63476fdc88faf72a0f70d9a2ffb", "paper_title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units", "paper_year": 2017, "paper_venue": "ArXiv", "paper_citations": 45, "paper_abstract": "We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic regularizer which randomly applies the identity or zero map to a neuron's input. This stochastic regularizer is comparable to nonlinearities aided by dropout, but it removes the need for a traditional nonlinearity. The connection between the GELU and the stochastic regularizer suggests a new probabilistic understanding of nonlinearities. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all tasks.", "citations": [], "references": ["061356704ec86334dbbc073985375fe13cd39088"]}, {"paper_id": "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "paper_title": "Dissecting Contextual Word Embeddings: Architecture and Representation", "paper_year": 2018, "paper_venue": "EMNLP", "paper_citations": 43, "paper_abstract": "Contextual word representations derived from pre-trained bidirectional language models (biLMs) have recently been shown to provide significant improvements to the state of the art for a wide range of NLP tasks. However, many questions remain as to how and why these models are so effective. In this paper, we present a detailed empirical study of how the choice of neural architecture (e.g. LSTM, CNN, or self attention) influences both end task accuracy and qualitative properties of the representations that are learned. We show there is a tradeoff between speed and accuracy, but all architectures learn high quality contextual representations that outperform word embeddings for four challenging NLP tasks. Additionally, all architectures learn representations that vary with network depth, from exclusively morphological based at the word embedding layer through local syntax based in the lower contextual layers to longer range semantics such coreference at the upper layers. Together, these results suggest that unsupervised biLMs, independent of architecture, are learning much more about the structure of language than previously appreciated.", "citations": [], "references": ["f37e1b62a767a307c046404ca96bc140b3e68cb5", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "1e077413b25c4d34945cc2707e17e46ed4fe784a", "1a2a770d23b4a171fa81de62a78a3deb0588f238", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "3febb2bed8865945e7fddc99efd791887bb7e14f", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "52eec5b914f72c4cd3f03eaedf1d38bb9a4df6de", "5d833331b0e22ff359db05c62a8bca18c4f04b68"]}, {"paper_id": "a97dc52807d80454e78d255f9fbd7b0fab56bd03", "paper_title": "Discourse-Based Objectives for Fast Unsupervised Sentence Representation Learning", "paper_year": 2017, "paper_venue": "ArXiv", "paper_citations": 35, "paper_abstract": "This work presents a novel objective function for the unsupervised training of neural network sentence encoders. It exploits signals from paragraph-level discourse coherence to train these models to understand text. Our objective is purely discriminative, allowing us to train models many times faster than was possible under prior methods, and it yields models which perform well in extrinsic evaluations.", "citations": ["bc1d609520290e0460c49b685675eb5a57fa5935"], "references": ["26e743d5bd465f49b9538deaf116c15e61b7951f", "0e6824e137847be0599bb0032e37042ed2ef5045", "687bac2d3320083eb4530bf18bb8f8f721477600", "51a55df1f023571a7e07e338ee45a3e3d66ef73e", "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "39dba6f22d72853561a4ed684be265e179a39e4f", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "47a87c2cbdd928bb081974d308b3d9cf678d257e"]}, {"paper_id": "b9de9599d7241459db9213b5cdd7059696f5ef8d", "paper_title": "Character-Level Language Modeling with Deeper Self-Attention", "paper_year": 2018, "paper_venue": "AAAI", "paper_citations": 26, "paper_abstract": "LSTMs and other RNN variants have shown strong performance on character-level language modeling. These models are typically trained using truncated backpropagation through time, and it is common to assume that their success stems from their ability to remember long-term contexts. In this paper, we show that a deep (64-layer) transformer model with fixed context outperforms RNN variants by a large margin, achieving state of the art on two popular benchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. To get good results at this depth, we show that it is important to add auxiliary losses, both at intermediate network layers and intermediate sequence positions.", "citations": [], "references": ["1827de6fa9c9c1b3d647a9d707042e89cf94abf0", "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "51a55df1f023571a7e07e338ee45a3e3d66ef73e", "bff427c18caa092afff57a400e353fde79254f22", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "23694a80bf1b9b38215be3e23068dd75296bc90f", "47a87c2cbdd928bb081974d308b3d9cf678d257e", "5d833331b0e22ff359db05c62a8bca18c4f04b68"]}], [{"paper_id": "cd62c9976534a6a2096a38244f6cbb03635a127e", "paper_title": "Phoneme recognition using time-delay neural networks", "paper_year": 1989, "paper_venue": "IEEE Trans. Acoustics, Speech, and Signal Processing", "paper_citations": 999, "paper_abstract": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5% correct while the rate obtained by the best of the HMMs was only 93.7%. >", "citations": ["a4cec122a08216fe8a3bc19b22e78fbaea096256", "1a07186bc10592f0330655519ad91652125cd907", "51a55df1f023571a7e07e338ee45a3e3d66ef73e"], "references": ["052b1d8ce63b07fec3de9dbb583772d860b7c769"]}, {"paper_id": "052b1d8ce63b07fec3de9dbb583772d860b7c769", "paper_title": "Learning representations by back-propagating errors", "paper_year": 1986, "paper_venue": "Nature", "paper_citations": 999, "paper_abstract": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.", "citations": [], "references": []}, {"paper_id": "47a87c2cbdd928bb081974d308b3d9cf678d257e", "paper_title": "Recurrent neural network based language model", "paper_year": 2010, "paper_venue": "INTERSPEECH", "paper_citations": 999, "paper_abstract": "A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5% on the much harder NIST RT05 task, even when the backoff model is trained on much more data than the RNN LM. We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity. Index Terms: language modeling, recurrent neural networks, speech recognition", "citations": ["39dba6f22d72853561a4ed684be265e179a39e4f", "c4fd9c86b2b41df51a6fe212406dda81b1997fd4", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38"], "references": ["23694a80bf1b9b38215be3e23068dd75296bc90f"]}, {"paper_id": "213d7af7107fa4921eb0adea82c9f711fd105232", "paper_title": "Reducing the dimensionality of data with neural networks.", "paper_year": 2006, "paper_venue": "Science", "paper_citations": 999, "paper_abstract": "High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such \"autoencoder\" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.", "citations": [], "references": []}, {"paper_id": "1827de6fa9c9c1b3d647a9d707042e89cf94abf0", "paper_title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "paper_year": 2015, "paper_venue": "ICML", "paper_citations": 999, "paper_abstract": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.", "citations": ["1e077413b25c4d34945cc2707e17e46ed4fe784a"], "references": []}, {"paper_id": "39dba6f22d72853561a4ed684be265e179a39e4f", "paper_title": "Sequence to Sequence Learning with Neural Networks", "paper_year": 2014, "paper_venue": "NIPS", "paper_citations": 999, "paper_abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.", "citations": ["26e743d5bd465f49b9538deaf116c15e61b7951f", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb"], "references": ["bff427c18caa092afff57a400e353fde79254f22", "052b1d8ce63b07fec3de9dbb583772d860b7c769", "23694a80bf1b9b38215be3e23068dd75296bc90f", "47a87c2cbdd928bb081974d308b3d9cf678d257e"]}, {"paper_id": "bff427c18caa092afff57a400e353fde79254f22", "paper_title": "BACKPROPAGATION THROUGH TIME: WHAT IT DOES AND HOW TO DO IT", "paper_year": 1990, "paper_venue": "", "paper_citations": 999, "paper_abstract": "", "citations": ["b9de9599d7241459db9213b5cdd7059696f5ef8d"], "references": []}, {"paper_id": "a4cec122a08216fe8a3bc19b22e78fbaea096256", "paper_title": "Deep Learning", "paper_year": 2015, "paper_venue": "Nature", "paper_citations": 999, "paper_abstract": "Machine-learning technology powers many aspects of modern society: from web searches to content filtering on social networks to recommendations on e-commerce websites, and it is increasingly present in consumer products such as cameras and smartphones. Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users\u2019 interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition or machine-learning system required careful engineering and considerable domain expertise to design a feature extractor that transformed the raw data (such as the pixel values of an image) into a suitable internal representation or feature vector from which the learning subsystem, often a classifier, could detect or classify patterns in the input. Representation learning is a set of methods that allows a machine to be fed with raw data and to automatically discover the representations needed for detection or classification. Deep-learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. With the composition of enough such transformations, very complex functions can be learned. For classification tasks, higher layers of representation amplify aspects of the input that are important for discrimination and suppress irrelevant variations. An image, for example, comes in the form of an array of pixel values, and the learned features in the first layer of representation typically represent the presence or absence of edges at particular orientations and locations in the image. The second layer typically detects motifs by spotting particular arrangements of edges, regardless of small variations in the edge positions. The third layer may assemble motifs into larger combinations that correspond to parts of familiar objects, and subsequent layers would detect objects as combinations of these parts. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. In addition to beating records in image recognition and speech recognition, it has beaten other machine-learning techniques at predicting the activity of potential drug molecules, analysing particle accelerator data, reconstructing brain circuits, and predicting the effects of mutations in non-coding DNA on gene expression and disease. Perhaps more surprisingly, deep learning has produced extremely promising results for various tasks in natural language understanding, particularly topic classification, sentiment analysis, question answering and language translation. We think that deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data. New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress.", "citations": [], "references": ["1827de6fa9c9c1b3d647a9d707042e89cf94abf0", "dac72f2c509aee67524d3321f77e97e8eff51de6", "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "14ce7635ff18318e7094417d0f92acbec6669f1c", "061356704ec86334dbbc073985375fe13cd39088", "687bac2d3320083eb4530bf18bb8f8f721477600", "38211dc39e41273c0007889202c69f841e02248a", "d4e8bed3b50a035e1eabad614fe4218a34b3b178", "cd62c9976534a6a2096a38244f6cbb03635a127e", "39dba6f22d72853561a4ed684be265e179a39e4f", "843959ffdccf31c6694d135fad07425924f785b1", "4a18360a14facea50dc819145b1daf4c53d5d59e", "0e650b5e54a3624792952899a0f79b91a1d68e79", "23694a80bf1b9b38215be3e23068dd75296bc90f", "1a07186bc10592f0330655519ad91652125cd907", "1a2a770d23b4a171fa81de62a78a3deb0588f238", "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd", "2d9e3f53fcdb548b0b3c4d4efb197f164fe0c381", "1005645c05585c2042e3410daeed638b55e2474d"]}, {"paper_id": "52eec5b914f72c4cd3f03eaedf1d38bb9a4df6de", "paper_title": "WaveNet: A Generative Model for Raw Audio", "paper_year": 2016, "paper_venue": "SSW", "paper_citations": 999, "paper_abstract": "This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.", "citations": [], "references": []}, {"paper_id": "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "paper_title": "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches", "paper_year": 2014, "paper_venue": "SSST@EMNLP", "paper_citations": 999, "paper_abstract": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder\u2010Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.", "citations": ["0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "3febb2bed8865945e7fddc99efd791887bb7e14f", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "a4cec122a08216fe8a3bc19b22e78fbaea096256"], "references": ["39dba6f22d72853561a4ed684be265e179a39e4f"]}, {"paper_id": "f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "paper_title": "Recurrent Neural Network Regularization", "paper_year": 2014, "paper_venue": "ArXiv", "paper_citations": 999, "paper_abstract": "We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.", "citations": ["b9de9599d7241459db9213b5cdd7059696f5ef8d", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "421fc2556836a6b441de806d7b393a35b6eaea58"], "references": []}, {"paper_id": "d0e51833b3db3af1c762ab723efd08f117a497c8", "paper_title": "Improved Training of Wasserstein GANs", "paper_year": 2017, "paper_venue": "NIPS", "paper_citations": 999, "paper_abstract": "Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only poor samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models with continuous generators. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.", "citations": ["7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d"], "references": ["5d833331b0e22ff359db05c62a8bca18c4f04b68"]}, {"paper_id": "51a55df1f023571a7e07e338ee45a3e3d66ef73e", "paper_title": "Character-level Convolutional Networks for Text Classification", "paper_year": 2015, "paper_venue": "NIPS", "paper_citations": 999, "paper_abstract": "This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.", "citations": ["1e077413b25c4d34945cc2707e17e46ed4fe784a", "a97dc52807d80454e78d255f9fbd7b0fab56bd03", "b9de9599d7241459db9213b5cdd7059696f5ef8d", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "8c1b00128e74f1cd92aede3959690615695d5101"], "references": ["cd62c9976534a6a2096a38244f6cbb03635a127e", "052b1d8ce63b07fec3de9dbb583772d860b7c769", "87f40e6f3022adbc1f1905e3e506abad05a9964f"]}, {"paper_id": "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd", "paper_title": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion", "paper_year": 2010, "paper_venue": "J. Mach. Learn. Res.", "paper_citations": 999, "paper_abstract": "We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.", "citations": ["26e743d5bd465f49b9538deaf116c15e61b7951f", "a4cec122a08216fe8a3bc19b22e78fbaea096256"], "references": ["213d7af7107fa4921eb0adea82c9f711fd105232", "843959ffdccf31c6694d135fad07425924f785b1"]}, {"paper_id": "2d9e3f53fcdb548b0b3c4d4efb197f164fe0c381", "paper_title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling", "paper_year": 2014, "paper_venue": "ArXiv", "paper_citations": 999, "paper_abstract": "In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.", "citations": [], "references": ["39dba6f22d72853561a4ed684be265e179a39e4f", "1eb09fecd75eb27825dce4f964b97f4f5cc399d7"]}, {"paper_id": "cbd1ade5b869b13d1853aa0753b82fb35c26bcba", "paper_title": "Neural networks and physical systems with emergent collective computational abilities", "paper_year": 1988, "paper_venue": "", "paper_citations": 999, "paper_abstract": "Computational properties of use to biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.", "citations": [], "references": []}]], [[{"paper_id": "05dd7254b632376973f3a1b4d39485da17814df5", "paper_title": "SQuAD: 100, 000+ Questions for Machine Comprehension of Text", "paper_year": 2016, "paper_venue": "EMNLP", "paper_citations": 926, "paper_abstract": "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. \nThe dataset is freely available at this https URL", "citations": ["8c1b00128e74f1cd92aede3959690615695d5101", "3c78c6df5eb1695b6a399e346dde880af27d1016", "27e98e09cf09bc13c913d01676e5f32624011050", "26b47e35fe6e4260fdf7b7cc98f279a73c277494", "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "3febb2bed8865945e7fddc99efd791887bb7e14f", "007ab5528b3bd310a80d553cccad4b78dc496b02", "93b8da28d006415866bf48f9a6e06b5242129195"], "references": ["38211dc39e41273c0007889202c69f841e02248a"]}, {"paper_id": "766ce989b8b8b984f7a4691fd8c9af4bdb2b74cd", "paper_title": "\"Cloze procedure\": a new tool for measuring readability.", "paper_year": 1953, "paper_venue": "", "paper_citations": 698, "paper_abstract": "Here is the first comprehensive statement of a research method and its theory which were introduced briefly during a workshop at the 1953 AEJ convention. Included are findings from three pilot studies and two experiments in which \u201ccloze procedure\u201d results are compared with those of two readability formulas.", "citations": [], "references": []}, {"paper_id": "007ab5528b3bd310a80d553cccad4b78dc496b02", "paper_title": "Bidirectional Attention Flow for Machine Comprehension", "paper_year": 2016, "paper_venue": "ICLR", "paper_citations": 611, "paper_abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "citations": ["8c1b00128e74f1cd92aede3959690615695d5101", "3c78c6df5eb1695b6a399e346dde880af27d1016", "27e98e09cf09bc13c913d01676e5f32624011050", "26b47e35fe6e4260fdf7b7cc98f279a73c277494", "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "e0222a1ae6874f7fff128c3da8769ab95963da04", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "3febb2bed8865945e7fddc99efd791887bb7e14f", "93b8da28d006415866bf48f9a6e06b5242129195"], "references": ["05dd7254b632376973f3a1b4d39485da17814df5", "f37e1b62a767a307c046404ca96bc140b3e68cb5"]}, {"paper_id": "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "paper_title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension", "paper_year": 2017, "paper_venue": "ACL", "paper_citations": 214, "paper_abstract": "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study. Data and code available at -- this http URL", "citations": ["3c78c6df5eb1695b6a399e346dde880af27d1016", "8c1b00128e74f1cd92aede3959690615695d5101", "26b47e35fe6e4260fdf7b7cc98f279a73c277494"], "references": ["007ab5528b3bd310a80d553cccad4b78dc496b02", "05dd7254b632376973f3a1b4d39485da17814df5"]}, {"paper_id": "8c1b00128e74f1cd92aede3959690615695d5101", "paper_title": "QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension", "paper_year": 2018, "paper_venue": "ICLR", "paper_citations": 164, "paper_abstract": "Current end-to-end machine reading and question answering (Q\\&A) models are primarily based on recurrent neural networks (RNNs) with attention. Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs. We propose a new Q\\&A model that does not require recurrent networks: It consists exclusively of attention and convolutions, yet achieves equivalent or better performance than existing models. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference. The speed-up gain allows us to train the model with much more data. We hence combine our model with data generated by backtranslation from a neural machine translation model. This data augmentation technique not only enhances the training examples but also diversifies the phrasing of the sentences, which results in immediate accuracy improvements. Our single model achieves 84.6 F1 score on the test set, which is significantly better than the best published F1 score of 81.8.", "citations": ["26b47e35fe6e4260fdf7b7cc98f279a73c277494"], "references": ["f37e1b62a767a307c046404ca96bc140b3e68cb5", "51a55df1f023571a7e07e338ee45a3e3d66ef73e", "3c78c6df5eb1695b6a399e346dde880af27d1016", "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "2d9e3f53fcdb548b0b3c4d4efb197f164fe0c381", "05dd7254b632376973f3a1b4d39485da17814df5", "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "007ab5528b3bd310a80d553cccad4b78dc496b02"]}, {"paper_id": "3c78c6df5eb1695b6a399e346dde880af27d1016", "paper_title": "Simple and Effective Multi-Paragraph Reading Comprehension", "paper_year": 2017, "paper_venue": "ACL", "paper_citations": 96, "paper_abstract": "We consider the problem of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Our proposed solution trains models to produce well calibrated confidence scores for their results on individual paragraphs. We sample multiple paragraphs from the documents during training, and use a shared-normalization training objective that encourages the model to produce globally correct output. We combine this method with a state-of-the-art pipeline for training models on document QA data. Experiments demonstrate strong performance on several document QA datasets. Overall, we are able to achieve a score of 71.3 F1 on the web portion of TriviaQA, a large improvement from the 56.7 F1 of the previous best system.", "citations": ["3febb2bed8865945e7fddc99efd791887bb7e14f", "8c1b00128e74f1cd92aede3959690615695d5101", "27e98e09cf09bc13c913d01676e5f32624011050", "26b47e35fe6e4260fdf7b7cc98f279a73c277494"], "references": ["007ab5528b3bd310a80d553cccad4b78dc496b02", "05dd7254b632376973f3a1b4d39485da17814df5", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "f010affab57b5fcf1cd6be23df79d8ec98c7289c"]}, {"paper_id": "e0222a1ae6874f7fff128c3da8769ab95963da04", "paper_title": "Reinforced Mnemonic Reader for Machine Reading Comprehension", "paper_year": 2017, "paper_venue": "IJCAI", "paper_citations": 46, "paper_abstract": "In this paper, we introduce the Reinforced Mnemonic Reader for machine reading comprehension tasks, which enhances previous attentive readers in two aspects. First, a reattention mechanism is proposed to refine current attentions by directly accessing to past attentions that are temporally memorized in a multi-round alignment architecture, so as to avoid the problems of attention redundancy and attention deficiency. Second, a new optimization approach, called dynamic-critical reinforcement learning, is introduced to extend the standard supervised method. It always encourages to predict a more acceptable answer so as to address the convergence suppression problem occurred in traditional reinforcement learning algorithms. Extensive experiments on the Stanford Question Answering Dataset (SQuAD) show that our model achieves state-of-the-art results. Meanwhile, our model outperforms previous systems by over 6% in terms of both Exact Match and F1 metrics on two adversarial SQuAD datasets.", "citations": ["27e98e09cf09bc13c913d01676e5f32624011050"], "references": ["3febb2bed8865945e7fddc99efd791887bb7e14f", "007ab5528b3bd310a80d553cccad4b78dc496b02", "f37e1b62a767a307c046404ca96bc140b3e68cb5"]}, {"paper_id": "26b47e35fe6e4260fdf7b7cc98f279a73c277494", "paper_title": "Multi-granularity hierarchical attention fusion networks for reading comprehension and question answering", "paper_year": 2018, "paper_venue": "ACL", "paper_citations": 35, "paper_abstract": "This paper describes a novel hierarchical attention network for reading comprehension style question answering, which aims to answer questions for a given narrative paragraph. In the proposed method, attention and fusion are conducted horizontally and vertically across layers at different levels of granularity between question and paragraph. Specifically, it first encode the question and paragraph with fine-grained language embeddings, to better capture the respective representations at semantic level. Then it proposes a multi-granularity fusion approach to fully fuse information from both global and attended representations. Finally, it introduces a hierarchical attention network to focuses on the answer span progressively with multi-level softalignment. Extensive experiments on the large-scale SQuAD and TriviaQA datasets validate the effectiveness of the proposed method. At the time of writing the paper (Jan. 12th 2018), our model achieves the first position on the SQuAD leaderboard for both single and ensemble models. We also achieves state-of-the-art results on TriviaQA, AddSent and AddOne-Sent datasets.", "citations": ["27e98e09cf09bc13c913d01676e5f32624011050"], "references": ["f37e1b62a767a307c046404ca96bc140b3e68cb5", "3c78c6df5eb1695b6a399e346dde880af27d1016", "3febb2bed8865945e7fddc99efd791887bb7e14f", "f010affab57b5fcf1cd6be23df79d8ec98c7289c", "05dd7254b632376973f3a1b4d39485da17814df5", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "8c1b00128e74f1cd92aede3959690615695d5101", "007ab5528b3bd310a80d553cccad4b78dc496b02"]}, {"paper_id": "27e98e09cf09bc13c913d01676e5f32624011050", "paper_title": "U-Net: Machine Reading Comprehension with Unanswerable Questions", "paper_year": 2018, "paper_venue": "ArXiv", "paper_citations": 8, "paper_abstract": "Machine reading comprehension with unanswerable questions is a new challenging task for natural language processing. A key subtask is to reliably predict whether the question is unanswerable. In this paper, we propose a unified model, called U-Net, with three important components: answer pointer, no-answer pointer, and answer verifier. We introduce a universal node and thus process the question and its context passage as a single contiguous sequence of tokens. The universal node encodes the fused information from both the question and passage, and plays an important role to predict whether the question is answerable and also greatly improves the conciseness of the U-Net. Different from the state-of-art pipeline models, U-Net can be learned in an end-to-end fashion. The experimental results on the SQuAD 2.0 dataset show that U-Net can effectively predict the unanswerability of questions and achieves an F1 score of 71.7 on SQuAD 2.0.", "citations": [], "references": ["f37e1b62a767a307c046404ca96bc140b3e68cb5", "3c78c6df5eb1695b6a399e346dde880af27d1016", "26b47e35fe6e4260fdf7b7cc98f279a73c277494", "e0222a1ae6874f7fff128c3da8769ab95963da04", "05dd7254b632376973f3a1b4d39485da17814df5", "3febb2bed8865945e7fddc99efd791887bb7e14f", "007ab5528b3bd310a80d553cccad4b78dc496b02"]}], []], [[{"paper_id": "9fa8d73e572c3ca824a04a5f551b602a17831bc5", "paper_title": "Domain Adaptation with Structural Correspondence Learning", "paper_year": 2006, "paper_venue": "EMNLP", "paper_citations": 925, "paper_abstract": "Discriminative learning methods are widely used in natural language processing. These methods work best when their training and test data are drawn from the same distribution. For many NLP tasks, however, we are confronted with new domains in which labeled data is scarce or non-existent. In such cases, we seek to adapt existing models from a resource-rich source domain to a resource-poor target domain. We introduce structural correspondence learning to automatically induce correspondences among features from different domains. We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger.", "citations": ["5ded2b8c64491b4a67f6d39ce473d4b9347a672e"], "references": ["783480acff435bfbc15ffcdb4f15eccddaa0c810", "2c5135a0531bc5ad7dd890f018e67a40529f5bcb"]}, {"paper_id": "2c5135a0531bc5ad7dd890f018e67a40529f5bcb", "paper_title": "A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data", "paper_year": 2005, "paper_venue": "J. Mach. Learn. Res.", "paper_citations": 822, "paper_abstract": "One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don't have a complete understanding of their effectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Specifically we consider learning predictive structures on hypothesis spaces (that is, what kind of classifiers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi-supervised learning setting.", "citations": ["1a07186bc10592f0330655519ad91652125cd907", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "9fa8d73e572c3ca824a04a5f551b602a17831bc5"], "references": ["15b2c44b3868a1055850846161aaca59083e0529", "02485a373142312c354b79552b3d326913eaf86d", "c7788fe99735eceff2bcc37401fc02d2825f739a", "45ee7447b9dd406496c4a5d9d8fb6556366a01c6", "5211c32fb5849a14855a91a7ab16cfb83483cc1d"]}, {"paper_id": "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb", "paper_title": "Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition", "paper_year": 2003, "paper_venue": "CoNLL", "paper_citations": 789, "paper_abstract": "We describe the CoNLL-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance.", "citations": ["0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "3febb2bed8865945e7fddc99efd791887bb7e14f", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e"], "references": []}, {"paper_id": "128cb6b891aee1b5df099acb48e2efecfcff689f", "paper_title": "The Winograd Schema Challenge", "paper_year": 2011, "paper_venue": "AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning", "paper_citations": 198, "paper_abstract": "In this paper, we present an alternative to the Turing Test that has some conceptual and practical advantages. Like the original, it involves responding to typed English sentences, and English-speaking adults will have no difficulty with it. Unlike the original, the subject is not required to engage in a conversation and fool an interrogator into believing she is dealing with a person. Moreover, the test is arranged in such a way that having full access to a large corpus of English text might not help much. Finally, the interrogator or a third party will be able to decide unambiguously after a few minutes whether or not a subject has passed the test.", "citations": ["93b8da28d006415866bf48f9a6e06b5242129195"], "references": ["f28cd8803a0d453d389cdc270923231cbf4ebafc"]}, {"paper_id": "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "paper_title": "Semi-supervised sequence tagging with bidirectional language models", "paper_year": 2017, "paper_venue": "ACL", "paper_citations": 145, "paper_abstract": "Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks. However, in most cases, the recurrent network that operates on word-level representations to produce context sensitive representations is trained on relatively little labeled data. In this paper, we demonstrate a general semi-supervised approach for adding pre- trained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks. We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers.", "citations": ["1e077413b25c4d34945cc2707e17e46ed4fe784a", "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "3febb2bed8865945e7fddc99efd791887bb7e14f", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "421fc2556836a6b441de806d7b393a35b6eaea58"], "references": ["26e743d5bd465f49b9538deaf116c15e61b7951f", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "1510cf4b8abea80b9f352325ca4c132887de21a0", "1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "c7788fe99735eceff2bcc37401fc02d2825f739a", "59761abc736397539bdd01ad7f9d91c8607c0457", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb", "23694a80bf1b9b38215be3e23068dd75296bc90f", "47a87c2cbdd928bb081974d308b3d9cf678d257e", "5d833331b0e22ff359db05c62a8bca18c4f04b68"]}, {"paper_id": "421fc2556836a6b441de806d7b393a35b6eaea58", "paper_title": "Contextual String Embeddings for Sequence Labeling", "paper_year": 2018, "paper_venue": "COLING", "paper_citations": 85, "paper_abstract": "Recent advances in language modeling using recurrent neural networks have made it viable to model language as distributions over characters. By learning to predict the next character on the basis of previous characters, such models have been shown to automatically internalize linguistic concepts such as words, sentences, subclauses and even sentiment. In this paper, we propose to leverage the internal states of a trained character language model to produce a novel type of word embedding which we refer to as contextual string embeddings. Our proposed embeddings have the distinct properties that they (a) are trained without any explicit notion of words and thus fundamentally model words as sequences of characters, and (b) are contextualized by their surrounding text, meaning that the same word will have different embeddings depending on its contextual use. We conduct a comparative evaluation against previous embeddings and find that our embeddings are highly useful for downstream tasks: across four classic sequence labeling tasks we consistently outperform the previous state-of-the-art. In particular, we significantly outperform previous work on English and German named entity recognition (NER), allowing us to report new state-of-the-art F1-scores on the CONLL03 shared task. We release all code and pre-trained language models in a simple-to-use framework to the research community, to enable reproduction of these experiments and application of our proposed embeddings to other tasks: https://github.com/zalandoresearch/flair", "citations": [], "references": ["f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "39dba6f22d72853561a4ed684be265e179a39e4f", "3febb2bed8865945e7fddc99efd791887bb7e14f", "5d833331b0e22ff359db05c62a8bca18c4f04b68"]}, {"paper_id": "cb0f3ee1e98faf92429d601cdcd76c69c1e484eb", "paper_title": "Neural Network Acceptability Judgments", "paper_year": 2018, "paper_venue": "ArXiv", "paper_citations": 34, "paper_abstract": "In this work, we explore the ability of artificial neural networks to judge the grammatical acceptability of a sentence. Machine learning research of this kind is well placed to answer important open questions about the role of prior linguistic bias in language acquisition by providing a test for the Poverty of the Stimulus Argument. In service of this goal, we introduce the Corpus of Linguistic Acceptability (CoLA), a set of 10,657 English sentences labeled as grammatical or ungrammatical by expert linguists. We train several recurrent neural networks to do binary acceptability classification. These models set a baseline for the task. Error-analysis testing the models on specific grammatical phenomena reveals that they learn some systematic grammatical generalizations like subject-verb-object word order without any grammatical supervision. We find that neural sequence models show promise on the acceptability classification task. However, human-like performance across a wide range of grammatical constructions remains far off.", "citations": ["93b8da28d006415866bf48f9a6e06b5242129195"], "references": ["f37e1b62a767a307c046404ca96bc140b3e68cb5", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "486af640c427afba9036799cbe2bc41774a4d6c2", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "39dba6f22d72853561a4ed684be265e179a39e4f", "a4cec122a08216fe8a3bc19b22e78fbaea096256", "3febb2bed8865945e7fddc99efd791887bb7e14f", "93b8da28d006415866bf48f9a6e06b5242129195"]}, {"paper_id": "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "paper_title": "Semi-Supervised Sequence Modeling with Cross-View Training", "paper_year": 2018, "paper_venue": "EMNLP", "paper_citations": 32, "paper_abstract": "Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models, mainly because they can take advantage of large amounts of unlabeled text. However, the supervised models only learn from task-specific labeled data during the main training phase. We therefore propose Cross-View Training (CVT), a semi-supervised learning algorithm that improves the representations of a Bi-LSTM sentence encoder using a mix of labeled and unlabeled data. On labeled examples, standard supervised learning is used. On unlabeled examples, CVT teaches auxiliary prediction modules that see restricted views of the input (e.g., only part of a sentence) to match the predictions of the full model seeing the whole input. Since the auxiliary modules and the full model share intermediate representations, this in turn improves the full model. Moreover, we show that CVT is particularly effective when combined with multi-task learning. We evaluate CVT on five sequence tagging tasks, machine translation, and dependency parsing, achieving state-of-the-art results.", "citations": [], "references": ["26e743d5bd465f49b9538deaf116c15e61b7951f", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "1a07186bc10592f0330655519ad91652125cd907", "1e077413b25c4d34945cc2707e17e46ed4fe784a", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "39dba6f22d72853561a4ed684be265e179a39e4f", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb", "3febb2bed8865945e7fddc99efd791887bb7e14f", "5d833331b0e22ff359db05c62a8bca18c4f04b68", "5211c32fb5849a14855a91a7ab16cfb83483cc1d"]}], [{"paper_id": "56623a496727d5c71491850e04512ddf4152b487", "paper_title": "1974] \\Beyond regression: New tools for predicting and analysis in the behavioral sciences", "paper_year": 1974, "paper_venue": "", "paper_citations": 999, "paper_abstract": "", "citations": [], "references": []}, {"paper_id": "45ee7447b9dd406496c4a5d9d8fb6556366a01c6", "paper_title": "Weak Convergence and Empirical Processes", "paper_year": 1996, "paper_venue": "", "paper_citations": 999, "paper_abstract": "", "citations": [], "references": []}, {"paper_id": "4de39c94e340a108fff01a90a67b0c17c86fb981", "paper_title": "Fast training of support vector machines using sequential minimal optimization", "paper_year": 1999, "paper_venue": "", "paper_citations": 999, "paper_abstract": "This chapter describes a new algorithm for training Support Vector Machines: Sequential Minimal Optimization, or SMO. Training a Support Vector Machine (SVM) requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which avoids using a time-consuming numerical QP optimization as an inner loop. The amount of memory required for SMO is linear in the training set size, which allows SMO to handle very large training sets. Because large matrix computation is avoided, SMO scales somewhere between linear and quadratic in the training set size for various test problems, while a standard projected conjugate gradient (PCG) chunking algorithm scales somewhere between linear and cubic in the training set size. SMO's computation time is dominated by SVM evaluation, hence SMO is fastest for linear SVMs and sparse data sets. For the MNIST database, SMO is as fast as PCG chunking; while for the UCI Adult database and linear SVMs, SMO can be more than 1000 times faster than the PCG chunking algorithm.", "citations": [], "references": []}, {"paper_id": "486af640c427afba9036799cbe2bc41774a4d6c2", "paper_title": "Constraints on variables in syntax", "paper_year": 1967, "paper_venue": "", "paper_citations": 999, "paper_abstract": "Massachusetts Institute of Technology. Dept. of Modern Languages and Linguistics. Thesis. 1967. Ph.D.", "citations": [], "references": []}, {"paper_id": "b25663fa149be5286de193c13324098aedd7e2cc", "paper_title": "Opinion Mining and Sentiment Analysis", "paper_year": 2007, "paper_venue": "Foundations and Trends in Information Retrieval", "paper_citations": 999, "paper_abstract": "An important part of our information-gathering behavior has always been to find out what other people think. With the growing availability and popularity of opinion-rich resources such as online review sites and personal blogs, new opportunities and challenges arise as people now can, and do, actively use information technologies to seek out and understand the opinions of others. The sudden eruption of activity in the area of opinion mining and sentiment analysis, which deals with the computational treatment of opinion, sentiment, and subjectivity in text, has thus occurred at least in part as a direct response to the surge of interest in new systems that deal directly with opinions as a first-class object. \n \nThis survey covers techniques and approaches that promise to directly enable opinion-oriented information-seeking systems. Our focus is on methods that seek to address the new challenges raised by sentiment-aware applications, as compared to those that are already present in more traditional fact-based analysis. We include material on summarization of evaluative text and on broader issues regarding privacy, manipulation, and economic impact that the development of opinion-oriented information-access services gives rise to. To facilitate future work, a discussion of available resources, benchmark datasets, and evaluation campaigns is also provided.", "citations": [], "references": []}, {"paper_id": "f28cd8803a0d453d389cdc270923231cbf4ebafc", "paper_title": "Computing Machinery and Intelligence", "paper_year": 1950, "paper_venue": "", "paper_citations": 999, "paper_abstract": "I propose to consider the question, \u201cCan machines think?\u201d\u2663 This should begin with definitions of the meaning of the terms \u201cmachine\u201d and \u201cthink\u201d. The definitions might be framed so as to reflect so far as possible the normal use of the words, but this attitude is dangerous. If the meaning of the words \u201cmachine\u201d and \u201cthink\u201d are to be found by examining how they are commonly used it is difficult to escape the conclusion that the meaning and the answer to the question, \u201cCan machines think?\u201d is to be sought in a statistical survey such as a Gallup poll.", "citations": [], "references": []}, {"paper_id": "15b2c44b3868a1055850846161aaca59083e0529", "paper_title": "Learning with Local and Global Consistency", "paper_year": 2003, "paper_venue": "NIPS", "paper_citations": 999, "paper_abstract": "We consider the general problem of learning from labeled and unlabeled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufficiently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classification problems and demonstrates effective use of unlabeled data.", "citations": ["2c5135a0531bc5ad7dd890f018e67a40529f5bcb"], "references": ["02485a373142312c354b79552b3d326913eaf86d"]}, {"paper_id": "5211c32fb5849a14855a91a7ab16cfb83483cc1d", "paper_title": "Unsupervised Word Sense Disambiguation Rivaling Supervised Methods", "paper_year": 1995, "paper_venue": "ACL", "paper_citations": 999, "paper_abstract": "This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints---that words tend to have one sense per discourse and one sense per collocation---exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%.", "citations": ["2c5135a0531bc5ad7dd890f018e67a40529f5bcb"], "references": []}, {"paper_id": "c7788fe99735eceff2bcc37401fc02d2825f739a", "paper_title": "Text Classification from Labeled and Unlabeled Documents using EM", "paper_year": 2000, "paper_venue": "Machine Learning", "paper_citations": 999, "paper_abstract": "This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available.We introduce an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates to convergence. This basic EM procedure works well when the data conform to the generative assumptions of the model. However these assumptions are often violated in practice, and poor performance can result. We present two extensions to the algorithm that improve classification accuracy under these conditions: (1) a weighting factor to modulate the contribution of the unlabeled data, and (2) the use of multiple mixture components per class. Experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 30%.", "citations": ["2c5135a0531bc5ad7dd890f018e67a40529f5bcb"], "references": []}, {"paper_id": "4a18360a14facea50dc819145b1daf4c53d5d59e", "paper_title": "Estimation of Dependences Based on Empirical Data", "paper_year": 2006, "paper_venue": "", "paper_citations": 999, "paper_abstract": "Realism and Instrumentalism: Classical Statistics and VC Theory (1960-1980).- Falsifiability and Parsimony: VC Dimension and the Number of Entities (1980-2000).- Noninductive Methods of Inference: Direct Inference Instead of Generalization (2000-...).- The Big Picture.", "citations": ["a4cec122a08216fe8a3bc19b22e78fbaea096256", "1e077413b25c4d34945cc2707e17e46ed4fe784a"], "references": []}, {"paper_id": "02485a373142312c354b79552b3d326913eaf86d", "paper_title": "Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions", "paper_year": 2003, "paper_venue": "ICML", "paper_citations": 999, "paper_abstract": "Active and semi-supervised learning are important techniques when labeled data are scarce. We combine the two under a Gaussian random field model. Labeled and unlabeled data are represented as vertices in a weighted graph, with edge weights encoding the similarity between instances. The semi-supervised learning problem is then formulated in terms of a Gaussian random field on this graph, the mean of which is characterized in terms of harmonic functions. Active learning is performed on top of the semisupervised learning scheme by greedily selecting queries from the unlabeled data to minimize the estimated expected classification error (risk); in the case of Gaussian fields the risk is efficiently computed using matrix methods. We present experimental results on synthetic data, handwritten digit recognition, and text classification tasks. The active learning scheme requires a much smaller number of queries to achieve high accuracy compared with random query selection.", "citations": ["2c5135a0531bc5ad7dd890f018e67a40529f5bcb"], "references": []}, {"paper_id": "df01d2dede1a243b9b0eb26c27246bc13705d930", "paper_title": "Exploiting Generative Models in Discriminative Classifiers", "paper_year": 1998, "paper_venue": "NIPS", "paper_citations": 999, "paper_abstract": "Generative probability models such as hidden Markov models provide a principled way of treating missing information and dealing with variable length sequences. On the other hand, discriminative methods such as support vector machines enable us to construct flexible decision boundaries and often result in classification performance superior to that of the model based approaches. An ideal classifier should combine these two complementary approaches. In this paper, we develop a natural way of achieving this combination by deriving kernel functions for use in discriminative methods such as support vector machines from generative probability models. We provide a theoretical justification for this combination as well as demonstrate a substantial improvement in the classification performance in the context of DNA and protein sequence analysis.", "citations": [], "references": []}]], [[{"paper_id": "38211dc39e41273c0007889202c69f841e02248a", "paper_title": "ImageNet: A large-scale hierarchical image database", "paper_year": 2009, "paper_venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition", "paper_citations": 999, "paper_abstract": "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \u201cImageNet\u201d, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.", "citations": ["a4cec122a08216fe8a3bc19b22e78fbaea096256"], "references": ["54d2b5c64a67f65c5dd812b89e07973f97699552", "5a5effa909cdeafaddbbb7855037e02f8e25d632", "0e650b5e54a3624792952899a0f79b91a1d68e79", "370b5757a5379b15e30d619e4d3fb9e8e13f3256", "6cf8ec34a008031b018c8a3a4640a87f476d0925", "b391878646123f5490ef2e2103de09a0947e4dc9"]}, {"paper_id": "081651b38ff7533550a3adfc1c00da333a8fe86c", "paper_title": "How transferable are features in deep neural networks?", "paper_year": 2014, "paper_venue": "NIPS", "paper_citations": 997, "paper_abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.", "citations": ["1e077413b25c4d34945cc2707e17e46ed4fe784a"], "references": ["38211dc39e41273c0007889202c69f841e02248a", "1a2a770d23b4a171fa81de62a78a3deb0588f238", "2f4df08d9072fc2ac181b7fced6a245315ce05c8"]}, {"paper_id": "0e6824e137847be0599bb0032e37042ed2ef5045", "paper_title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books", "paper_year": 2015, "paper_venue": "2015 IEEE International Conference on Computer Vision (ICCV)", "paper_citations": 287, "paper_abstract": "Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in the current datasets. To align movies and books we propose a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.", "citations": ["a97dc52807d80454e78d255f9fbd7b0fab56bd03", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "af5c4b80fbf847f69a202ba5a780a3dd18c1a027"], "references": ["39dba6f22d72853561a4ed684be265e179a39e4f", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "2d9e3f53fcdb548b0b3c4d4efb197f164fe0c381"]}, {"paper_id": "af5c4b80fbf847f69a202ba5a780a3dd18c1a027", "paper_title": "SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference", "paper_year": 2018, "paper_venue": "EMNLP", "paper_citations": 64, "paper_abstract": "Given a partial description like \"she opened the hood of the car,\" humans can reason about the situation and anticipate what might come next (\"then, she examined the engine\"). In this paper, we introduce the task of grounded commonsense inference, unifying natural language inference and commonsense reasoning. \nWe present SWAG, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations. To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data. To account for the aggressive adversarial filtering, we use state-of-the-art language models to massively oversample a diverse set of potential counterfactuals. Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88%), various competitive models struggle on our task. We provide comprehensive analysis that indicates significant opportunities for future research.", "citations": [], "references": ["54aaf902fcd112d91783bf35861cd9137b342e4e", "0e6824e137847be0599bb0032e37042ed2ef5045", "f37e1b62a767a307c046404ca96bc140b3e68cb5", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "3febb2bed8865945e7fddc99efd791887bb7e14f", "5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "93b8da28d006415866bf48f9a6e06b5242129195"]}], [{"paper_id": "446fbff6a2a7c9989b0a0465f960e236d9a5e886", "paper_title": "Context Encoders: Feature Learning by Inpainting", "paper_year": 2016, "paper_venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "paper_citations": 999, "paper_abstract": "We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders - a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.", "citations": ["bc1d609520290e0460c49b685675eb5a57fa5935"], "references": ["1a07186bc10592f0330655519ad91652125cd907", "1a2a770d23b4a171fa81de62a78a3deb0588f238", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "843959ffdccf31c6694d135fad07425924f785b1", "2f4df08d9072fc2ac181b7fced6a245315ce05c8", "213d7af7107fa4921eb0adea82c9f711fd105232"]}, {"paper_id": "370b5757a5379b15e30d619e4d3fb9e8e13f3256", "paper_title": "Labeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments", "paper_year": 2008, "paper_venue": "", "paper_citations": 999, "paper_abstract": "Most face databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, background, camera quality, and gender. While there are many applications for face recognition technology in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database, Labeled Faces in the Wild, is provided as an aid in studying the latter, unconstrained, recognition problem. The database contains labeled face photographs spanning the range of conditions typically encountered in everyday life. The database exhibits \u201cnatural\u201d variability in factors such as pose, lighting, race, accessories, occlusions, and background. In addition to describing the details of the database, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible. We provide baseline results, including results of a state of the art face recognition system combined with a face alignment system. To facilitate experimentation on the database, we provide several parallel databases, including an aligned version.", "citations": [], "references": []}, {"paper_id": "2f4df08d9072fc2ac181b7fced6a245315ce05c8", "paper_title": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation", "paper_year": 2013, "paper_venue": "2014 IEEE Conference on Computer Vision and Pattern Recognition", "paper_citations": 999, "paper_abstract": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.", "citations": [], "references": ["38211dc39e41273c0007889202c69f841e02248a"]}, {"paper_id": "061356704ec86334dbbc073985375fe13cd39088", "paper_title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "paper_year": 2014, "paper_venue": "ICLR", "paper_citations": 999, "paper_abstract": "Abstract: In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.", "citations": [], "references": ["38211dc39e41273c0007889202c69f841e02248a", "5a5effa909cdeafaddbbb7855037e02f8e25d632"]}, {"paper_id": "54aaf902fcd112d91783bf35861cd9137b342e4e", "paper_title": "The Ecological Approach to Visual Perception", "paper_year": 1989, "paper_venue": "", "paper_citations": 999, "paper_abstract": "", "citations": [], "references": []}, {"paper_id": "0e650b5e54a3624792952899a0f79b91a1d68e79", "paper_title": "One-shot learning of object categories", "paper_year": 2006, "paper_venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "paper_citations": 999, "paper_abstract": "Learning visual models of object categories notoriously requires hundreds or thousands of training examples. We show that it is possible to learn much information about a category from just one, or a handful, of images. The key insight is that, rather than learning from scratch, one can take advantage of knowledge coming from previously learned categories, no matter how different these categories might be. We explore a Bayesian implementation of this idea. Object categories are represented by probabilistic models. Prior knowledge is represented as a probability density function on the parameters of these models. The posterior model for an object category is obtained by updating the prior in the light of one or more observations. We test a simple implementation of our algorithm on a database of 101 diverse object categories. We compare category models learned by an implementation of our Bayesian approach to models learned from by maximum likelihood (ML) and maximum a posteriori (MAP) methods. We find that on a database of more than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other methods to operate successfully.", "citations": ["a4cec122a08216fe8a3bc19b22e78fbaea096256", "38211dc39e41273c0007889202c69f841e02248a", "1a2a770d23b4a171fa81de62a78a3deb0588f238"], "references": []}, {"paper_id": "6cf8ec34a008031b018c8a3a4640a87f476d0925", "paper_title": "Labeling images with a computer game", "paper_year": 2004, "paper_venue": "CHI", "paper_citations": 999, "paper_abstract": "We introduce a new interactive system: a game that is fun and can be used to create valuable output. When people play the game they help determine the contents of images by providing meaningful labels for them. If the game is played as much as popular online games, we estimate that most images on the Web can be labeled in a few months. Having proper labels associated with each image on the Web would allow for more accurate image search, improve the accessibility of sites (by providing descriptions of images to visually impaired individuals), and help users block inappropriate images. Our system makes a significant contribution because of its valuable output and because of the way it addresses the image-labeling problem. Rather than using computer vision techniques, which don't work well enough, we encourage people to do the work by taking advantage of their desire to be entertained.", "citations": [], "references": []}, {"paper_id": "b391878646123f5490ef2e2103de09a0947e4dc9", "paper_title": "Scalable Recognition with a Vocabulary Tree", "paper_year": 2006, "paper_venue": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)", "paper_citations": 999, "paper_abstract": "A recognition scheme that scales efficiently to a large number of objects is presented. The efficiency and quality is exhibited in a live demonstration that recognizes CD-covers from a database of 40000 images of popular music CD\u2019s. The scheme builds upon popular techniques of indexing descriptors extracted from local regions, and is robust to background clutter and occlusion. The local region descriptors are hierarchically quantized in a vocabulary tree. The vocabulary tree allows a larger and more discriminatory vocabulary to be used efficiently, which we show experimentally leads to a dramatic improvement in retrieval quality. The most significant property of the scheme is that the tree directly defines the quantization. The quantization and the indexing are therefore fully integrated, essentially being one and the same. The recognition quality is evaluated through retrieval on a database with ground truth, showing the power of the vocabulary tree approach, going as high as 1 million images.", "citations": ["38211dc39e41273c0007889202c69f841e02248a"], "references": []}, {"paper_id": "0626908dd710b91aece1a81f4ca0635f23fc47f3", "paper_title": "Rethinking the Inception Architecture for Computer Vision", "paper_year": 2015, "paper_venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "paper_citations": 999, "paper_abstract": "Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.", "citations": [], "references": ["061356704ec86334dbbc073985375fe13cd39088", "2f4df08d9072fc2ac181b7fced6a245315ce05c8", "1827de6fa9c9c1b3d647a9d707042e89cf94abf0"]}, {"paper_id": "54d2b5c64a67f65c5dd812b89e07973f97699552", "paper_title": "80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition", "paper_year": 2008, "paper_venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "paper_citations": 999, "paper_abstract": "With the advent of the Internet, billions of images are now freely available online and constitute a dense sampling of the visual world. Using a variety of non-parametric methods, we explore this world with the aid of a large dataset of 79,302,017 images collected from the Internet. Motivated by psychophysical results showing the remarkable tolerance of the human visual system to degradations in image resolution, the images in the dataset are stored as 32 x 32 color images. Each image is loosely labeled with one of the 75,062 non-abstract nouns in English, as listed in the Wordnet lexical database. Hence the image database gives a comprehensive coverage of all object categories and scenes. The semantic information from Wordnet can be used in conjunction with nearest-neighbor methods to perform object classification over a range of semantic levels minimizing the effects of labeling noise. For certain classes that are particularly prevalent in the dataset, such as people, we are able to demonstrate a recognition performance comparable to class-specific Viola-Jones style detectors.", "citations": ["38211dc39e41273c0007889202c69f841e02248a"], "references": ["5a5effa909cdeafaddbbb7855037e02f8e25d632", "b391878646123f5490ef2e2103de09a0947e4dc9"]}, {"paper_id": "1a2a770d23b4a171fa81de62a78a3deb0588f238", "paper_title": "Visualizing and Understanding Convolutional Networks", "paper_year": 2013, "paper_venue": "ECCV", "paper_citations": 999, "paper_abstract": "Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \\etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.", "citations": [], "references": ["0e650b5e54a3624792952899a0f79b91a1d68e79", "843959ffdccf31c6694d135fad07425924f785b1"]}, {"paper_id": "14ce7635ff18318e7094417d0f92acbec6669f1c", "paper_title": "DeepFace: Closing the Gap to Human-Level Performance in Face Verification", "paper_year": 2014, "paper_venue": "2014 IEEE Conference on Computer Vision and Pattern Recognition", "paper_citations": 999, "paper_abstract": "In modern face recognition, the conventional pipeline consists of four stages: detect => align => represent => classify. We revisit both the alignment step and the representation step by employing explicit 3D face modeling in order to apply a piecewise affine transformation, and derive a face representation from a nine-layer deep neural network. This deep network involves more than 120 million parameters using several locally connected layers without weight sharing, rather than the standard convolutional layers. Thus we trained it on the largest facial dataset to-date, an identity labeled dataset of four million facial images belonging to more than 4, 000 identities. The learned representations coupling the accurate model-based alignment with the large facial database generalize remarkably well to faces in unconstrained environments, even with a simple classifier. Our method reaches an accuracy of 97.35% on the Labeled Faces in the Wild (LFW) dataset, reducing the error of the current state of the art by more than 27%, closely approaching human-level performance.", "citations": [], "references": ["370b5757a5379b15e30d619e4d3fb9e8e13f3256", "052b1d8ce63b07fec3de9dbb583772d860b7c769"]}, {"paper_id": "5a5effa909cdeafaddbbb7855037e02f8e25d632", "paper_title": "Caltech-256 Object Category Dataset", "paper_year": 2007, "paper_venue": "", "paper_citations": 999, "paper_abstract": "We introduce a challenging set of 256 object categories containing a total of 30607 images. The original Caltech-101 [1] was collected by choosing a set of object categories, downloading examples from Google Images and then manually screening out all images that did not fit the category. Caltech-256 is collected in a similar manner with several improvements: a) the number of categories is more than doubled, b) the minimum number of images in any category is increased from 31 to 80, c) artifacts due to image rotation are avoided and d) a new and larger clutter category is introduced for testing background rejection. We suggest several testing paradigms to measure classification performance, then benchmark the dataset using two simple metrics as well as a state-of-the-art spatial pyramid matching [2] algorithm. Finally we use the clutter category to train an interest detector which rejects uninformative background regions.", "citations": ["54d2b5c64a67f65c5dd812b89e07973f97699552", "38211dc39e41273c0007889202c69f841e02248a"], "references": []}]], [[{"paper_id": "1510cf4b8abea80b9f352325ca4c132887de21a0", "paper_title": "Distributed Representations of Sentences and Documents", "paper_year": 2014, "paper_venue": "ICML", "paper_citations": 999, "paper_abstract": "Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, \"powerful,\" \"strong\" and \"Paris\" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.", "citations": ["26e743d5bd465f49b9538deaf116c15e61b7951f", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38"], "references": ["052b1d8ce63b07fec3de9dbb583772d860b7c769", "dac72f2c509aee67524d3321f77e97e8eff51de6", "649d03490ef72c5274e3bccd03d7a299d2f8da91", "c4fd9c86b2b41df51a6fe212406dda81b1997fd4", "df01d2dede1a243b9b0eb26c27246bc13705d930", "1005645c05585c2042e3410daeed638b55e2474d"]}, {"paper_id": "687bac2d3320083eb4530bf18bb8f8f721477600", "paper_title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "paper_year": 2013, "paper_venue": "EMNLP", "paper_citations": 999, "paper_abstract": "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.", "citations": ["bc8fa64625d9189f5801837e7b133e7fe3c581f7", "f04df4e20a18358ea2f689b4c129781628ef7fc1", "bc1d609520290e0460c49b685675eb5a57fa5935"], "references": ["23694a80bf1b9b38215be3e23068dd75296bc90f", "b25663fa149be5286de193c13324098aedd7e2cc", "1a07186bc10592f0330655519ad91652125cd907"]}, {"paper_id": "1a07186bc10592f0330655519ad91652125cd907", "paper_title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "paper_year": 2008, "paper_venue": "ICML", "paper_citations": 999, "paper_abstract": "We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.", "citations": ["a4cec122a08216fe8a3bc19b22e78fbaea096256", "dac72f2c509aee67524d3321f77e97e8eff51de6", "687bac2d3320083eb4530bf18bb8f8f721477600", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e"], "references": ["cd62c9976534a6a2096a38244f6cbb03635a127e", "23694a80bf1b9b38215be3e23068dd75296bc90f", "2c5135a0531bc5ad7dd890f018e67a40529f5bcb"]}, {"paper_id": "f37e1b62a767a307c046404ca96bc140b3e68cb5", "paper_title": "Glove: Global Vectors for Word Representation", "paper_year": 2014, "paper_venue": "EMNLP", "paper_citations": 999, "paper_abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.", "citations": ["af5c4b80fbf847f69a202ba5a780a3dd18c1a027"], "references": ["1a07186bc10592f0330655519ad91652125cd907", "dac72f2c509aee67524d3321f77e97e8eff51de6", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "c4fd9c86b2b41df51a6fe212406dda81b1997fd4", "10f97f1fb4f5c2c8e6c44d4a33da46d331dd4aeb", "23694a80bf1b9b38215be3e23068dd75296bc90f"]}, {"paper_id": "87f40e6f3022adbc1f1905e3e506abad05a9964f", "paper_title": "Distributed Representations of Words and Phrases and their Compositionality", "paper_year": 2013, "paper_venue": "NIPS", "paper_citations": 999, "paper_abstract": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.", "citations": ["0c47cad9729c38d9db1f75491b1ee4bd883a5d4e"], "references": ["1a07186bc10592f0330655519ad91652125cd907", "052b1d8ce63b07fec3de9dbb583772d860b7c769", "dac72f2c509aee67524d3321f77e97e8eff51de6", "1005645c05585c2042e3410daeed638b55e2474d", "23694a80bf1b9b38215be3e23068dd75296bc90f"]}, {"paper_id": "783480acff435bfbc15ffcdb4f15eccddaa0c810", "paper_title": "Class-Based n-gram Models of Natural Language", "paper_year": 1992, "paper_venue": "Computational Linguistics", "paper_citations": 999, "paper_abstract": "We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics.", "citations": ["d4e8bed3b50a035e1eabad614fe4218a34b3b178", "dac72f2c509aee67524d3321f77e97e8eff51de6", "9fa8d73e572c3ca824a04a5f551b602a17831bc5"], "references": []}, {"paper_id": "dac72f2c509aee67524d3321f77e97e8eff51de6", "paper_title": "Word Representations: A Simple and General Method for Semi-Supervised Learning", "paper_year": 2010, "paper_venue": "ACL", "paper_citations": 999, "paper_abstract": "If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize.com/projects/wordreprs/", "citations": ["f37e1b62a767a307c046404ca96bc140b3e68cb5", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "649d03490ef72c5274e3bccd03d7a299d2f8da91", "c4fd9c86b2b41df51a6fe212406dda81b1997fd4", "3febb2bed8865945e7fddc99efd791887bb7e14f", "59761abc736397539bdd01ad7f9d91c8607c0457"], "references": ["783480acff435bfbc15ffcdb4f15eccddaa0c810", "23694a80bf1b9b38215be3e23068dd75296bc90f", "1a07186bc10592f0330655519ad91652125cd907", "1005645c05585c2042e3410daeed638b55e2474d"]}, {"paper_id": "1005645c05585c2042e3410daeed638b55e2474d", "paper_title": "A Scalable Hierarchical Distributed Language Model", "paper_year": 2008, "paper_venue": "NIPS", "paper_citations": 591, "paper_abstract": "Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the non-hierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models.", "citations": ["1510cf4b8abea80b9f352325ca4c132887de21a0", "dac72f2c509aee67524d3321f77e97e8eff51de6", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "c4fd9c86b2b41df51a6fe212406dda81b1997fd4", "bc1d609520290e0460c49b685675eb5a57fa5935", "a4cec122a08216fe8a3bc19b22e78fbaea096256"], "references": ["783480acff435bfbc15ffcdb4f15eccddaa0c810", "d4e8bed3b50a035e1eabad614fe4218a34b3b178", "23694a80bf1b9b38215be3e23068dd75296bc90f"]}, {"paper_id": "26e743d5bd465f49b9538deaf116c15e61b7951f", "paper_title": "Learning Distributed Representations of Sentences from Unlabelled Data", "paper_year": 2016, "paper_venue": "HLT-NAACL", "paper_citations": 238, "paper_abstract": "Unsupervised methods for learning distributed representations of words are ubiquitous in today's NLP research, but far less is known about the best ways to learn distributed phrase or sentence representations from unlabelled data. This paper is a systematic comparison of models that learn such representations. We find that the optimal approach depends critically on the intended application. Deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-linear models work best for building representation spaces that can be decoded with simple spatial distance metrics. We also propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance.", "citations": ["0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "a23fa96e7217ba0e9405d9e1fe3cdedd57b6e096", "ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "a97dc52807d80454e78d255f9fbd7b0fab56bd03", "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "bc1d609520290e0460c49b685675eb5a57fa5935", "93b8da28d006415866bf48f9a6e06b5242129195", "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e"], "references": ["1510cf4b8abea80b9f352325ca4c132887de21a0", "e2b7f37cd97a7907b1b8a41138721ed06a0b76cd", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "39dba6f22d72853561a4ed684be265e179a39e4f", "843959ffdccf31c6694d135fad07425924f785b1"]}, {"paper_id": "59761abc736397539bdd01ad7f9d91c8607c0457", "paper_title": "context2vec: Learning Generic Context Embedding with Bidirectional LSTM", "paper_year": 2016, "paper_venue": "CoNLL", "paper_citations": 124, "paper_abstract": "Context representations are central to various NLP tasks, such as word sense disambiguation, named entity recognition, coreference resolution, and many more. In this work we present a neural model for efficiently learning a generic context embedding function from large corpora, using bidirectional LSTM. With a very simple application of our context representations, we manage to surpass or nearly reach state-of-the-art results on sentence completion, lexical substitution and word sense disambiguation tasks, while substantially outperforming the popular context representation of averaged word embeddings. We release our code and pretrained models, suggesting they could be useful in a wide variety of NLP tasks.", "citations": ["3febb2bed8865945e7fddc99efd791887bb7e14f", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38"], "references": ["dac72f2c509aee67524d3321f77e97e8eff51de6", "6e795c6e9916174ae12349f5dc3f516570c17ce8", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "f37e1b62a767a307c046404ca96bc140b3e68cb5"]}], [{"paper_id": "d4e8bed3b50a035e1eabad614fe4218a34b3b178", "paper_title": "An Empirical Study of Smoothing Techniques for Language Modeling", "paper_year": 1996, "paper_venue": "ACL", "paper_citations": 999, "paper_abstract": "We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We investigate for the first time how factors such as training data size, corpus (e.g., Brown versus Wall Street Journal), and n-gram order (bigram versus trigram) affect the relative performance of these methods, which we measure through the cross-entropy of test data. In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods.", "citations": ["1005645c05585c2042e3410daeed638b55e2474d"], "references": ["783480acff435bfbc15ffcdb4f15eccddaa0c810"]}, {"paper_id": "0a6383b13794452fb7339a7f8a5384885186ccf6", "paper_title": "Enriching Word Vectors with Subword Information", "paper_year": 2016, "paper_venue": "Transactions of the Association for Computational Linguistics", "paper_citations": 999, "paper_abstract": "Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.", "citations": ["3febb2bed8865945e7fddc99efd791887bb7e14f"], "references": []}, {"paper_id": "649d03490ef72c5274e3bccd03d7a299d2f8da91", "paper_title": "Learning Word Vectors for Sentiment Analysis", "paper_year": 2011, "paper_venue": "ACL", "paper_citations": 999, "paper_abstract": "Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term--document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.", "citations": ["1e077413b25c4d34945cc2707e17e46ed4fe784a", "1510cf4b8abea80b9f352325ca4c132887de21a0", "7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d", "4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "bc8fa64625d9189f5801837e7b133e7fe3c581f7"], "references": ["1a07186bc10592f0330655519ad91652125cd907", "dac72f2c509aee67524d3321f77e97e8eff51de6"]}, {"paper_id": "c4fd9c86b2b41df51a6fe212406dda81b1997fd4", "paper_title": "Linguistic Regularities in Continuous Space Word Representations", "paper_year": 2013, "paper_venue": "HLT-NAACL", "paper_citations": 999, "paper_abstract": "Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, \u201cKing Man + Woman\u201d results in a vector very close to \u201cQueen.\u201d We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems.", "citations": ["1510cf4b8abea80b9f352325ca4c132887de21a0", "bc1d609520290e0460c49b685675eb5a57fa5935", "f37e1b62a767a307c046404ca96bc140b3e68cb5"], "references": ["1a07186bc10592f0330655519ad91652125cd907", "dac72f2c509aee67524d3321f77e97e8eff51de6", "1005645c05585c2042e3410daeed638b55e2474d", "213d7af7107fa4921eb0adea82c9f711fd105232", "23694a80bf1b9b38215be3e23068dd75296bc90f", "47a87c2cbdd928bb081974d308b3d9cf678d257e"]}, {"paper_id": "23694a80bf1b9b38215be3e23068dd75296bc90f", "paper_title": "A Neural Probabilistic Language Model", "paper_year": 2000, "paper_venue": "NIPS", "paper_citations": 999, "paper_abstract": "A goal of statistical language modeling is to learn the joint probability function of sequences of words. This is intrinsically difficult because of the curse of dimensionality: we propose to fight it with its own weapons. In the proposed approach one learns simultaneously (1) a distributed representation for each word (i.e. a similarity between words) along with (2) the probability function for word sequences, expressed with these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar to words forming an already seen sentence. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach very significantly improves on a state-of-the-art trigram model.", "citations": ["c4fd9c86b2b41df51a6fe212406dda81b1997fd4", "87f40e6f3022adbc1f1905e3e506abad05a9964f", "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38"], "references": ["783480acff435bfbc15ffcdb4f15eccddaa0c810", "d4e8bed3b50a035e1eabad614fe4218a34b3b178"]}]]], "clusterNames": ["semantic textual similarity", "recurrent neural network", "machine reading comprehension", "named entity recognition", "object category", "word representation"]}
